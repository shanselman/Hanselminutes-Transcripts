WEBVTT FILE

1
00:00:00.660 --> 00:00:03.810
Hi, this is Scott. I
really appreciate our sponsors because

2
00:00:03.810 --> 00:00:07.110
they make the show possible.
Today's show is sponsored by

3
00:00:07.110 --> 00:00:11.400
Tellerik create compelling app experiences
across any screen with the

4
00:00:11.400 --> 00:00:16.260
Tellerik platform Telerx end to
end platform. Uniquely combines industry

5
00:00:16.260 --> 00:00:19.950
leading UI tools with cloud
services to simplify the entire

6
00:00:19.950 --> 00:00:24.420
app development cycle Tellerik offers
everything.net developers need to build

7
00:00:24.420 --> 00:00:33.840
quality apps faster. Try it
free at tellerik.com/platform that's tellerik.com/platform

8
00:00:47.150 --> 00:00:52.580
From the hanselminutes.com. It's Hanselman.
It's a weekly discussion with

9
00:00:52.580 --> 00:00:57.680
web developer and technologist Scott
Hanselman. This is Lawrence Ryan announcing

10
00:00:57.680 --> 00:01:02.360
show number 503. In this
episode, Scott talks with computer

11
00:01:02.360 --> 00:01:07.250
vision expert, Greg Borenstein about
imagining and designing a realistic

12
00:01:07.250 --> 00:01:14.570
future Scott Hanselman. This is
another episode of Hansel minutes,

13
00:01:14.570 --> 00:01:17.060
and today we're chatting with
Greg Bornstein. He is a

14
00:01:17.060 --> 00:01:19.880
futurist consultant on one of
my favorite shows, the new

15
00:01:19.880 --> 00:01:23.390
minority report. He's got an
extensive background in computer vision

16
00:01:23.390 --> 00:01:26.660
and graphics programming, and just
all things interactive. And you

17
00:01:26.660 --> 00:01:28.360
were at the MIT media
lab for a while. Why

18
00:01:28.370 --> 00:01:30.500
aren't you? Yeah, that's right.
I just graduated in June

19
00:01:30.500 --> 00:01:32.930
from the playful systems group
there. I love that there

20
00:01:32.930 --> 00:01:37.250
is even a playful systems
group. Is it just literally

21
00:01:37.280 --> 00:01:39.680
making stuff that's fun or
why is it called playful

22
00:01:39.680 --> 00:01:42.460
systems? One big aspect of
it you can think of

23
00:01:42.480 --> 00:01:44.330
as kind of the future
of games, like thinking about

24
00:01:44.330 --> 00:01:46.460
how to make games in
different environments. And then the

25
00:01:46.460 --> 00:01:50.060
other aspect of it is
really using a playful approach

26
00:01:50.060 --> 00:01:52.550
to all the different systems
that exist in our world.

27
00:01:52.550 --> 00:01:54.620
The same way games do
with the systems that they

28
00:01:54.620 --> 00:01:58.430
present taking a similar kind
of systemic interaction approach to

29
00:01:58.730 --> 00:02:00.560
the real systems in our
world that we live in.

30
00:02:01.190 --> 00:02:05.690
Very cool. So you are
currently a futurist consultant or

31
00:02:05.690 --> 00:02:08.930
consulting on the future in
minority report, which is the

32
00:02:08.930 --> 00:02:12.170
TV show that is currently
out right now with making

33
00:02:12.170 --> 00:02:15.650
good, which is kind of
an extension of the minority

34
00:02:15.650 --> 00:02:18.590
report movie with Tom cruise.
How do you get a

35
00:02:18.590 --> 00:02:22.430
gig like that? Well, first
of all, I try to

36
00:02:22.430 --> 00:02:24.470
call myself a futurist because
I think that's often a,

37
00:02:25.490 --> 00:02:28.640
a name for, for people
who are like, take a

38
00:02:28.640 --> 00:02:30.890
little bit too much credit
for their ability to predict

39
00:02:32.330 --> 00:02:34.790
that prediction, people who put
thought leader in their Twitter

40
00:02:34.790 --> 00:02:37.370
bio. Yeah. Futurist has that
same kind of feeling to

41
00:02:37.370 --> 00:02:39.170
me because as I think
of it as like, you

42
00:02:39.170 --> 00:02:41.900
know, scientific and research consultant
and a big part of

43
00:02:41.900 --> 00:02:44.930
that is it's less about
trying to predict the future,

44
00:02:45.860 --> 00:02:47.750
you know, and then it
is about how do you

45
00:02:47.960 --> 00:02:50.810
depict a future in a
way that says something interesting

46
00:02:50.810 --> 00:02:56.210
about today and also what
technologies or coming social changes

47
00:02:56.210 --> 00:02:58.640
or things like that that
are coming are gonna, you

48
00:02:58.640 --> 00:03:00.940
know, are going to shape
how We think of, you

49
00:03:00.940 --> 00:03:02.980
know, how we think of
society, how we live as

50
00:03:02.980 --> 00:03:06.880
people, and then how can
we depict and explain those

51
00:03:06.880 --> 00:03:09.760
effects to an audience has
never seen, seen them before.

52
00:03:10.380 --> 00:03:12.450
Interesting. Let's, let's take that
apart a little bit. I

53
00:03:12.450 --> 00:03:14.610
like the way that you
phrase that. So this isn't

54
00:03:14.610 --> 00:03:17.760
as much me, like I'm
going to go into the

55
00:03:17.760 --> 00:03:19.740
future and then come back
and then tell Greg whether

56
00:03:19.740 --> 00:03:22.680
he was right or not
as much as it's taking

57
00:03:22.680 --> 00:03:25.470
where we're kind of headed
the vector that we're headed

58
00:03:25.470 --> 00:03:28.470
in and, and getting a
general sense of like, yeah,

59
00:03:28.470 --> 00:03:31.050
I could see this coming
to a certain point. And

60
00:03:31.050 --> 00:03:33.510
then what would be the
social ramifications of that? If

61
00:03:33.510 --> 00:03:36.180
in fact we went to
that. Yeah, absolutely. And part

62
00:03:36.180 --> 00:03:37.680
of that is because of
the timeline of the show,

63
00:03:37.680 --> 00:03:40.410
which is 50 years out,
it's set in 2065. If

64
00:03:40.410 --> 00:03:42.480
you talk to anyone, you
know, I, as I did

65
00:03:42.480 --> 00:03:44.250
for this to do the
work for the show, talk

66
00:03:44.250 --> 00:03:47.400
to a ton of scientists
and technologists and engineers working

67
00:03:47.400 --> 00:03:50.070
in a wide variety of
different fields and you ask

68
00:03:50.070 --> 00:03:51.780
them 50 years, they all
throw up their hands and

69
00:03:51.780 --> 00:03:53.820
say, 50 years is impossible.
You know, they can tell

70
00:03:53.820 --> 00:03:56.580
you very, exactly what will
happen in five years. They

71
00:03:56.580 --> 00:03:59.880
can give you extremely grand
longterm ambitions and you know,

72
00:03:59.880 --> 00:04:03.750
a hundred years, but like
50, 50 years is totally

73
00:04:03.750 --> 00:04:06.240
impossible. It's like, it's like
off the end of anything

74
00:04:06.240 --> 00:04:09.060
currently being imagined, but not
far enough to be totally

75
00:04:09.060 --> 00:04:11.580
magic. So, so part of
that, the way we take

76
00:04:11.580 --> 00:04:13.410
advantage of that is like,
you know, some of the

77
00:04:13.410 --> 00:04:15.780
things we put in the
show, we say they're 50

78
00:04:15.780 --> 00:04:18.300
years, but they're really, they're
much, much sooner than that.

79
00:04:18.330 --> 00:04:20.700
They're five to 10 years,
but a naive audience. Who's

80
00:04:20.700 --> 00:04:22.830
never heard of the topic
before. Wouldn't believe it. If

81
00:04:22.830 --> 00:04:24.660
you told them, that's what
it was. So the great

82
00:04:24.660 --> 00:04:27.120
example of that is in
the, in the first episode,

83
00:04:27.360 --> 00:04:31.230
the bringing the passenger pigeons
back from extinction, which is

84
00:04:31.230 --> 00:04:34.530
something that people are actively
working on today and will

85
00:04:34.530 --> 00:04:37.350
happen in the next five
to 10 years. And that

86
00:04:37.350 --> 00:04:39.420
is, but if you tell
people that, that, that happened

87
00:04:39.420 --> 00:04:42.540
in 20, 20, 20, 25,
they wouldn't believe you. So

88
00:04:42.540 --> 00:04:45.390
by being out 50 years,
you get, you know, much

89
00:04:45.390 --> 00:04:48.810
more leeway for people, believing
things, surprising, things like that.

90
00:04:49.380 --> 00:04:52.500
Interesting. You know, it's so
funny that they say hindsight

91
00:04:52.500 --> 00:04:56.040
is 2020. You know, the
iPad was not that long

92
00:04:56.040 --> 00:04:59.790
ago, you know, and, and
star Trek, the next generation

93
00:05:00.030 --> 00:05:03.470
had, you know, these L
cars, pads that they called

94
00:05:03.480 --> 00:05:06.750
in PADD that were effectively
iPads. And it seemed almost,

95
00:05:06.780 --> 00:05:09.240
almost a little bit less
sophisticated than what we have

96
00:05:09.240 --> 00:05:11.580
now. And they were going
300 years out. If you

97
00:05:11.580 --> 00:05:13.560
would tell someone by now
we'd have these things and

98
00:05:13.560 --> 00:05:16.680
we could ask Siri questions,
then I think, Yeah, the

99
00:05:16.740 --> 00:05:18.690
iPads or the iPads and
star Trek, you're saying I'm

100
00:05:18.690 --> 00:05:21.360
totally obsessed with, because it's
an example. I think of

101
00:05:21.360 --> 00:05:23.940
something that people get really
wrong about futurism, which is

102
00:05:24.240 --> 00:05:26.370
they have the form, right?
Do people talk about like

103
00:05:26.460 --> 00:05:28.920
how star Trek invented the
cell phone or invented the

104
00:05:28.920 --> 00:05:31.340
iPad like that? And those
iPads, if you watched them

105
00:05:31.340 --> 00:05:33.060
on star Trek, they have
the form factor, you know,

106
00:05:33.060 --> 00:05:36.270
it's this tablet, computer, but
the way they use them,

107
00:05:36.780 --> 00:05:38.790
when you look at them
today, it seems just crazy.

108
00:05:39.060 --> 00:05:41.640
Like people are constantly handing
them to each other or

109
00:05:41.640 --> 00:05:44.040
like in one of the
movies, they show Picard's desk.

110
00:05:44.040 --> 00:05:45.900
And they're trying to indicate
that he's very busy. He's

111
00:05:45.900 --> 00:05:48.060
got a stack of them,
like spilling over on his

112
00:05:48.060 --> 00:05:51.330
desk, you know? And if
you, for a while watching

113
00:05:51.330 --> 00:05:53.580
that stuff, every watched a
bunch of those recently, and

114
00:05:53.580 --> 00:05:55.860
I was like, couldn't understand
why they were, why they

115
00:05:55.860 --> 00:05:58.580
were so being used so
weirdly. And the answer is

116
00:05:58.580 --> 00:06:01.280
they don't have the internet,
they have iPads and not

117
00:06:01.280 --> 00:06:04.340
the internet. So I pads
are just space paper. They're

118
00:06:04.340 --> 00:06:08.090
not like, like our tablets
are today. Like only makes

119
00:06:08.090 --> 00:06:10.940
sense as a portal into
the network. You know, everything

120
00:06:10.940 --> 00:06:13.220
we do on them is
about their use on the

121
00:06:13.220 --> 00:06:16.430
network. You would never have
multiple pads for different documents,

122
00:06:16.640 --> 00:06:19.070
obviously because those documents live
on the digital device and

123
00:06:19.070 --> 00:06:21.440
are copied through the internet.
And so as soon as

124
00:06:21.440 --> 00:06:24.140
you see that, like they
don't actually have like the

125
00:06:24.200 --> 00:06:26.480
iPads and tablets as we
see them today, they are

126
00:06:26.480 --> 00:06:29.420
mostly the internet with a
little bit of interface and

127
00:06:29.420 --> 00:06:31.670
star Trek doesn't have the
internet. It just has the

128
00:06:31.670 --> 00:06:35.030
interface, which actually makes the
futurism like totally wrong from

129
00:06:35.030 --> 00:06:38.290
my perspective. Yeah. Yeah. And
it makes me realize that

130
00:06:38.290 --> 00:06:41.440
it was very kind of
hub and spoke, you know,

131
00:06:41.440 --> 00:06:44.530
there was the central internet,
but it was really just

132
00:06:44.590 --> 00:06:47.350
the core machine that was
really smart. You know, the,

133
00:06:47.860 --> 00:06:50.650
the, the gene Roddenberry's wife,
you know, the voice of

134
00:06:50.650 --> 00:06:52.690
Michelle Barrett kind of like in
the background, she was, It

135
00:06:52.690 --> 00:06:56.170
was like the, the how
Of star Trek. And there

136
00:06:56.170 --> 00:06:57.850
was a hub and spoke
where all the pads could

137
00:06:57.850 --> 00:07:00.580
basically talk to that, but
that wasn't the cloud. And

138
00:07:00.580 --> 00:07:02.710
there was no peer to
peer. And it only works

139
00:07:02.710 --> 00:07:04.690
when it's live. You know,
they're talking constantly talking about

140
00:07:04.690 --> 00:07:08.770
like, they can't get subspace,
you know, communications basically CB

141
00:07:08.770 --> 00:07:11.800
radio. And, you know, they
can, they have these digital

142
00:07:11.800 --> 00:07:14.980
fabricators that can like produce
beverages that can get aliens

143
00:07:14.980 --> 00:07:17.200
of different species drunk, but
they can't like, they don't

144
00:07:17.200 --> 00:07:19.570
seem to be able to
make those, you know, digital,

145
00:07:19.600 --> 00:07:21.490
then they can make all
those digital computers, but they

146
00:07:21.490 --> 00:07:23.440
don't seem to be able
to like, have the information

147
00:07:23.440 --> 00:07:27.130
technology to transmit data between
them. It's really interesting when

148
00:07:27.130 --> 00:07:28.780
you juxtapose that with some
of the things that are

149
00:07:28.780 --> 00:07:31.060
going on in minority report,
because like, let's say that,

150
00:07:31.060 --> 00:07:33.190
you know, Picard goes up
to number one and it

151
00:07:33.190 --> 00:07:36.550
says here and hands him,
the pad in minority report,

152
00:07:37.060 --> 00:07:39.310
there's just a swipe gesture.
It's just, I'm swiping it

153
00:07:39.310 --> 00:07:41.650
in your general direction and
it jumps onto your table

154
00:07:41.650 --> 00:07:43.690
or it jumps into your
eyes or it jumps onto

155
00:07:43.690 --> 00:07:46.420
your pad. Yeah, absolutely. And
that's one thing, one of

156
00:07:46.420 --> 00:07:49.690
my, my tiny little Wars,
I fight as we'd work

157
00:07:49.690 --> 00:07:51.940
on the show is I
try never to have anyone

158
00:07:51.940 --> 00:07:54.910
hand everyone, anyone else, a
flashdrive or like a little

159
00:07:54.910 --> 00:07:58.390
hard drive. And that's like,
it's a tiny little thing.

160
00:07:58.390 --> 00:08:00.100
But to me it's something
that matters because of what

161
00:08:00.100 --> 00:08:02.370
you exactly what you just
said, you know, it's the,

162
00:08:02.370 --> 00:08:04.900
the screenwriters often want it
for narrative reasons. Like how

163
00:08:04.900 --> 00:08:08.440
do you depict the story
element of, I am passing

164
00:08:08.440 --> 00:08:10.720
you a file. I'm giving
you this critical information to

165
00:08:10.720 --> 00:08:13.330
our story. And, you know,
and if, if you and

166
00:08:13.330 --> 00:08:14.920
I met and I, you
asked me, will you send

167
00:08:14.920 --> 00:08:16.810
me, send me that file?
I wouldn't whip out a

168
00:08:16.810 --> 00:08:18.490
flash drive and hand it
to you, I'd say, okay.

169
00:08:18.490 --> 00:08:19.840
And then I would tap
my phone and then you

170
00:08:19.840 --> 00:08:22.750
would have it, but that's
totally invisible and non, you

171
00:08:22.750 --> 00:08:25.090
know, not dramatic to watch.
And that's, that's one of

172
00:08:25.090 --> 00:08:27.130
the big challenges. And that's
something like, one of my

173
00:08:27.370 --> 00:08:30.100
ways of trying to address
that challenge in minority report

174
00:08:30.280 --> 00:08:33.190
is to fight against handling
flash drives and to have

175
00:08:33.190 --> 00:08:34.390
to have them just have
a line or two of

176
00:08:34.390 --> 00:08:36.610
dialogue to send like, Oh,
I'll send you like a

177
00:08:36.610 --> 00:08:38.890
secure download link for that
or whatever, you know, something

178
00:08:38.890 --> 00:08:41.470
like that. So that it's,
it's clear that they're actually

179
00:08:41.470 --> 00:08:43.510
just using the network and
not passing those drives around.

180
00:08:43.780 --> 00:08:46.600
It does seem like so
many futurists and people who

181
00:08:46.600 --> 00:08:49.810
think about the future completely
missed the idea of wireless

182
00:08:49.840 --> 00:08:53.200
and ubiquitous connectivity. I mean,
in snow crash, they get

183
00:08:53.200 --> 00:08:55.410
this amazing matrix that they
Jack into, but they do

184
00:08:55.410 --> 00:08:58.070
it from a payphone. Yup.
Yeah. Which goes all the

185
00:08:58.070 --> 00:09:00.950
way back to, to William Gibson
and the origins of cyberspace

186
00:09:00.980 --> 00:09:03.530
in the phone system. Yeah.
Everyone seems to have a

187
00:09:03.530 --> 00:09:06.260
fun, you know, there's a
payphone and right now actually

188
00:09:06.260 --> 00:09:07.760
recently one of my seven
year old, my seven year

189
00:09:07.760 --> 00:09:09.830
old saw a payphone and
was like, there's just a

190
00:09:09.830 --> 00:09:14.270
random phone sitting right here.
It is weird. Yeah. And

191
00:09:14.270 --> 00:09:16.850
that's actually, you know, that's
the, that's something that is

192
00:09:16.850 --> 00:09:20.570
even much more stronger in
television or film, which is

193
00:09:20.570 --> 00:09:22.940
that there's this visual bias,
right. It's not a novel

194
00:09:22.940 --> 00:09:25.250
it's that there's, it's a
visual medium. And so that

195
00:09:25.250 --> 00:09:28.580
creates this incredibly strong bias
towards any visual depiction of

196
00:09:28.580 --> 00:09:31.340
the future. So, cool, weird
looking hardware that, you know,

197
00:09:31.700 --> 00:09:34.100
cars are the things that
look really different from today

198
00:09:34.250 --> 00:09:37.160
or interfaces that are visual
and splashy, like the original

199
00:09:37.160 --> 00:09:39.950
minor minority report movie is
extremely influential as a piece

200
00:09:39.950 --> 00:09:43.250
of science fiction, but it's
also like, I love it

201
00:09:43.250 --> 00:09:44.690
as a movie, don't get
me wrong, but it's very

202
00:09:44.690 --> 00:09:47.270
shallow as a piece of
science fiction. It basically depicts

203
00:09:47.270 --> 00:09:49.880
a world totally unchanged from
the world of today, except

204
00:09:49.880 --> 00:09:54.590
with like really beautiful, fascinating
gesture interfaces, you know, and

205
00:09:54.590 --> 00:09:57.500
like it's, most of those
lessons in terms of the

206
00:09:57.500 --> 00:09:59.510
world is the pic that
have already been fully digested

207
00:09:59.510 --> 00:10:02.990
10 years, 13 years later.
And that's, to me, you

208
00:10:02.990 --> 00:10:04.640
know, a little bit of
a miss in terms of

209
00:10:04.640 --> 00:10:07.760
what I want from science,
No I'm sitting here and

210
00:10:07.760 --> 00:10:10.250
I'm talking to you with
a 30 inch monitor that

211
00:10:10.250 --> 00:10:13.970
is probably six or seven
years old works great. And

212
00:10:13.970 --> 00:10:15.890
I've got a 25 here
in a 25 years. So

213
00:10:15.890 --> 00:10:19.070
I've got, you know, five
or 6,000 pixels across. I

214
00:10:19.070 --> 00:10:22.820
am pretty much hooked up,
but it's not really changed

215
00:10:22.820 --> 00:10:27.050
in the last eight years
in minority report. When you

216
00:10:27.050 --> 00:10:31.100
go to the police station,
they've got clear see-through LCDs.

217
00:10:31.100 --> 00:10:32.810
This seems to be the
thing that says we're in

218
00:10:32.810 --> 00:10:37.010
the future. Right. You show
someone the reverse side of

219
00:10:37.010 --> 00:10:40.640
the LCD, but then when
they go and visit one

220
00:10:40.640 --> 00:10:44.270
of their helpers, one of
their like exiled police helpers,

221
00:10:44.540 --> 00:10:46.520
he doesn't have the fancy
tech. He doesn't have the

222
00:10:46.520 --> 00:10:50.270
see-through LCD. What does that
fascination with these transparent LCDs?

223
00:10:50.420 --> 00:10:52.160
And was that a conscious
decision to indicate that he

224
00:10:52.160 --> 00:10:54.590
doesn't have his fancy stuff
as they do back in

225
00:10:54.590 --> 00:10:56.540
the base? Yeah, definitely. So
that comes from one of

226
00:10:56.540 --> 00:10:58.640
our core principles of the
futurism of the show, which

227
00:10:58.640 --> 00:11:02.570
is that, you know, there's
the famous William Gibson quote that

228
00:11:02.570 --> 00:11:04.160
everyone knows, which is that
the future is here. It's

229
00:11:04.160 --> 00:11:07.730
not just, it's just not
evenly distributed. And that really,

230
00:11:07.760 --> 00:11:09.800
we, we really try to
do that, which is in

231
00:11:09.800 --> 00:11:12.110
a lot of science fiction
visions, you get like, Oh,

232
00:11:12.300 --> 00:11:14.720
this is the future of
X, you know? And then

233
00:11:14.900 --> 00:11:18.290
every single person has X
technology. You know, no matter

234
00:11:18.500 --> 00:11:20.120
how rich they are, how
poor they are, whether they're

235
00:11:20.120 --> 00:11:22.610
working in an institution, you
know, like a police station

236
00:11:22.610 --> 00:11:24.620
or a fancy tech company,
or whether they're just some

237
00:11:24.620 --> 00:11:27.530
individual on the street. And
as anyone who's at all

238
00:11:27.710 --> 00:11:30.920
notices, the role of technology
in society knows that's just

239
00:11:30.920 --> 00:11:33.410
not how technology works, you
know, at any given time

240
00:11:33.410 --> 00:11:35.770
there's some technology, that's the
new hot thing that everybody's

241
00:11:35.770 --> 00:11:38.120
super excited about. You know,
right now we might name

242
00:11:38.150 --> 00:11:41.360
virtual reality or gesture interfaces.
Are there things like that?

243
00:11:41.360 --> 00:11:44.060
And then there's the ones
that were, are that are

244
00:11:44.090 --> 00:11:47.390
ubiquitous, but still feel very
new, like smartphones. Then there's

245
00:11:47.390 --> 00:11:49.790
the older ones that are
even more ubiquitous, but are,

246
00:11:49.850 --> 00:11:52.130
you know, nobody even notices
them as technology anymore, like

247
00:11:52.130 --> 00:11:55.360
laptops. And then you go
back to, you know, I

248
00:11:55.510 --> 00:11:57.460
remember it wasn't that many
years ago. I remember, I

249
00:11:57.460 --> 00:12:00.010
think it was in 2014.
I remember reading the headline

250
00:12:00.280 --> 00:12:03.130
that India had just turned
off their last Telegraph system

251
00:12:03.460 --> 00:12:06.250
and still had 25,000 users.
You know? So like the

252
00:12:06.250 --> 00:12:09.460
past, doesn't just like that
the snap just disappear. It

253
00:12:09.460 --> 00:12:13.060
stays around. And so a
realistic world of the future,

254
00:12:13.300 --> 00:12:14.800
there will still be people
using things to look very

255
00:12:14.800 --> 00:12:17.710
much like smartphones. There might
be like an AR in

256
00:12:17.830 --> 00:12:20.980
the future of 20, 65
in minority report. The newest

257
00:12:20.980 --> 00:12:24.700
piece of technology is the
contact lens, augmented reality computing,

258
00:12:24.880 --> 00:12:26.860
you know, and that's not,
it's not like every person

259
00:12:26.860 --> 00:12:29.080
in the street suddenly has
one of those. It's used

260
00:12:29.080 --> 00:12:31.510
for experts who need them
in their job. Like detective

261
00:12:31.720 --> 00:12:33.970
Vega, who's the lead character
in the show she's using

262
00:12:33.970 --> 00:12:35.830
it constantly in the course
of doing her work because

263
00:12:35.830 --> 00:12:38.140
she's a specialist who needs
it and you'll see a

264
00:12:38.140 --> 00:12:40.630
few other people, but it's
not ubiquitous. And then you'll

265
00:12:40.630 --> 00:12:43.450
see things like those variations.
You're talking about like the

266
00:12:43.450 --> 00:12:46.840
police stations, having the coolest,
newest tech, you know, gesture

267
00:12:46.840 --> 00:12:50.050
interfaces. And then Wally who's
the, the tech that go

268
00:12:50.050 --> 00:12:53.170
to the side has the
tactual touchscreen, you know, which

269
00:12:53.170 --> 00:12:55.300
isn't as slick and is
not buying it with government's

270
00:12:55.300 --> 00:12:57.520
money. And, you know, it's,
it's a little, and it's

271
00:12:57.520 --> 00:13:00.430
probably a little bit older
too. You know, that's really,

272
00:13:00.460 --> 00:13:03.240
really valuable Pointed that out
because I'm thinking about like

273
00:13:03.270 --> 00:13:06.780
the Microsoft just announced the
price on the HoloLens developer

274
00:13:06.780 --> 00:13:09.890
kit, and it's gonna be
$3,000. And I think there's

275
00:13:10.050 --> 00:13:13.530
like a minimum buy of
two, right? So they immediately

276
00:13:13.530 --> 00:13:17.070
priced it all out completely,
but they keep saying, Hey,

277
00:13:17.070 --> 00:13:20.730
the hallow HoloLens is not
the Oculus rift. This is

278
00:13:20.730 --> 00:13:23.460
a tool for high end
for, you know, for forensics,

279
00:13:23.460 --> 00:13:26.010
for architects and things like
that. I think we all

280
00:13:26.010 --> 00:13:29.130
want whatever the new tech
is to become ubiquitous and

281
00:13:29.130 --> 00:13:33.090
$300 overnight. Yeah. And that,
you know, some tech never

282
00:13:33.090 --> 00:13:36.330
becomes ubiquitous, you know, like
the sr 71 Blackbird was

283
00:13:36.330 --> 00:13:38.550
pretty impressive in the seventies,
but there's not one in

284
00:13:38.550 --> 00:13:41.520
every garage, you know, like
that's just, and, and that's,

285
00:13:41.520 --> 00:13:43.860
you know, that's kind of
ridiculous example, but even then

286
00:13:43.860 --> 00:13:47.160
it's like, there's plenty of
tech that like only lives

287
00:13:47.160 --> 00:13:50.130
in institutions and specialized jobs
and never, and never water's

288
00:13:50.130 --> 00:13:52.980
down like that, that, you
know, the, the Gartner hype

289
00:13:52.980 --> 00:13:56.640
cycle is not a rule
of nature that happens for

290
00:13:56.640 --> 00:14:01.740
every technology. Hey folks, I
wanted to take a moment

291
00:14:01.740 --> 00:14:04.830
to tell you about Reagan
and their new product called

292
00:14:04.830 --> 00:14:08.250
pulse. Reagan is an error
and crash reporting software provider

293
00:14:08.550 --> 00:14:12.390
and their new product pulse.
It's a realtime user monitoring

294
00:14:12.390 --> 00:14:15.720
product. It gives you performance
data and user insights. Lets

295
00:14:15.720 --> 00:14:19.020
you understand exactly what's happening
when users interact with your

296
00:14:19.020 --> 00:14:22.710
software. So you're never left
guessing. Reagan provides you with

297
00:14:22.710 --> 00:14:25.560
the answers to your performance
questions and they've found over

298
00:14:25.560 --> 00:14:29.790
10 billion that's billion with a
B bugs and customer apps

299
00:14:29.790 --> 00:14:33.210
with their crash reporting product.
And now Reagan will help

300
00:14:33.210 --> 00:14:37.050
you understand application quality. Like
no one else over 30,000

301
00:14:37.050 --> 00:14:40.110
developers worldwide, can't be wrong.
I use Reagan and all

302
00:14:40.110 --> 00:14:42.300
the time and I enjoyed
it very much. You can

303
00:14:42.300 --> 00:14:44.850
try it out today with
a no risk 30 day

304
00:14:44.850 --> 00:14:49.290
free trial start improving your
software quality immediately try Reagan

305
00:14:49.290 --> 00:14:56.390
for free today@reagan.io. When you're
thinking of these kinds of

306
00:14:56.390 --> 00:14:58.460
technologies, you're trying to take
a current technology and move

307
00:14:58.460 --> 00:15:02.180
it forward. It does make
me wonder why things don't

308
00:15:02.180 --> 00:15:05.420
exist now to use the
example of two people meet,

309
00:15:05.540 --> 00:15:07.700
I've got a pad, you've
got a phone and I

310
00:15:07.730 --> 00:15:12.110
make a swiping gesture over
to you right now. I

311
00:15:12.110 --> 00:15:16.040
mean, Bluetooth is a nightmare
of pairing and lost connections

312
00:15:16.070 --> 00:15:18.710
and ridiculous like pin numbers.
We used to do the

313
00:15:18.710 --> 00:15:21.290
whole pin number thing. And
now half of my Bluetooth

314
00:15:21.290 --> 00:15:24.200
devices just have a pin
number of all zeros. You

315
00:15:24.200 --> 00:15:28.490
know, why is the idea
of peer-to-peer pairing so ridiculous

316
00:15:28.490 --> 00:15:30.410
and why am I not
just moving my hand and

317
00:15:30.410 --> 00:15:32.810
swiping things over to you
and having jumped the gap?

318
00:15:33.280 --> 00:15:35.530
Yeah. So this is an
area that John under Koffler

319
00:15:35.530 --> 00:15:37.480
who did, he was the
person who had my job

320
00:15:37.480 --> 00:15:40.300
on the original movie in
that back in 2002, he

321
00:15:40.300 --> 00:15:42.760
was a graduate student at
MIT at the media lab

322
00:15:42.760 --> 00:15:46.540
as well in Hershey issues,
tangible media group. And he

323
00:15:46.540 --> 00:15:49.030
did the futurism for the
movie. And that's what he

324
00:15:49.030 --> 00:15:52.330
does. Is he, his company
oblong does exactly. That's the

325
00:15:52.330 --> 00:15:55.330
problem. They work on solving
exactly what you're describing. They

326
00:15:55.330 --> 00:15:57.580
built the Fox news desk.
If you've seen it as

327
00:15:57.590 --> 00:16:00.160
like their set, which, and
they have amazing demos where

328
00:16:00.160 --> 00:16:01.150
you go in a room
and you walk in with

329
00:16:01.150 --> 00:16:03.640
your device and you know,
everybody, you can throw things

330
00:16:03.640 --> 00:16:06.460
from your tablet to the
screen and like, no matter

331
00:16:06.460 --> 00:16:08.050
what operating systems people are
in, it all kind of

332
00:16:08.050 --> 00:16:11.020
works. And, and it's amazing.
But I, I think that

333
00:16:11.020 --> 00:16:13.090
that's an area where I
actually have a very different

334
00:16:13.090 --> 00:16:15.700
view as a futurist, which
is, I really like another

335
00:16:15.700 --> 00:16:17.590
one of my little things
I haven't succeeded in getting

336
00:16:17.590 --> 00:16:18.580
in the show yet, but
I want to do a

337
00:16:18.580 --> 00:16:20.530
scene where they all come
in with their different tablets

338
00:16:20.530 --> 00:16:23.740
and they actually spend five
seconds pairing and getting stuck

339
00:16:23.740 --> 00:16:26.230
and not having that work
like every room I've ever

340
00:16:26.230 --> 00:16:28.690
been in that and use
the projector because I think

341
00:16:28.690 --> 00:16:31.660
that's not a technical problem.
That's a social and institutional

342
00:16:31.660 --> 00:16:34.540
problem, which doesn't get fixed,
which is permanent because it's

343
00:16:34.540 --> 00:16:36.640
not about, it's not a
technical problem that gets solved.

344
00:16:36.640 --> 00:16:39.280
It's not silicone getting cheaper
or faster. It's nothing like

345
00:16:39.280 --> 00:16:42.520
that. It's a nature of
human institutions and like compare

346
00:16:42.550 --> 00:16:46.120
competition in business and standard
settings and like the nature

347
00:16:46.120 --> 00:16:48.820
of human as social animals.
That means that those kinds

348
00:16:48.820 --> 00:16:52.030
of problems never get solved.
You know, You have to

349
00:16:52.030 --> 00:16:56.980
do that. So I work
remotely for Microsoft and I

350
00:16:56.980 --> 00:16:59.470
was on a meeting day
before yesterday and we spent,

351
00:16:59.470 --> 00:17:03.850
I swear to God, 12
or 13 minutes, just, can

352
00:17:03.850 --> 00:17:05.780
you hear me? Can you,
can you, can you turn

353
00:17:05.780 --> 00:17:08.260
it? I mean, it was
ridic and these were, these

354
00:17:08.260 --> 00:17:11.920
were smart people and you
know, I, so I thought

355
00:17:11.920 --> 00:17:14.140
that, Oh, this is a
problem with Skype, right. It

356
00:17:14.140 --> 00:17:16.630
must be, it must be
wonderful on the other side

357
00:17:16.750 --> 00:17:18.820
and someone else there used
to work for Google and

358
00:17:18.820 --> 00:17:20.830
he's like, no, we have
the same problem with Hangouts.

359
00:17:21.160 --> 00:17:23.440
And someone else uses blue
jeans. It's like, there is

360
00:17:23.440 --> 00:17:26.590
literally no system that you
can simply call someone and

361
00:17:26.590 --> 00:17:28.690
have the camera work and
the mic work every single

362
00:17:28.690 --> 00:17:30.880
time reliably a hundred percent.
So I would love to

363
00:17:30.880 --> 00:17:33.910
see that not work 50
years. Yeah. This is part

364
00:17:33.910 --> 00:17:35.710
of my view about like
when we, when we cover

365
00:17:35.710 --> 00:17:38.620
internet of things, he things
also, which is like the,

366
00:17:39.010 --> 00:17:42.250
you know, software as anybody
who works on software works

367
00:17:42.250 --> 00:17:45.910
intimately with software knows. Software
is amazing. It's incredibly flexible

368
00:17:46.060 --> 00:17:48.550
and it's broken at least
20% of the time. And

369
00:17:48.550 --> 00:17:50.550
so one of things that,
you know, if you're lucky,

370
00:17:50.580 --> 00:17:52.800
like a great system is
broken only 20% of the

371
00:17:52.800 --> 00:17:55.320
time. And so one of
the things that's going to

372
00:17:55.320 --> 00:17:57.930
happen as we put software
in everything, is that it's

373
00:17:57.930 --> 00:18:02.340
going to have all this
flexibility and programmability and changeability,

374
00:18:02.430 --> 00:18:04.710
and it will be broken
20% of the time, you

375
00:18:04.710 --> 00:18:07.590
know, systems that were used
to being extremely robust and

376
00:18:07.590 --> 00:18:10.050
broken away less will be
broken more. And you know,

377
00:18:10.060 --> 00:18:12.480
that might mean we don't
put software in the intimate

378
00:18:12.480 --> 00:18:14.820
parts of somewhere like mission
critical systems where that really,

379
00:18:14.940 --> 00:18:16.560
you know, life or death
things where that really can't

380
00:18:16.560 --> 00:18:19.410
be. But it might also
just mean that we'd make

381
00:18:19.410 --> 00:18:21.570
that trade off. You know,
we get used to, you

382
00:18:21.570 --> 00:18:23.580
know, like, you know, if
you have a Tesla, you

383
00:18:23.580 --> 00:18:25.230
and your Tesla, it's doing
a software update and you

384
00:18:25.230 --> 00:18:27.350
can't drive for 10 minutes
or, you know, a couple

385
00:18:27.370 --> 00:18:29.040
hours and then that's, you
just get used to that.

386
00:18:29.040 --> 00:18:31.410
Cause there's so many advantages.
And like we make, we

387
00:18:31.410 --> 00:18:33.420
mock that. And especially as
programmers, we have a tendency

388
00:18:33.420 --> 00:18:37.920
to brutally criticize ourselves for
our crappy quality and like

389
00:18:37.920 --> 00:18:40.830
how broken everything always is.
But like, that's actually, that's

390
00:18:40.830 --> 00:18:44.010
not a dystopian future. That's
actually, you know, a trade

391
00:18:44.010 --> 00:18:46.110
off that we're making for
re for good reasons, you

392
00:18:46.110 --> 00:18:48.060
know, we should be conscious
of those and critical of

393
00:18:48.060 --> 00:18:50.670
them. And we shouldn't believe,
you know, certain kind of

394
00:18:50.670 --> 00:18:54.030
utopian pitches for technologies like
the IOT that claimed that

395
00:18:54.030 --> 00:18:55.980
that will never happen. We
should always know that we

396
00:18:55.980 --> 00:18:58.350
should be realistic about it,
but that doesn't, you know,

397
00:18:59.400 --> 00:19:01.990
that's like, there's a worse
as better thing that actually

398
00:19:02.010 --> 00:19:03.750
is better. It's a, it's
a trade off we want

399
00:19:03.750 --> 00:19:06.260
to make. Yeah. I think
it all depends though. And

400
00:19:06.260 --> 00:19:08.600
this is where it's hard
to predict, right? I've got

401
00:19:08.660 --> 00:19:11.450
a wifi light bulbs and
I thought they were going

402
00:19:11.450 --> 00:19:13.790
to be a game changer.
And I recently spent two

403
00:19:13.790 --> 00:19:15.980
solid hours trying to get
the firmware updated on these

404
00:19:15.980 --> 00:19:18.650
light bulbs. And then I
realized, okay, that's stupid wifi,

405
00:19:18.680 --> 00:19:20.870
light bulbs are a dumb
idea, but I've got a

406
00:19:20.870 --> 00:19:24.710
nest, which is a wifi
enabled thermostat. And it's amazing.

407
00:19:24.710 --> 00:19:26.720
And I walked by and
I waved my hand over

408
00:19:26.720 --> 00:19:28.730
it and it shows me
the weather and like, that's,

409
00:19:29.090 --> 00:19:32.060
you know, that works. I
would not have guessed that

410
00:19:32.060 --> 00:19:34.430
I needed a wifi enabled
thermostat. And I definitely wouldn't

411
00:19:34.430 --> 00:19:37.700
have guessed that the light
bulbs would be okay. Yeah.

412
00:19:37.700 --> 00:19:39.590
One of the classic little
metaphors for this, that lives

413
00:19:39.590 --> 00:19:41.780
in my head is like,
whenever you go, I remember

414
00:19:41.810 --> 00:19:43.910
like when I started going
into public bathrooms and there

415
00:19:43.910 --> 00:19:48.110
would be the Persimmony sensors
on the, on the faucets

416
00:19:48.110 --> 00:19:50.180
and on the pound paper
towel dispensers. And I remember

417
00:19:50.180 --> 00:19:52.550
it for a long time
years. I was so angry

418
00:19:52.550 --> 00:19:54.530
every time I interacted with
them because they worked so

419
00:19:54.530 --> 00:19:56.210
badly, you know, they make
you feel like you're like

420
00:19:56.240 --> 00:19:58.160
begging for a paper towel
or for some water to

421
00:19:58.160 --> 00:20:00.200
come out of the tap.
And I thought like, why

422
00:20:00.200 --> 00:20:03.080
is this change happen? Like
this is some ridiculous adoption

423
00:20:03.080 --> 00:20:05.840
of fancy technology that is
just making the experience worse.

424
00:20:05.840 --> 00:20:08.510
And then, you know, maybe
it's obvious to other people,

425
00:20:08.510 --> 00:20:10.130
but then I realized that
it's not about the user

426
00:20:10.130 --> 00:20:12.890
experience. It's about, it's about
sensation, you know, it's about

427
00:20:13.010 --> 00:20:15.830
like, not spreading germs by
touching things. So it's like

428
00:20:16.100 --> 00:20:18.350
that. And that's actually why
they've spread and become ubiquitous.

429
00:20:18.350 --> 00:20:21.170
Cause it's actually far better
on just like an aspect

430
00:20:21.170 --> 00:20:23.570
of that experience I hadn't
really thought through. And so

431
00:20:23.570 --> 00:20:26.240
that works 20% of the
time. Yeah. And so that

432
00:20:26.240 --> 00:20:29.060
idea that like the idea
that there's this curve of

433
00:20:29.090 --> 00:20:34.370
permanent, you know, improving smoothness
transparency of user interface and

434
00:20:34.370 --> 00:20:37.130
that's always the future is
not, you know, I just

435
00:20:37.130 --> 00:20:39.350
isn't me. I just simply
don't believe. Yeah, we definitely

436
00:20:39.350 --> 00:20:43.580
need to see a firmware
where Vegas Vegas contact lenses

437
00:20:43.580 --> 00:20:46.190
get a firmware update. That'd
be brilliant. Let's, let's dig

438
00:20:46.190 --> 00:20:49.030
in a little bit more
about amazing technologies that were

439
00:20:49.090 --> 00:20:51.070
a promise, but maybe only
work a little bit of

440
00:20:51.070 --> 00:20:53.080
the time. You are an
author of a book called

441
00:20:53.080 --> 00:20:56.560
making things see from O'Reilly
you talk about connect and

442
00:20:56.560 --> 00:20:59.560
three D vision and things
like that. Connect feels like

443
00:20:59.560 --> 00:21:03.250
one of those amazing technologies
that sometimes just as weird

444
00:21:03.250 --> 00:21:05.500
stuff, I'll be standing in
front of it. It totally

445
00:21:05.500 --> 00:21:08.500
sees my skeleton. And then
suddenly my right leg is

446
00:21:08.500 --> 00:21:10.690
up on top of my
head and it just shakes

447
00:21:10.690 --> 00:21:14.500
for a while and then
I'm freaking out, but it's

448
00:21:14.500 --> 00:21:17.380
still an interesting and valuable
technology. Why is that so

449
00:21:17.380 --> 00:21:20.500
hard? Why is picking up
motion capturing skeletal captures so

450
00:21:20.530 --> 00:21:23.340
complex? Well, I mean, as
a technology, it's one of

451
00:21:23.340 --> 00:21:24.750
the things that's just kind
of amazing that it works

452
00:21:24.750 --> 00:21:26.160
at all. You know, if
you think about what it's

453
00:21:26.160 --> 00:21:28.620
doing, like three D capture
at a high frame rate

454
00:21:28.620 --> 00:21:31.560
and high resolution, and the
incredible, especially in the current

455
00:21:31.560 --> 00:21:35.220
Microsoft stack, just incredible software
stack, that's going in like

456
00:21:35.490 --> 00:21:38.910
the machine learning and incredible,
just incredible, you know, software

457
00:21:38.910 --> 00:21:42.240
achievements that are going into
making the skeleton detection and

458
00:21:42.240 --> 00:21:44.730
all that stuff. But on
the other hand, I think,

459
00:21:44.730 --> 00:21:46.800
I just don't think about
a lot. And it's, I

460
00:21:46.800 --> 00:21:49.170
think it's not just, it's
not just the reliability of

461
00:21:49.170 --> 00:21:51.510
it in some kind of
objective sense. It's that the

462
00:21:51.510 --> 00:21:54.180
way it breaks is different
than things that we're used

463
00:21:54.180 --> 00:21:56.700
to, you know, we're used
to the way, you know,

464
00:21:56.730 --> 00:21:59.730
Oh, Spinney beach ball on
the Mac, or like, Oh,

465
00:21:59.730 --> 00:22:02.010
this webpage, you know, the
JavaScript is making the Chrome

466
00:22:02.010 --> 00:22:04.530
tab, not responsive. Like there's
like we get used to

467
00:22:05.250 --> 00:22:08.280
the way technological systems fail.
And so when we encounter

468
00:22:08.280 --> 00:22:11.820
new ones, especially ones that
work in very different ways,

469
00:22:11.820 --> 00:22:14.880
like, like the connect is
an example of computer vision

470
00:22:14.880 --> 00:22:17.820
and particularly machine learning systems,
which tend to fail very

471
00:22:17.820 --> 00:22:22.200
differently than traditional interactive software
systems. Is that the way

472
00:22:22.200 --> 00:22:24.780
they fail is just alien.
And so it's not that

473
00:22:24.780 --> 00:22:26.400
they're feeling any worse. It's
just that we don't have

474
00:22:26.400 --> 00:22:28.710
expectations around those. They're not
lit, it's not legible to

475
00:22:28.710 --> 00:22:30.840
us how they're going to
fail. And so a lot

476
00:22:30.840 --> 00:22:33.810
of the good design work
in around those areas has

477
00:22:33.810 --> 00:22:35.970
to be about like explaining
to the user. Like one

478
00:22:35.970 --> 00:22:37.740
of the, one of the
things in the early, when

479
00:22:37.740 --> 00:22:39.780
I started making connect projects,
we just learned right away

480
00:22:39.780 --> 00:22:42.280
is you just have to
show the user what you're,

481
00:22:42.720 --> 00:22:44.610
you know, what you're seeing
of them. Like either the

482
00:22:45.000 --> 00:22:47.520
RGB color image, they can
tell because you have users,

483
00:22:47.520 --> 00:22:49.050
like it's not working something.
And like, they're not standing

484
00:22:49.050 --> 00:22:51.180
on camera because they can't
tell if the camera is

485
00:22:51.180 --> 00:22:53.070
seeing them or you shouldn't
have the depth image and

486
00:22:53.070 --> 00:22:55.410
they don't realize like, Oh,
they're leaning back against the

487
00:22:55.410 --> 00:22:56.640
seat. Or, you know, as
soon as you have to

488
00:22:56.640 --> 00:22:58.980
show them some aspect of
how the system is seeing

489
00:22:58.980 --> 00:23:01.350
them, because it's totally illegible
to them and they have

490
00:23:01.350 --> 00:23:03.450
no mental model, cause they've
never interacted with anything that

491
00:23:03.450 --> 00:23:05.340
sees the world in the
same way. And so, you

492
00:23:05.340 --> 00:23:08.430
know, that's, and that's, you
know, depending on how ubiquitous

493
00:23:08.430 --> 00:23:10.560
the systems become or, or
what you to get a

494
00:23:10.560 --> 00:23:12.600
body of users who use
them a lot like that,

495
00:23:13.470 --> 00:23:15.690
the literacy will improve and
you'll stop having to do

496
00:23:15.690 --> 00:23:18.960
that as much. But especially
in these early adoption periods

497
00:23:18.960 --> 00:23:21.390
or in technology, that's, that's
not, you don't use it.

498
00:23:21.570 --> 00:23:23.310
You don't use it ubiquitous
in your daily life. You

499
00:23:23.310 --> 00:23:27.780
have to teach users. Right.
I've got a, a surface.

500
00:23:27.780 --> 00:23:29.970
And as soon as I
started using a touchscreen, I,

501
00:23:30.150 --> 00:23:33.330
I can't not have a
touch screen anywhere else. And

502
00:23:33.330 --> 00:23:34.650
I think it would have
been touched first, came out.

503
00:23:34.650 --> 00:23:36.390
People thought that it was
going to be a replacement

504
00:23:36.390 --> 00:23:38.850
for something. And really it
just augments things. You go

505
00:23:39.120 --> 00:23:41.550
keyboard, mouse, touch, keyboard, mouse,
touch, and you move your

506
00:23:41.820 --> 00:23:45.150
hands around. I thought that
when, when I saw minority

507
00:23:45.500 --> 00:23:48.170
for the first time, that
gesture based, you know, simple

508
00:23:48.170 --> 00:23:51.980
gestures would be ready now
by 2015, like right now

509
00:23:51.980 --> 00:23:54.410
I'm looking at a Skype
window view. I'd like to

510
00:23:54.410 --> 00:23:56.540
just point at it and
go like that and just

511
00:23:56.540 --> 00:23:58.130
flick it off to the
left and have it jumped

512
00:23:58.130 --> 00:24:01.610
over to the monitors. It's
almost unacceptable that that hasn't

513
00:24:01.610 --> 00:24:05.030
happened yet. And I've tried
the leap motion. I've tried

514
00:24:05.030 --> 00:24:07.700
connect. I've tried the there's
a thing called a MEO,

515
00:24:07.700 --> 00:24:11.800
which connects to your, your
forearm. Yeah. He uses my

516
00:24:11.980 --> 00:24:16.090
electrical. Yeah. It uses my
electrical impulses. Yeah. None of

517
00:24:16.090 --> 00:24:19.030
those things are reliable enough.
They all make amazing demos,

518
00:24:19.030 --> 00:24:21.280
but none of them have
taken over the world. Well,

519
00:24:21.280 --> 00:24:22.960
yeah. And there's other problems
with them too, as somebody

520
00:24:22.960 --> 00:24:25.480
who's done a ton of
connect projects, you know, there's

521
00:24:25.480 --> 00:24:28.900
a famous thing about the
original minority report, which is

522
00:24:28.900 --> 00:24:31.000
that, first of all, you
know, a lot of the

523
00:24:31.000 --> 00:24:33.790
gesture interface vocabulary, John under
Kauflin Hershey had come up

524
00:24:33.790 --> 00:24:35.860
with before the movie, but
not all of it. Some

525
00:24:35.860 --> 00:24:37.330
of it, the gesture interface
stuff you see in the

526
00:24:37.330 --> 00:24:40.930
movie, Tom cruise actually invented
like the gesture of using

527
00:24:41.050 --> 00:24:43.660
one hand down the lane,
the arm to do fast

528
00:24:43.660 --> 00:24:46.450
forward and rewind scrubbing. Like
Tom cruise came up with

529
00:24:46.450 --> 00:24:48.490
that on set because he's
like, I need a precise

530
00:24:48.490 --> 00:24:50.560
gesture to do this. I
can't. And he like, he

531
00:24:50.560 --> 00:24:52.330
was a precise enough actor
that he had to think

532
00:24:52.330 --> 00:24:53.470
it through in the end,
he came up with it

533
00:24:53.470 --> 00:24:55.240
on set. But one of
the other things that happened

534
00:24:55.240 --> 00:24:57.010
on the set of that
movie is that he was

535
00:24:57.010 --> 00:25:01.060
standing for hours recording takes
of the scenes where he's

536
00:25:01.060 --> 00:25:03.280
waving his hands around things
and his arms got so

537
00:25:03.280 --> 00:25:05.410
tired. He couldn't hold them
up anymore. And so in

538
00:25:05.410 --> 00:25:07.150
some of the scenes actually
wearing arm braces under his

539
00:25:07.150 --> 00:25:09.400
shirt that helped hold his
arms up. And like they

540
00:25:09.400 --> 00:25:11.650
had to limit the takes
and stuff so that the

541
00:25:11.650 --> 00:25:14.290
fatigue could let him do
the gestures. And to me,

542
00:25:14.290 --> 00:25:16.030
if I was a, you
know, somebody who worked in

543
00:25:16.030 --> 00:25:18.010
interaction design on that set,
I'd be like, wow, this

544
00:25:18.010 --> 00:25:20.200
is a red flag. Like
this is not something that

545
00:25:20.200 --> 00:25:22.480
people are going to do
daily, daily lives. Like, I

546
00:25:22.480 --> 00:25:23.800
don't know about you, but
I'm not in as good

547
00:25:23.800 --> 00:25:26.500
shape as Tom cruise. And
like, I am not gonna

548
00:25:26.500 --> 00:25:27.730
be able to hold my
arm out in front of

549
00:25:27.730 --> 00:25:29.590
my, in front of myself
for five or eight hours

550
00:25:29.590 --> 00:25:31.180
a day while I'm working.
You know? So I think

551
00:25:31.180 --> 00:25:34.240
you're right about the kind
of hybrid, you know, multimodal

552
00:25:34.240 --> 00:25:36.760
thing of like you move
between and you, you flick

553
00:25:36.760 --> 00:25:39.040
and stuff like that. That's
not the same thing as

554
00:25:39.040 --> 00:25:40.900
doing all of your work
with your arm outstretched in

555
00:25:40.900 --> 00:25:43.420
front of you, right. This
is what they call gorilla

556
00:25:43.420 --> 00:25:45.430
arms. This was what the
gorilla arms was, what people

557
00:25:45.430 --> 00:25:49.420
would say that was going
to cause gesture based motions

558
00:25:49.420 --> 00:25:52.030
to, to just not happen.
But I'm absolutely convinced that

559
00:25:52.030 --> 00:25:55.120
the future is multimodal. And
I don't see it unreasonable

560
00:25:55.120 --> 00:25:57.310
at all for me to
type type swipe, you know,

561
00:25:57.340 --> 00:25:59.320
just flick with my hands.
And then hand goes right

562
00:25:59.320 --> 00:26:02.260
back down to the keyboard.
It feels like the tech

563
00:26:02.500 --> 00:26:05.080
could be there. Like there's
no reason. Maybe it's just

564
00:26:05.080 --> 00:26:07.360
organizational willpower or maybe I
have to go and write

565
00:26:07.360 --> 00:26:09.160
it myself. Doesn't that seem
like a good idea to

566
00:26:09.160 --> 00:26:11.380
just flick them on a
window over to another one.

567
00:26:12.040 --> 00:26:14.080
Yeah. I mean, there are
those, some of those gesture

568
00:26:14.080 --> 00:26:16.180
systems are starting to come
into places where they're really

569
00:26:16.180 --> 00:26:18.580
needed. Like I've seen a
bunch of the best gesture

570
00:26:18.580 --> 00:26:21.040
work I've seen as in
sterile environments, like in surgery

571
00:26:21.040 --> 00:26:22.660
and stuff like that, you
know, there's more and more

572
00:26:22.660 --> 00:26:26.620
telemedicine and like just more
technology in the operating room.

573
00:26:26.860 --> 00:26:28.780
And so I've seen some
work or, and that's an,

574
00:26:28.780 --> 00:26:30.820
obviously an area where like
you can't type on a

575
00:26:30.820 --> 00:26:32.830
keyboard or use a mouse
while you're in a sterile

576
00:26:32.830 --> 00:26:35.770
environment. So there's a really
strong reason to do gestures

577
00:26:35.770 --> 00:26:37.870
and there's money, you know,
you can put in more

578
00:26:37.870 --> 00:26:40.780
expensive systems and more kind
of customized systems. And I

579
00:26:40.780 --> 00:26:42.370
think that's a big part
of it too, is like

580
00:26:42.580 --> 00:26:44.790
the systems going to train
us as much as, you

581
00:26:44.790 --> 00:26:46.500
know, as much as you
were waving your hand, every

582
00:26:46.500 --> 00:26:47.790
time you mentioned that in
the way you want to

583
00:26:47.790 --> 00:26:49.980
do it, you know, like
one thing you learn when

584
00:26:49.980 --> 00:26:51.570
you do computer vision projects
is like, if I can

585
00:26:51.570 --> 00:26:54.210
tell the user exactly how
to do it, and then

586
00:26:54.210 --> 00:26:56.490
I, it will work a
lot better. So constraints, you

587
00:26:56.490 --> 00:26:58.680
know, and that's, that's just,
to me that's more feasible

588
00:26:58.680 --> 00:27:00.780
in an expert system or
like we're in a system

589
00:27:00.780 --> 00:27:04.290
designed to be used by
experts because they can do

590
00:27:04.290 --> 00:27:06.150
some training on it, you
know? And that's, you can't

591
00:27:06.150 --> 00:27:09.510
expect that, you know, for
just a regular, you know,

592
00:27:10.350 --> 00:27:13.230
non informed technology user for
whom something's gotta be legible

593
00:27:13.230 --> 00:27:15.140
and kind of work the
first time, Right. In any

594
00:27:15.140 --> 00:27:18.380
system that does recognition a
constraint vocabulary, whether that be

595
00:27:18.380 --> 00:27:20.900
specific words you can say,
or a specific motion so

596
00:27:20.900 --> 00:27:22.610
that you know what to
look for, you're going to

597
00:27:22.610 --> 00:27:26.750
make a huge difference. One
of the other things Shifting

598
00:27:26.750 --> 00:27:29.630
gears a little bit that
impressed me about the show

599
00:27:29.870 --> 00:27:33.770
was there are Kind of,
if you watch, you can

600
00:27:33.770 --> 00:27:35.630
almost Watch the show twice.
You watch it for the

601
00:27:35.630 --> 00:27:37.370
actors, you watch it for
the story, but then you

602
00:27:37.370 --> 00:27:39.650
go back and you just
watch the background action, just

603
00:27:39.680 --> 00:27:42.170
little subtle things that are
happening. There was one scene

604
00:27:42.380 --> 00:27:45.020
when they're in a park
and a couple walks by

605
00:27:45.200 --> 00:27:47.210
and their kids in a,
in a pram in a,

606
00:27:47.210 --> 00:27:51.110
in a, in a little
pushcart and it's an interracial

607
00:27:51.110 --> 00:27:54.290
couple, it's a gay couple
and the kid is inside

608
00:27:54.290 --> 00:27:57.770
the pram, but the pram
has a translucent LCD iPad

609
00:27:57.770 --> 00:28:00.620
looking thing and their plan,
some boxes in lines game.

610
00:28:00.830 --> 00:28:04.490
So you've got know future
of changing social mores, and

611
00:28:04.490 --> 00:28:08.600
you've got the, the transparent
LCDs. I think the pram

612
00:28:08.600 --> 00:28:11.660
is driving itself And the
Paramus driving itself all condensed

613
00:28:11.660 --> 00:28:14.540
into one thing I, that
has to have been completely

614
00:28:14.540 --> 00:28:17.060
planned out and scripted. Yeah,
absolutely. And that's the, I

615
00:28:17.060 --> 00:28:19.130
mean, that's the, almost the
opposite of what I was

616
00:28:19.130 --> 00:28:22.640
saying about the difficulty of
showing, you know, interface and

617
00:28:22.640 --> 00:28:25.790
the kind of the superficial
aspects of technology, you know,

618
00:28:25.790 --> 00:28:28.850
the, in a, in a
televisual environment is that, you

619
00:28:28.850 --> 00:28:30.350
know, it's hard to show
the internet, but there's certain

620
00:28:30.350 --> 00:28:33.080
other kinds of changes that
can be so dense and

621
00:28:33.080 --> 00:28:35.030
you can just show them
so quickly. And a lot

622
00:28:35.030 --> 00:28:37.220
of those, like the, one
of those I care the

623
00:28:37.220 --> 00:28:40.130
most about honestly, is the,
the demographic one. You know,

624
00:28:40.130 --> 00:28:41.960
that's a huge thing for
us that we think about

625
00:28:41.960 --> 00:28:45.380
a lot. And it comes
from real demographic research in

626
00:28:45.380 --> 00:28:48.650
terms of, if you look
at like what the age

627
00:28:49.220 --> 00:28:52.730
race, you know, gender distributions
of people in different jobs

628
00:28:52.730 --> 00:28:54.470
and of in living in
American cities will be in

629
00:28:54.470 --> 00:28:57.080
2050, you know, when it
comes to futurism, you almost

630
00:28:57.080 --> 00:28:59.930
never know anything for certain,
but demographics is one of

631
00:28:59.930 --> 00:29:01.730
the few things we actually
can say some things with

632
00:29:01.730 --> 00:29:04.130
some confidence, like there's not
going to be more 50

633
00:29:04.130 --> 00:29:06.200
year olds and 20, 65
than there are people born

634
00:29:06.200 --> 00:29:09.770
in 2015. You know, I
don't, I don't say a

635
00:29:09.770 --> 00:29:11.000
lot of things with confidence.
So I can say that

636
00:29:11.000 --> 00:29:12.880
one with a lot of
confidence, you know, they're, they're

637
00:29:12.890 --> 00:29:14.210
going to be fewer, but
they're not going to be

638
00:29:14.210 --> 00:29:16.730
more. And that's an area
we just know a lot

639
00:29:16.730 --> 00:29:18.830
about like, you know, the
future is going to be

640
00:29:18.830 --> 00:29:22.130
more multi-racial and it's going
to be more equitable in

641
00:29:22.130 --> 00:29:26.510
terms of sexual orientation and
sexual identification. And, you know,

642
00:29:26.510 --> 00:29:28.670
it's going to be more
equal for men and women

643
00:29:28.670 --> 00:29:30.890
in the workplace and all
of these things. And so

644
00:29:30.890 --> 00:29:33.140
that's just a way that
we can very easily show

645
00:29:33.410 --> 00:29:36.170
it's the future, just in
terms of casting. So like

646
00:29:36.320 --> 00:29:43.480
we cast all non nonspecifically,
mentioned parts, racial, blind, and

647
00:29:43.480 --> 00:29:46.810
gender blind, and, you know,
and we do, and we

648
00:29:46.810 --> 00:29:49.900
really emphasize, we push constantly
with our production teams, for

649
00:29:49.900 --> 00:29:53.440
the background actors to get,
you know, really diverse background

650
00:29:53.440 --> 00:29:56.320
actors. Cause I think it's
a really subtle visual cue

651
00:29:56.350 --> 00:29:59.980
clue that lets you know,
you're in the future and,

652
00:30:00.370 --> 00:30:02.560
and yeah, the, the visual
storytelling and all of those

653
00:30:02.560 --> 00:30:05.590
moments and taking small details
and letting them tell you

654
00:30:05.590 --> 00:30:08.110
something bigger about the world
is something that we, we

655
00:30:08.110 --> 00:30:09.520
try really hard to do.
One of my favorite ones

656
00:30:09.520 --> 00:30:12.040
like that is in the
first episode, which is that

657
00:30:12.040 --> 00:30:14.890
the, the episode which features
the candidate for the mayor

658
00:30:14.890 --> 00:30:17.140
of DC. And at one
point he mentioned that he

659
00:30:17.140 --> 00:30:19.690
was a professional football player
and he played for the

660
00:30:19.690 --> 00:30:24.010
Washington football team, which is
the Washington red clouds. And

661
00:30:24.010 --> 00:30:26.290
that was came out of,
you know, obviously today the

662
00:30:26.410 --> 00:30:28.420
Washington football team is named
the Redskins, which is a,

663
00:30:28.690 --> 00:30:30.310
you know, a name with
a racist history that there's

664
00:30:30.310 --> 00:30:33.280
a lot of resistance and
pressure to change it. And

665
00:30:33.280 --> 00:30:35.860
so I looked up what,
you know, what people who

666
00:30:35.860 --> 00:30:37.300
advocate for changing it, what
would they want to change

667
00:30:37.300 --> 00:30:39.100
it to? And they want
a name after red cloud

668
00:30:39.100 --> 00:30:44.450
who's was a native leader.
And you know, that, I

669
00:30:44.500 --> 00:30:46.270
believe that that was something
that will change in that

670
00:30:46.270 --> 00:30:48.640
time. And actually this is
an area also, it gets

671
00:30:48.640 --> 00:30:50.500
into kind of a wider
sense of how we try

672
00:30:50.500 --> 00:30:52.780
to think about the future,
which is, you know, a

673
00:30:52.780 --> 00:30:55.360
lot of science fiction. You
want to be critical of

674
00:30:55.360 --> 00:30:56.740
the present. It's like one
of the things you want

675
00:30:56.740 --> 00:30:58.450
to do, you want to
like look at, what's not,

676
00:30:58.720 --> 00:31:01.090
what's wrong about the president
and talk about what's wrong

677
00:31:01.090 --> 00:31:04.600
with it and identify those
problems. And in the last

678
00:31:04.750 --> 00:31:07.390
30 years, especially the main
mode for doing that has

679
00:31:07.390 --> 00:31:10.510
been dystopia. It's been a
way of taking the negative

680
00:31:10.540 --> 00:31:13.390
things that are bad today
and, and pushing them out

681
00:31:13.390 --> 00:31:15.970
further and imagining kind of
a reductio ad absurdum of

682
00:31:16.000 --> 00:31:17.800
them. And I think that's,
you know, some of that's

683
00:31:17.800 --> 00:31:20.590
been very powerful, like one
that comes to mind for

684
00:31:20.590 --> 00:31:22.750
me is children of men,
which shows a kind of

685
00:31:22.990 --> 00:31:27.120
security state dystopia. That's just
very visceral and powerful. That's

686
00:31:27.120 --> 00:31:29.550
such an amazing movie for
anyone who's listening, like please

687
00:31:29.760 --> 00:31:31.800
children of men. Like you
might as well just pause

688
00:31:31.800 --> 00:31:35.580
the podcast now and go
and rent. Yeah, absolutely. And

689
00:31:35.580 --> 00:31:38.070
so, but I also think
that as you know, and

690
00:31:38.070 --> 00:31:40.320
I'm a huge fan in
literature, in Saifai literature of

691
00:31:40.350 --> 00:31:43.740
cyber punk, you know, from
William Gibson and verse Sterling and

692
00:31:43.740 --> 00:31:46.410
Neil Stevenson and in the
early eighties, you know, going

693
00:31:46.410 --> 00:31:48.630
forward. But I actually, I
think that to some extent

694
00:31:48.840 --> 00:31:51.540
that that has become such
a ubiquitous tone and mode

695
00:31:51.540 --> 00:31:53.730
and science fiction, that it
doesn't have the impact it

696
00:31:53.730 --> 00:31:55.500
used to have. Like, it
doesn't feel critical anymore. It

697
00:31:55.500 --> 00:31:58.320
just feels depressing. People bounce
off of it. So then

698
00:31:58.320 --> 00:32:01.230
the question was, we want
to tell this, like present

699
00:32:01.230 --> 00:32:03.720
this complex world is not
safe. It's not star Trek.

700
00:32:03.750 --> 00:32:06.900
It's not just a utopia
it's it's, but it's optimistic.

701
00:32:07.650 --> 00:32:09.480
You know, it's as optimistic
as I feel about the

702
00:32:09.480 --> 00:32:11.430
present and, but we still
want to be able to

703
00:32:11.430 --> 00:32:13.530
be critical. So that's a
kind of conundrum that we

704
00:32:13.530 --> 00:32:16.650
faced. And I think things
like that, red clouds example

705
00:32:16.650 --> 00:32:19.410
and the diversity, our example,
that something that we've really

706
00:32:19.410 --> 00:32:21.750
fixed on as a strategy
for that, that I'm, I'm

707
00:32:21.750 --> 00:32:23.760
really excited about. And I
want to see other science

708
00:32:23.760 --> 00:32:26.160
fiction properties do, which is
the way you criticize the

709
00:32:26.160 --> 00:32:29.490
present is you show it
being exceeded. You show the

710
00:32:29.490 --> 00:32:31.680
future of being better. You
see these problems that were

711
00:32:31.680 --> 00:32:34.140
stuck, that we can't get
past today, that we're constantly

712
00:32:34.530 --> 00:32:36.930
like you see them just
improve. You see like, Oh,

713
00:32:36.930 --> 00:32:40.280
there's just, you know, multiracial
couples and you gender diversity

714
00:32:40.280 --> 00:32:42.230
and people don't and fewer
people have a problem with

715
00:32:42.230 --> 00:32:44.510
it. And you know, it's
not being Pollyanna about that

716
00:32:44.510 --> 00:32:47.240
in the present. It's modeling
a world that can have

717
00:32:47.240 --> 00:32:50.000
that and can get past
the kind of difficult problems.

718
00:32:50.000 --> 00:32:51.770
And there is a route
to that and we're possible

719
00:32:51.890 --> 00:32:54.830
showing what that route is
by being specific and precise

720
00:32:54.890 --> 00:32:58.420
about the way you do
that. Very cool. Very cool.

721
00:32:58.690 --> 00:33:00.760
Well, I really, really appreciate
having you on the show

722
00:33:00.760 --> 00:33:03.460
today. And I encourage folks
to, to watch a minority

723
00:33:03.460 --> 00:33:05.770
report. It's really a really
a great show and I'm

724
00:33:05.770 --> 00:33:07.810
enjoying it very much. Thanks
a lot, Scott. Thanks for

725
00:33:07.810 --> 00:33:10.690
having me. It's been great
to be here. Absolutely. This

726
00:33:10.690 --> 00:33:12.970
has been another episode of
Hanselminutes and we'll see you

727
00:33:12.970 --> 00:33:13.930
again next week.

