WEBVTT FILE

1
00:00:00.480 --> 00:00:03.240
Hey friends. I want to
thank our sponsor. Reagan, are

2
00:00:03.240 --> 00:00:05.880
you struggling to replicate the
bugs and performance issues that

3
00:00:05.880 --> 00:00:09.120
your customers are reporting? Plug
Reagan into your web and

4
00:00:09.120 --> 00:00:12.780
mobile applications right now, and
diagnose problems in minutes rather

5
00:00:12.780 --> 00:00:16.110
than hours. Kiss goodbye. Having
to dig through log files

6
00:00:16.110 --> 00:00:19.740
and relying on frustrated users
to report issues, make your

7
00:00:19.740 --> 00:00:23.400
software development life so much
easier. Using Reagan's error, crash,

8
00:00:23.400 --> 00:00:27.510
and performance monitoring tools. Every
software team can create flawless

9
00:00:27.510 --> 00:00:30.930
software experiences for their customers
with Reagan. Try it free

10
00:00:30.930 --> 00:00:49.890
today@raygun.com. That's our a Y
G U n.com. Hey friends,

11
00:00:49.890 --> 00:00:52.020
this is Scott Hanselman. This
is another episode of Hansel

12
00:00:52.020 --> 00:00:54.480
minutes today. I'm talking with
Melanie and sciencey, the head

13
00:00:54.480 --> 00:00:58.350
of security and privacy communications
at Uber, but more importantly,

14
00:00:58.350 --> 00:01:01.340
she's a shark diver. How
are you? I'm good, Scott.

15
00:01:01.370 --> 00:01:03.800
Thank you so much for
having me. I love your

16
00:01:03.800 --> 00:01:08.180
Twitter, but it's a very
interesting mix between like, you

17
00:01:08.180 --> 00:01:10.790
know, privacy and security and
like Def con type stuff

18
00:01:10.880 --> 00:01:14.960
and sharks what's with the
sharks. Yeah, so I actually

19
00:01:14.960 --> 00:01:19.010
started college as a Marine
biology major as from a

20
00:01:19.010 --> 00:01:21.200
really young age as a
kid. I wanted to be

21
00:01:21.200 --> 00:01:24.830
a shark scientist. And so
while it is not what

22
00:01:24.830 --> 00:01:27.860
I ended up doing professionally,
it is still a really

23
00:01:27.860 --> 00:01:31.450
big part of my life.
And, you know, folks who

24
00:01:31.490 --> 00:01:34.010
do follow me on Twitter
will get to see all

25
00:01:34.010 --> 00:01:36.890
kinds of like photos and
videos from the shark dives

26
00:01:36.890 --> 00:01:39.200
that I do from time
to time. It's kind of

27
00:01:39.200 --> 00:01:43.400
my way to, to recharge
and take a break in.

28
00:01:43.790 --> 00:01:45.860
And when I need a
vacation from all of the

29
00:01:45.950 --> 00:01:50.990
security and privacy drama, swimming
with sharks is far more

30
00:01:50.990 --> 00:01:55.010
relaxing. The irony is thick,
dealing with sharks online, and

31
00:01:55.010 --> 00:01:56.540
then I need to relax.
I'm going to go and

32
00:01:56.540 --> 00:01:59.810
swim with some sharks. Yeah.
I mean, to be honest,

33
00:01:59.840 --> 00:02:03.380
the, the sharks in the
ocean are far lower risk

34
00:02:05.960 --> 00:02:09.320
And that is absolutely true,
but so this is a

35
00:02:09.320 --> 00:02:13.250
hobby, but you, you didn't
like you didn't abandon your

36
00:02:13.250 --> 00:02:16.250
degree. Like you're probably like
doing low key amateur research

37
00:02:16.250 --> 00:02:19.610
on sharks. I mean, I
think so. I think I'm

38
00:02:19.610 --> 00:02:22.580
more following and learning from
the research that all of

39
00:02:22.580 --> 00:02:25.970
my friends are doing that
was, that actually completed our

40
00:02:25.970 --> 00:02:28.790
program and, and went on
to actually become, you know,

41
00:02:28.790 --> 00:02:33.140
PhDs in Marine biology and
ecology and, and those related

42
00:02:33.140 --> 00:02:38.570
fields. But I do draw
a lot of experiences and

43
00:02:38.600 --> 00:02:43.250
insights from that time in
my life. And the research

44
00:02:43.250 --> 00:02:45.200
that's come out of that
field in the way that

45
00:02:45.200 --> 00:02:48.590
I do my job now
in terms of communications for

46
00:02:48.920 --> 00:02:53.600
security and privacy issues. So
while I may not spend

47
00:02:53.600 --> 00:02:55.430
as much time in the
ocean, as I would like

48
00:02:55.430 --> 00:02:57.800
to on a day to
day basis, like some of

49
00:02:57.800 --> 00:03:02.440
my friends who are professional
researchers, a lot of, I,

50
00:03:02.560 --> 00:03:04.900
like I said, I draw
a lot from that field

51
00:03:05.500 --> 00:03:08.700
in the role that I
do have. Now You were

52
00:03:08.700 --> 00:03:11.880
an adjunct professor at Boston
university. You worked at Facebook,

53
00:03:11.880 --> 00:03:14.910
you've worked with Def con
for the world's largest hacker

54
00:03:14.910 --> 00:03:17.400
conference for many years. And
now you're at, at Uber.

55
00:03:17.580 --> 00:03:20.850
What got you excited about
kind of cybersecurity in general,

56
00:03:21.600 --> 00:03:25.410
Morbid curiosity, I think at
first, but it wasn't a

57
00:03:25.410 --> 00:03:31.950
field that I intentionally went
into after I shifted away

58
00:03:31.950 --> 00:03:35.610
from Marine biology. I started
doing more kind of environmental

59
00:03:35.610 --> 00:03:43.050
communications and started getting into
technology communications through kind of

60
00:03:43.050 --> 00:03:46.560
a field of corporate sustainability.
And at the time that

61
00:03:46.560 --> 00:03:49.230
I, you know, entered the
field like software was coming

62
00:03:49.230 --> 00:03:52.410
into play in terms of
doing environmental, like footprint analysis

63
00:03:52.410 --> 00:03:54.930
and things like that. So
working with those types of

64
00:03:54.930 --> 00:03:58.890
companies from the beginning, I
was kind of, you know,

65
00:03:58.890 --> 00:04:04.320
making kind of the step
into the technology space and

66
00:04:05.190 --> 00:04:09.540
through a chance kind of
set up of different circumstances.

67
00:04:10.470 --> 00:04:13.650
I ended up working with
the chief security office at,

68
00:04:13.650 --> 00:04:17.610
at and T for about
six years. And that's really

69
00:04:17.610 --> 00:04:21.210
where I cut my teeth
in security, was working with

70
00:04:21.240 --> 00:04:27.750
their engineers, their product solutions
team, understanding not just what

71
00:04:27.750 --> 00:04:31.800
engineers do in regards to
security, but also businesses and

72
00:04:31.800 --> 00:04:35.070
companies in terms of how
do they invest in this

73
00:04:35.070 --> 00:04:38.970
area from a product perspective
and a procurement perspective. So

74
00:04:39.030 --> 00:04:42.810
I didn't seek out cybersecurity
the way that, you know,

75
00:04:42.810 --> 00:04:46.380
like a CS major might
or a computer science major.

76
00:04:47.490 --> 00:04:50.460
But, you know, when I
started working with this team,

77
00:04:50.460 --> 00:04:52.950
it was almost kind of
love at first sight. It

78
00:04:52.950 --> 00:04:56.250
hit all of the buttons
that I had as a

79
00:04:56.250 --> 00:04:59.190
nerd growing up as a
kid. You know, my, my

80
00:04:59.190 --> 00:05:01.380
dad did teach me to
code when I was young.

81
00:05:01.920 --> 00:05:05.070
It was something that I
really enjoyed doing. And so,

82
00:05:05.130 --> 00:05:08.070
you know, being able to
see more of the behind

83
00:05:08.070 --> 00:05:11.370
the scenes of how some
of these technologies and software

84
00:05:11.370 --> 00:05:13.920
that I was using as
a consumer, how all of

85
00:05:13.920 --> 00:05:16.500
this was working was very
interesting to me. And I

86
00:05:16.500 --> 00:05:19.500
was really intrigued by it.
And at the same time,

87
00:05:19.500 --> 00:05:24.030
working in the field of
security and privacy enabled me

88
00:05:24.030 --> 00:05:27.090
to do something that has
always been really important to

89
00:05:27.090 --> 00:05:29.880
me, which is to help
other people. That's cool. You

90
00:05:29.880 --> 00:05:32.370
found your tribe. Yeah. And
there's, you know, there's a

91
00:05:32.370 --> 00:05:35.190
cause behind what a lot
of us do right there.

92
00:05:35.190 --> 00:05:38.730
We're, we're not totally in
this for ourselves. And that

93
00:05:38.730 --> 00:05:44.070
makes it, I think, a
lot more sustainable in terms

94
00:05:44.070 --> 00:05:47.550
of the high pressure situations
that many of us are

95
00:05:47.550 --> 00:05:50.040
in from time to time
because of, you know, what

96
00:05:50.040 --> 00:05:53.220
we do for a living.
And, you know, when you

97
00:05:53.220 --> 00:05:56.970
realize that you're part of
something bigger that really drives,

98
00:05:57.080 --> 00:06:00.310
motivates you, How do, how
do companies, or how should

99
00:06:00.310 --> 00:06:05.050
companies find balance between being
secure and being private, but

100
00:06:05.050 --> 00:06:08.380
also like mining the data
that they, that they have.

101
00:06:08.380 --> 00:06:11.290
Like, I understand that I
am the product when it

102
00:06:11.290 --> 00:06:13.180
comes to Facebook or Google
or whatever, they look at

103
00:06:13.180 --> 00:06:16.120
every click, not actively, but
they are the metadata that

104
00:06:16.120 --> 00:06:19.240
I am consuming by simply
being by simply looking at

105
00:06:19.240 --> 00:06:22.540
a photo, we always have
that creepy experiment experience. When

106
00:06:22.540 --> 00:06:24.850
you look at something on
Amazon and then suddenly every

107
00:06:24.850 --> 00:06:26.560
website in the world wants
to sell you that thing

108
00:06:26.560 --> 00:06:31.720
you looked at, I want
to understand like security and

109
00:06:31.720 --> 00:06:34.120
privacy. People are trying to
lock things down, but I

110
00:06:34.120 --> 00:06:36.130
assume that the corporations are
trying to open things up.

111
00:06:36.130 --> 00:06:37.870
Like they want to look
at that data, but you

112
00:06:37.870 --> 00:06:40.930
want to keep it private.
So I think to be

113
00:06:40.930 --> 00:06:46.060
honest, I don't think it's
a binary choice. I think

114
00:06:46.060 --> 00:06:51.070
there's, there is a lot
of options along that spectrum.

115
00:06:51.070 --> 00:06:54.100
And I think what companies
are starting to realize now,

116
00:06:54.490 --> 00:06:57.100
and, and I think more
folks in the community will

117
00:06:57.100 --> 00:07:00.010
realize it over time is
that there doesn't have to

118
00:07:00.010 --> 00:07:04.300
be this all or nothing
experience. And that, you know,

119
00:07:04.300 --> 00:07:08.170
creating a product where people
do have the option in

120
00:07:08.170 --> 00:07:12.460
the control to choose the
experience that they're most comfortable

121
00:07:12.460 --> 00:07:15.370
with, I think is really
where, you know, the, the

122
00:07:15.370 --> 00:07:18.880
smartest, most innovative companies are
going. There's, you know, a

123
00:07:18.880 --> 00:07:22.030
number of tech companies who
are, you know, in response

124
00:07:22.030 --> 00:07:25.720
to proposed privacy legislation and
things are, you know, bringing

125
00:07:25.720 --> 00:07:28.090
up the issue of innovation,
you know, and whether or

126
00:07:28.090 --> 00:07:30.580
not all of these new
laws and rules are going

127
00:07:30.580 --> 00:07:35.080
to hamper innovation, particularly in
the technology sector. And the

128
00:07:35.080 --> 00:07:39.070
reality is, is in 2019,
you can't have innovation without

129
00:07:39.070 --> 00:07:44.080
privacy. We simply should not
be accepting technology that doesn't

130
00:07:44.080 --> 00:07:47.890
have privacy protections in it
as a standard of innovation.

131
00:07:48.850 --> 00:07:51.430
And so, you know, a
lot of where I think

132
00:07:51.430 --> 00:07:54.820
the new innovation is going
to come from is, you

133
00:07:54.820 --> 00:07:58.330
know, more along, like I
said, along this spectrum and

134
00:07:58.330 --> 00:08:02.590
giving people more choices so
that you can use some

135
00:08:02.590 --> 00:08:05.050
of these platforms or other
tools in the way that

136
00:08:05.380 --> 00:08:09.400
suits your needs and where
you have more choice and

137
00:08:09.400 --> 00:08:15.040
control upfront about what data
is collected, what it's used

138
00:08:15.040 --> 00:08:19.480
for. And I think part
of this also reflects the

139
00:08:19.510 --> 00:08:26.560
growing awareness and public consciousness
of this issue. 10 years

140
00:08:26.560 --> 00:08:30.040
ago, people were willing to,
you know, to say yes

141
00:08:30.040 --> 00:08:32.830
to something, cause they didn't
particularly understand the risks that

142
00:08:32.830 --> 00:08:36.160
were involved. And so, you
know, when we, a lot

143
00:08:36.160 --> 00:08:37.810
of us, when we first
signed up for, you know,

144
00:08:37.810 --> 00:08:42.010
like a social media account
or an Amazon account, you

145
00:08:42.010 --> 00:08:44.710
know, it, it was more
of, I think what we

146
00:08:44.710 --> 00:08:47.410
can think of in that
traditional sense of like, am

147
00:08:47.410 --> 00:08:49.570
I willing to trade my
data for this convenience or

148
00:08:49.570 --> 00:08:53.140
for this service? And fortunately,
we're now getting to the

149
00:08:53.140 --> 00:08:57.690
point, not just with, but
as a society where we

150
00:08:57.690 --> 00:09:02.730
can have a more sophisticated
and nuanced conversation about what

151
00:09:02.730 --> 00:09:05.790
types of conveniences do you
want. Right. So again, moving

152
00:09:05.790 --> 00:09:10.920
away from that all or
nothing model in creating more,

153
00:09:11.370 --> 00:09:15.180
more options so that, you
know, somebody who is comfortable

154
00:09:15.600 --> 00:09:18.630
or who perceives value or
trade offs a little bit

155
00:09:18.630 --> 00:09:22.320
differently can go to one
end of the spectrum if

156
00:09:22.320 --> 00:09:24.960
they want. But I think
most of us are actually

157
00:09:24.960 --> 00:09:27.450
going to fall somewhere in
the middle, right. Where you

158
00:09:27.450 --> 00:09:32.100
are I'm okay. Giving Amazon
parts of my information because

159
00:09:32.190 --> 00:09:34.170
you know, of the service
or experience that I have

160
00:09:34.170 --> 00:09:36.900
with them, that doesn't necessarily
mean that I'm comfortable with

161
00:09:36.900 --> 00:09:40.740
them, shared it with all
of their partners. Right. And

162
00:09:40.740 --> 00:09:43.320
so I think we're, like
I said, we're moving in,

163
00:09:44.130 --> 00:09:46.680
in what I think is
a positive direction in terms

164
00:09:46.680 --> 00:09:50.340
of the level of sophistication
that we're able to have

165
00:09:50.340 --> 00:09:53.670
in our conversations, not just
as customers and consumers, but

166
00:09:53.970 --> 00:09:58.320
you know, also with our
policymakers and regulators, and this

167
00:09:58.320 --> 00:10:00.840
is, you know, it's a
trend that's happened, you know,

168
00:10:00.840 --> 00:10:04.170
in other industries we've, you
know, happened in security first,

169
00:10:04.200 --> 00:10:07.950
I think, and now privacy
is, is following suit. And,

170
00:10:08.460 --> 00:10:09.810
you know, a lot of
that has to do with

171
00:10:09.810 --> 00:10:14.010
the fact that privacy is
moving out of just legal

172
00:10:14.010 --> 00:10:17.490
conversation. We used to talk
all the time about how

173
00:10:17.490 --> 00:10:20.280
security was like coming out
of the corners of the

174
00:10:20.280 --> 00:10:24.150
it department and now becoming
mainstream. Right. And privacy is

175
00:10:24.150 --> 00:10:27.300
going through a similar evolution
right now. You know, it's

176
00:10:27.300 --> 00:10:31.490
just that it's origin started
in the legal department. It

177
00:10:31.490 --> 00:10:34.220
feels like one of the
challenges with privacy and security

178
00:10:34.220 --> 00:10:36.320
and I'm gonna use them
as a group for now

179
00:10:36.650 --> 00:10:41.090
is the proactive versus the
reactive where everything's totally fine

180
00:10:41.450 --> 00:10:45.800
until one minute it's dramatically
in catastrophic, not fine. Like

181
00:10:46.130 --> 00:10:49.250
I might have a slider
bar that describes my privacy

182
00:10:49.250 --> 00:10:53.210
preference, like we're talking about,
and my nontechnical parent might

183
00:10:53.210 --> 00:10:56.000
set that privacy bar somewhere,
but then something happens that

184
00:10:56.000 --> 00:11:00.050
makes them feel gross. And
then they immediately flip out

185
00:11:00.080 --> 00:11:03.770
because of lack of understanding,
lack of education, how do

186
00:11:03.770 --> 00:11:07.010
we build controls and systems
such that we don't always

187
00:11:07.580 --> 00:11:10.460
take a great int like
you described and flip it

188
00:11:10.460 --> 00:11:13.190
into a binary, because like
you say, it's great until

189
00:11:13.190 --> 00:11:14.600
it's totally not great. I
didn't know. You knew that

190
00:11:14.600 --> 00:11:17.780
about me and now I'm
upset. Sure. So I think,

191
00:11:17.900 --> 00:11:20.240
you know, the way that
you described this in terms

192
00:11:20.240 --> 00:11:25.580
of being proactive is exactly
kind of the, the remedy

193
00:11:25.580 --> 00:11:28.220
here. And, and I don't
mean proactive in terms of

194
00:11:28.220 --> 00:11:32.330
just offering that slider bar
or that, that setting, right?

195
00:11:32.330 --> 00:11:35.510
Like there, there are companies
and platforms that have dozens

196
00:11:35.510 --> 00:11:40.490
and dozens of privacy settings,
and that it's really overwhelming

197
00:11:40.820 --> 00:11:43.430
for the average consumer. They,
you know, to just look

198
00:11:43.430 --> 00:11:46.220
at this laundry list of
options, some of them, you

199
00:11:46.220 --> 00:11:49.850
may not even understand. And
so we think part of,

200
00:11:50.000 --> 00:11:53.830
you know, this, this notion
of being proactive that has

201
00:11:53.830 --> 00:11:59.320
to do with first understanding
what the expectations of your

202
00:11:59.320 --> 00:12:03.070
users and customers are, if
you can imagine, even in

203
00:12:03.070 --> 00:12:06.430
our personal lives, it's a
very jarring when somebody does

204
00:12:06.430 --> 00:12:10.210
something that seems out of
character or when somebody doesn't

205
00:12:10.210 --> 00:12:13.570
live up to your expectations,
you know, it's easy to

206
00:12:13.570 --> 00:12:17.320
get to support. It's easy
to feel violated. And, you

207
00:12:17.320 --> 00:12:19.840
know, there's a lot of
things that we can, you

208
00:12:19.840 --> 00:12:24.190
know, that we experience or
learn in our interpersonal relationships

209
00:12:24.460 --> 00:12:28.420
that also apply to the
relationship that companies and brands

210
00:12:28.420 --> 00:12:32.170
have with their customers. So
it shouldn't surprise us that,

211
00:12:32.350 --> 00:12:36.220
you know, my, my friends
or family, you know, may

212
00:12:36.220 --> 00:12:39.130
feel uncomfortable when I do
something that seems out of

213
00:12:39.130 --> 00:12:43.510
character in the same way
that my customers may get

214
00:12:43.510 --> 00:12:46.540
really uncomfortable if you know,
my company or my team

215
00:12:46.540 --> 00:12:49.960
does something that they didn't
expect from us. Right. And

216
00:12:49.960 --> 00:12:53.680
so there's a lot of
just fun, foundational and fundamental

217
00:12:53.680 --> 00:12:58.210
things in terms of human
interaction and communication that is

218
00:12:58.210 --> 00:13:03.220
now being applied in B
technology space and in bringing

219
00:13:03.220 --> 00:13:06.520
in a lot more expertise
from other disciplines other than

220
00:13:06.520 --> 00:13:11.260
just the strict engineering or
product development track. Right. I

221
00:13:11.260 --> 00:13:14.380
mean, I do not have
a technical background. Like I

222
00:13:14.380 --> 00:13:17.560
said, I started in Marine
biology. My master's degree is

223
00:13:17.560 --> 00:13:22.870
in communications. This is, you
know, different disciplines who understand

224
00:13:22.870 --> 00:13:26.440
different aspects of humanity who
are coming into tech to

225
00:13:26.440 --> 00:13:30.100
say, you know, we have,
we have some information, we

226
00:13:30.100 --> 00:13:32.860
have some knowledge that can
actually help you be better

227
00:13:32.860 --> 00:13:34.860
at what you're trying to
do. You're bringing up a

228
00:13:34.860 --> 00:13:38.130
really interesting point is I've
been writing about interpersonal relationships

229
00:13:38.130 --> 00:13:40.680
and I'm starting to figure
out obvious things. I think

230
00:13:40.680 --> 00:13:43.140
we've all figured out is
that when expectations are not

231
00:13:43.140 --> 00:13:46.050
met, that's when anger and
fear kind of happen. Like

232
00:13:46.050 --> 00:13:48.690
if you, if I understood
ahead of time that X,

233
00:13:48.690 --> 00:13:51.540
Y, Z thing was going
to happen, then I would

234
00:13:51.540 --> 00:13:54.210
feel some kind of way,
but I was surprised. And

235
00:13:54.210 --> 00:13:57.210
it's that surprise? It's that
email that, Oh, all my

236
00:13:57.210 --> 00:14:00.690
information at this hotel chain
is now public. Wow. That

237
00:14:00.960 --> 00:14:03.870
does not meet my expectations.
Yeah. I mean, like who

238
00:14:03.870 --> 00:14:06.450
knew that Marriott was actually
holding on to all of

239
00:14:06.450 --> 00:14:09.360
those passport numbers for that
long? I thought they just

240
00:14:09.360 --> 00:14:11.010
wanted it so I could
check in, they don't need

241
00:14:11.010 --> 00:14:14.820
it after. Yup. And so,
you know, this is, you

242
00:14:14.820 --> 00:14:17.700
know, the, like I said,
these, these companies that are

243
00:14:17.700 --> 00:14:20.670
a little bit more forward
thinking in, in really focusing

244
00:14:20.670 --> 00:14:24.090
on their customers and their
expectations of their users, you

245
00:14:24.090 --> 00:14:29.190
know, getting, having a pulse
on what their expectations are

246
00:14:29.190 --> 00:14:31.890
and how they feel about
certain things. You know, I

247
00:14:31.890 --> 00:14:34.170
have conversations with my teams
all the time where I'm

248
00:14:34.170 --> 00:14:37.680
like, look, our customers don't
expect that we have this

249
00:14:37.680 --> 00:14:41.280
information or that we're doing
this certain thing with it.

250
00:14:41.280 --> 00:14:46.170
So why, why are we
right? And it puts more

251
00:14:46.170 --> 00:14:49.080
onus on the technical teams
to, to justify all of

252
00:14:49.080 --> 00:14:53.540
that behavior. And in some,
you know, that isn't enough

253
00:14:53.570 --> 00:14:55.670
feedback for them to say,
you know, you're absolutely right.

254
00:14:55.670 --> 00:14:58.250
Let's just kill it. And
then, you know, the project

255
00:14:58.250 --> 00:15:00.440
goes away or the collection
goes away. I mean, we've

256
00:15:00.440 --> 00:15:04.010
moved, removed a number of
features even within our own

257
00:15:04.010 --> 00:15:07.700
mobile app based on feedback
from our users on their

258
00:15:07.700 --> 00:15:13.670
expectations. And, you know, so
again, it's, you have to

259
00:15:13.670 --> 00:15:18.050
proactively understand who you're building
things for and, and what

260
00:15:18.050 --> 00:15:20.200
they're expecting from you. Well,
not to put you on

261
00:15:20.200 --> 00:15:22.420
the spot too much, but
you had a great tweet.

262
00:15:22.600 --> 00:15:25.570
One of those real talk
type tweets where you said

263
00:15:25.570 --> 00:15:29.200
it's not engineers who cause
the worst privacy issues it's

264
00:15:29.200 --> 00:15:33.430
MBAs. And I, you certainly
have to, you know, go,

265
00:15:33.540 --> 00:15:36.280
you know, move seamlessly between
the business people and the

266
00:15:36.280 --> 00:15:39.070
technical people. But why would
you, why do you say

267
00:15:39.070 --> 00:15:42.370
that? Why is it that
the MBAs might, might inadvertently

268
00:15:42.370 --> 00:15:46.270
or fully verdantly drive the,
make the, the worst privacy

269
00:15:46.270 --> 00:15:48.790
issues? I think a lot
of it is because they

270
00:15:48.790 --> 00:15:54.310
don't always understand what it
is they're asking for. And

271
00:15:54.310 --> 00:15:56.860
so they, you know, they,
they look at a business

272
00:15:56.860 --> 00:15:59.830
problem or business challenge, you
know, from the perspective of

273
00:16:00.160 --> 00:16:04.570
how can I solve this
through a certain lens. And,

274
00:16:04.600 --> 00:16:06.630
you know, to be honest
right now, like the, the

275
00:16:06.710 --> 00:16:09.250
big thing that like everybody
in B school seems to

276
00:16:09.250 --> 00:16:12.700
be graduating with is this
like obsession with data as

277
00:16:12.700 --> 00:16:16.000
if data is the fix-all
for everything Mine at all

278
00:16:16.000 --> 00:16:20.410
right. Mine and sell it
tomorrow. And if like volumes

279
00:16:20.410 --> 00:16:24.220
of data inherently bring more
value, right? Like having a

280
00:16:24.220 --> 00:16:26.830
lot of data really doesn't
help your business at all,

281
00:16:26.830 --> 00:16:28.690
if you don't know how
to analyze it and use

282
00:16:28.690 --> 00:16:31.840
it properly, right. And then
you're just paying to store

283
00:16:31.840 --> 00:16:33.790
it and protect it and
secure it. And then you

284
00:16:33.790 --> 00:16:38.410
have these situations where things
catastrophes happen. And, and so

285
00:16:38.410 --> 00:16:39.610
I think that just a
lot of it has to

286
00:16:39.610 --> 00:16:41.740
do with the fact that,
you know, a lot of,

287
00:16:42.100 --> 00:16:45.040
and a lot of MBAs
will not necessarily even end

288
00:16:45.040 --> 00:16:48.520
up being for like that
particular tour. I wasn't even

289
00:16:48.520 --> 00:16:52.270
necessarily thinking about like executives,
I'm thinking about like product

290
00:16:52.270 --> 00:16:56.860
managers, right. And you know,
the folks who, who think

291
00:16:56.860 --> 00:16:59.590
they, you know, they're just
kind of solving problems through

292
00:16:59.980 --> 00:17:04.000
like a more narrow lens
engineers. I have found are

293
00:17:04.000 --> 00:17:07.510
very aware of what data
is being collected, where it's

294
00:17:07.510 --> 00:17:10.480
going, what it's being used
for. Cause they, they work

295
00:17:10.480 --> 00:17:13.780
in these systems day to
day. And so in my

296
00:17:13.780 --> 00:17:16.600
experience, working with engineers, all
I have to do is

297
00:17:16.600 --> 00:17:19.090
give them the guidance of
here's, what you can do.

298
00:17:19.090 --> 00:17:21.610
Here's what you can't do.
And they can build the

299
00:17:21.610 --> 00:17:25.780
systems to meet those requirements.
But, you know, for the

300
00:17:25.780 --> 00:17:30.940
folks who have different success
metrics, I think a lot

301
00:17:30.940 --> 00:17:32.980
of it just has to
do with the, they don't

302
00:17:32.980 --> 00:17:36.370
really understand the consequences of
what they're asking for or

303
00:17:36.370 --> 00:17:39.940
what has to be built
or what, you know, actual

304
00:17:39.940 --> 00:17:44.260
practices will come out of
those expectations. Hey folks, you're

305
00:17:44.260 --> 00:17:48.040
a very good developer, probably
regardless. You do write bugs,

306
00:17:48.040 --> 00:17:51.150
that's unavoidable. What is affordable
is wasting time trying to

307
00:17:51.150 --> 00:17:55.350
track down the cause of
those bugs century.io provides full

308
00:17:55.350 --> 00:17:58.920
stack error tracking that lets
you monitor and fix problems.

309
00:17:58.920 --> 00:18:00.990
In real time, you can
see the severity and the

310
00:18:00.990 --> 00:18:04.020
scope of the error get
immediate access to the stack,

311
00:18:04.020 --> 00:18:06.690
trace, connect the problem to
the commit that caused it

312
00:18:07.080 --> 00:18:10.080
and fix it without delay
century. It's a name so

313
00:18:10.080 --> 00:18:12.330
common that they have to
include the top level domain

314
00:18:12.330 --> 00:18:16.320
in their advertising to make
sure you remember it. century.io,

315
00:18:16.320 --> 00:18:20.490
that century dot I O
open source, full stack web

316
00:18:20.490 --> 00:18:23.940
apps, native apps, mobile games,
smart oven mitts. If you

317
00:18:23.940 --> 00:18:26.790
can program it, they can
make it far easier to

318
00:18:26.790 --> 00:18:29.730
fix any errors you encounter
with it. Your code may

319
00:18:29.730 --> 00:18:35.580
be broken, let's fix it
together. Visit century.io. It feels

320
00:18:35.580 --> 00:18:39.150
like a lot of startups
basically start with a perspective

321
00:18:39.150 --> 00:18:42.450
of just log everything, get
telemetry on everything and we'll

322
00:18:42.450 --> 00:18:45.990
figure out what's important or
what's useful later because everyone

323
00:18:45.990 --> 00:18:48.810
clicks the ULA when they
sign up early on and

324
00:18:48.810 --> 00:18:51.480
no one really goes back
and reasserts that ULA as

325
00:18:51.480 --> 00:18:54.210
things change in the future.
But so know kind of

326
00:18:54.210 --> 00:18:57.030
open by default and unlimited
telemetry by default can kind

327
00:18:57.030 --> 00:19:00.170
of lull us into a,
a, a sense of, of

328
00:19:00.450 --> 00:19:03.030
security and things are okay.
When huge amounts of data

329
00:19:03.030 --> 00:19:05.280
are being logged, it just
really never were asked for

330
00:19:05.280 --> 00:19:08.720
or needed. Right. Exactly. I
mean, so I think there's

331
00:19:09.290 --> 00:19:12.320
two distinct things here is
there are certainly services and

332
00:19:12.350 --> 00:19:15.770
apps who are, and I
think the New York times had

333
00:19:15.770 --> 00:19:18.440
a really good example of
this a couple of months

334
00:19:18.440 --> 00:19:21.710
ago with their story about
these apps that were collecting

335
00:19:21.710 --> 00:19:24.980
location data, but they weren't
actually like location based apps,

336
00:19:25.340 --> 00:19:27.560
right. They're just collecting location
data and then kind of

337
00:19:27.560 --> 00:19:30.150
be coming like a data
broker for those things and

338
00:19:30.200 --> 00:19:33.980
selling that information to other
parties. Right. And, and again,

339
00:19:33.980 --> 00:19:36.290
to your earlier point, that
that surprised a lot of

340
00:19:36.290 --> 00:19:39.350
people, they were not expecting
that, you know, this flashlight

341
00:19:39.350 --> 00:19:44.030
app is collecting, you know,
location data, but there's also

342
00:19:44.030 --> 00:19:49.010
the expectation and realm where,
you know, people would be

343
00:19:49.010 --> 00:19:52.340
expecting location data to be
collected. But, you know, for

344
00:19:52.340 --> 00:19:56.150
example, by a company who's
actually like, you know, providing

345
00:19:56.150 --> 00:20:02.690
that transportation, but there's the
conversation in the industry, at

346
00:20:02.690 --> 00:20:04.940
least with those of us
who, who do this full

347
00:20:04.940 --> 00:20:09.770
time inside companies is no,
we don't expect that anyone

348
00:20:09.800 --> 00:20:14.240
is actually reading terms of
service. It's just not for

349
00:20:14.240 --> 00:20:18.500
human consumption, to be honest.
Right. And Sarah, we're thinking

350
00:20:18.500 --> 00:20:22.610
about creating more of like
security and privacy that you

351
00:20:22.610 --> 00:20:27.620
can actually feel in your
experience of using the product.

352
00:20:27.800 --> 00:20:30.620
Right? So instead of just,
you know, having, you know,

353
00:20:30.620 --> 00:20:35.660
this contractual consent process, when
you first create your account

354
00:20:35.660 --> 00:20:39.770
or first use an app,
thinking about what are you

355
00:20:39.770 --> 00:20:43.040
doing in the app and
what are the moments that

356
00:20:43.040 --> 00:20:45.920
matter in that experience where
we could surface some of

357
00:20:45.920 --> 00:20:48.880
this information and help you
make a more informed decision.

358
00:20:49.390 --> 00:20:52.210
Right? So, you know, it's
not just like the one

359
00:20:52.210 --> 00:20:54.220
and done of when you
create an account, but we

360
00:20:54.220 --> 00:20:57.880
actually want this to be
part of the user experience

361
00:20:58.330 --> 00:21:01.330
so that there aren't any
surprises. And so that, you

362
00:21:01.330 --> 00:21:04.090
know, in those moments where
you have the opportunity to

363
00:21:04.090 --> 00:21:07.720
make a choice, that you're
aware that that choice exists

364
00:21:07.750 --> 00:21:11.140
and that, you know, you
know, you're given the tools

365
00:21:11.140 --> 00:21:13.570
and the information in order
to take action based on

366
00:21:13.570 --> 00:21:17.010
your preference. Are you familiar
with the term, the uncanny

367
00:21:17.010 --> 00:21:19.920
Valley when it comes to
three D rendering of faces?

368
00:21:20.040 --> 00:21:23.700
I am not. If I
may explain really briefly. So

369
00:21:23.880 --> 00:21:26.370
this is where like, Hey,
that's a cute robot. It's

370
00:21:26.370 --> 00:21:28.950
like a Wally. And then
like, Oh wow, that robot

371
00:21:28.950 --> 00:21:31.110
has a face. That's cute.
Oh my God, that looks

372
00:21:31.110 --> 00:21:34.320
like a zombie. Like as
you start to render things

373
00:21:34.320 --> 00:21:37.500
and they look like more
and more like people, there's

374
00:21:37.500 --> 00:21:40.890
a moment when it's like,
look at that, it's got

375
00:21:40.890 --> 00:21:43.290
dead eyes and then it
suddenly becomes amazing again. Right.

376
00:21:44.430 --> 00:21:48.060
There's an uncanny Valley of
privacy and security when it

377
00:21:48.060 --> 00:21:50.520
comes to things like artificial
intelligence was like, Oh, that's

378
00:21:50.520 --> 00:21:53.370
delightful. Oh, that's so cool.
Like, Oh my goodness, they're

379
00:21:53.370 --> 00:21:56.730
putting different faces on my
Netflix album art because they

380
00:21:56.730 --> 00:21:58.590
know what I've been looking
at. And now they're trying

381
00:21:58.590 --> 00:22:01.470
to make me click on
movies that suck. That's the,

382
00:22:01.470 --> 00:22:04.770
I've just fallen into the
unbaked Kenny Valley, uncanny Valley

383
00:22:04.770 --> 00:22:06.690
of privacy. Like I get
why they want to know

384
00:22:06.690 --> 00:22:09.630
stuff, but then there's a
moment it's like, Oh, that

385
00:22:09.630 --> 00:22:11.970
went way too far. I
think we're still as an

386
00:22:11.970 --> 00:22:14.520
industry trying to figure out
what that uncanny Valley of

387
00:22:14.520 --> 00:22:18.120
privacy is. Well, and I
think that there's, I mean,

388
00:22:18.120 --> 00:22:23.670
some of those examples, I
think are more about companies

389
00:22:23.760 --> 00:22:28.170
building features and tools in
order to collect more data

390
00:22:28.920 --> 00:22:33.570
versus look, you know, thinking
about what data would make

391
00:22:33.570 --> 00:22:38.340
the service better and just,
and, you know, building something

392
00:22:38.400 --> 00:22:42.660
that is worthwhile for the
user, right. Based on that

393
00:22:42.660 --> 00:22:48.870
data, there is unfortunately, you
know, pockets of technology companies

394
00:22:48.870 --> 00:22:52.290
where they're thinking about, you
know, we really want this

395
00:22:52.290 --> 00:22:55.350
data set over here that
we don't currently collect. What

396
00:22:55.350 --> 00:23:00.690
can we build in order
to work that well? And

397
00:23:00.690 --> 00:23:05.040
so it's backwards, right? And
so, you know, the privacy

398
00:23:05.040 --> 00:23:09.630
and security teams are sitting
down with product teams and

399
00:23:09.690 --> 00:23:13.020
engineers to say, there needs
to be a justification for

400
00:23:13.020 --> 00:23:16.800
this feature, from the user
perspective. First, we're not going

401
00:23:16.800 --> 00:23:20.880
to build a product for
the purpose of collecting data.

402
00:23:21.690 --> 00:23:23.490
And that's, that's a shift
in the way that a

403
00:23:23.490 --> 00:23:27.660
lot of Silicon Valley companies
have traditionally thought. That is

404
00:23:27.660 --> 00:23:30.030
a really interesting way of
thinking about it. And that's

405
00:23:30.030 --> 00:23:32.310
very helpful to me because
I work on, you know,

406
00:23:32.310 --> 00:23:35.340
developer tools and we have
this command line where people

407
00:23:35.340 --> 00:23:39.540
are typing in different commands
and we collect anonymous telemetry.

408
00:23:39.540 --> 00:23:41.400
We're very explicit to all
public about what we, and

409
00:23:41.400 --> 00:23:43.710
we actually released the telemetry
data back out to the,

410
00:23:44.130 --> 00:23:47.270
to the customer about like,
what are they typing at

411
00:23:47.270 --> 00:23:48.770
the command line? But we
want to be careful that

412
00:23:48.770 --> 00:23:52.460
we don't catch like code
names and, you know, secret

413
00:23:52.640 --> 00:23:55.490
namespaces of people's projects and
stuff. So, you know, what

414
00:23:55.490 --> 00:23:57.290
ends, what starts as a
black list becomes a white

415
00:23:57.290 --> 00:23:59.990
list. And this is all
very useful data that helps

416
00:23:59.990 --> 00:24:04.040
us improve the product, but
that, that generic by giving

417
00:24:04.040 --> 00:24:06.620
us your data, it helps
us improve the product is

418
00:24:06.620 --> 00:24:09.290
such a vagary. It doesn't
feel like this is a

419
00:24:09.290 --> 00:24:12.710
substantively making the product better.
So you're saying invert, the

420
00:24:12.710 --> 00:24:14.540
whole thing, flip it on
its head and say, what

421
00:24:14.540 --> 00:24:17.690
can we collect specifically to
make the product better now,

422
00:24:18.220 --> 00:24:20.920
Do we need that to
make the product better? In

423
00:24:20.920 --> 00:24:24.370
fact, I have a number
of heart to heart conversations

424
00:24:24.370 --> 00:24:27.220
with folks all the time,
not just at my own

425
00:24:27.220 --> 00:24:31.030
company, but you know, across
the, the industry where, you

426
00:24:31.030 --> 00:24:34.840
know, the conversation we're having
is first, I need you

427
00:24:34.840 --> 00:24:37.240
to justify to me that
this problem can only be

428
00:24:37.240 --> 00:24:41.260
solved with more data, right?
And again, it's, you know,

429
00:24:41.260 --> 00:24:43.210
this is where it got.
I'm going back to the

430
00:24:43.210 --> 00:24:46.300
comment about MBAs is like,
data is not the answer

431
00:24:46.300 --> 00:24:50.650
for everything. And sometimes it's
just a matter of, you're

432
00:24:50.650 --> 00:24:54.430
not using the data you
already have in a really

433
00:24:54.430 --> 00:24:58.960
smart way. So, you know,
before we get to the

434
00:24:58.960 --> 00:25:02.590
point of what is the
feature, you know, before we

435
00:25:02.590 --> 00:25:04.990
get to the conversation of
what do we need to

436
00:25:04.990 --> 00:25:09.140
collect in order to enable
this feature? You know, the,

437
00:25:09.140 --> 00:25:11.920
the question is prove to
me that data is the

438
00:25:11.920 --> 00:25:16.270
only solution, right? That it's
the only path, because from

439
00:25:16.270 --> 00:25:20.290
my perspective, collecting more data
in order to solve a

440
00:25:20.290 --> 00:25:24.820
problem should be, you know,
last case scenario, we should

441
00:25:24.820 --> 00:25:28.870
have exhausted all of the
alternatives before we assume that

442
00:25:29.050 --> 00:25:31.990
collecting data is going to
be the answer. This sounds

443
00:25:31.990 --> 00:25:33.850
like a song that I
want to sing. And I'm

444
00:25:33.850 --> 00:25:35.980
a fan. I like this
perspective and this way of

445
00:25:35.980 --> 00:25:40.630
thinking, how do we build
that into the ethos of

446
00:25:40.630 --> 00:25:42.970
everyone? And then how do
you explain it to nontechnical

447
00:25:42.970 --> 00:25:47.710
parent and God help us
nontechnical lawmaker? You know, to

448
00:25:47.710 --> 00:25:52.120
be honest, I don't know
if we can actually do

449
00:25:52.120 --> 00:25:57.310
that without building it into
the product. First I look

450
00:25:57.310 --> 00:26:00.190
at, I mean, I know
from a privacy perspective, there's,

451
00:26:00.280 --> 00:26:04.210
you know, some very vocal
and valid criticisms in terms

452
00:26:04.210 --> 00:26:07.660
of like, you know, some
of the platform companies and

453
00:26:07.660 --> 00:26:10.600
social media companies, but one
of the great things that

454
00:26:10.600 --> 00:26:14.590
we've gotten from all of
those companies is massive awareness

455
00:26:14.590 --> 00:26:22.090
and visibility among consumers about
security issues. You know, Google

456
00:26:22.090 --> 00:26:28.450
made HTTPS a thing on
the internet, right? I think

457
00:26:28.510 --> 00:26:31.300
some of the engagement that
Facebook has done in their

458
00:26:31.300 --> 00:26:35.290
product has really increased awareness
of things like two factor

459
00:26:35.290 --> 00:26:39.790
authentication or, you know, using
their trusted context feature so

460
00:26:39.790 --> 00:26:41.680
that you can get back
into your account if you

461
00:26:41.680 --> 00:26:45.870
ever get locked out. Right.
And so there is a

462
00:26:45.870 --> 00:26:48.990
lot of value in terms
of the volume of user

463
00:26:48.990 --> 00:26:51.090
base that these companies have.
And I think on the

464
00:26:51.090 --> 00:26:54.900
security side, we have seen
a lot of that awareness

465
00:26:54.900 --> 00:26:58.260
and education happened because these
companies built it into their

466
00:26:58.260 --> 00:27:03.870
products first. And so that
gave billions of people, frequent

467
00:27:03.870 --> 00:27:08.760
engagement using these types of
technologies and tools from a

468
00:27:08.760 --> 00:27:12.990
security perspective. And I think,
you know, those of us

469
00:27:12.990 --> 00:27:15.930
that, you know, that also
work in, in privacy, we

470
00:27:15.930 --> 00:27:19.410
have to build this into
our tools first. I don't

471
00:27:19.410 --> 00:27:22.260
think that you're necessarily, you're
not going to get feedback

472
00:27:22.260 --> 00:27:26.310
from, you know, the average
consumer saying I would like

473
00:27:26.310 --> 00:27:29.250
a privacy feature that does
X, Y, and Z, but

474
00:27:29.250 --> 00:27:31.140
they're going to say as
these are all the things

475
00:27:31.140 --> 00:27:33.870
that I'm worried about or
the things that scare me,

476
00:27:34.800 --> 00:27:38.310
and then we have to
build the tool within the

477
00:27:38.310 --> 00:27:42.180
product and within our services
so that they can become

478
00:27:42.180 --> 00:27:45.750
familiar with what what's the
behavior and what are the

479
00:27:45.810 --> 00:27:49.770
actions that, that you should
be taking if you have

480
00:27:49.770 --> 00:27:53.410
a certain worry or a
certain fear. Yeah, the is

481
00:27:53.410 --> 00:27:56.390
certainly Facebook is the easy,
the big, you know, gorilla

482
00:27:56.390 --> 00:27:58.460
so we can poke at
it. But it makes me

483
00:27:58.460 --> 00:28:01.460
think about those videos that
they send out the auto-generated,

484
00:28:01.460 --> 00:28:04.880
this was year, year videos.
It's like maybe I had

485
00:28:04.880 --> 00:28:07.130
a really crappy year. I
don't want a video with

486
00:28:07.130 --> 00:28:10.100
like jolly music about, you
know, my pet dying or

487
00:28:10.100 --> 00:28:13.010
whatever horrible thing happened this
year. I don't remember ever

488
00:28:13.010 --> 00:28:15.380
asking for those videos to
be made, but I'm sure

489
00:28:15.380 --> 00:28:18.650
that they delight more people
than they don't, but arguably

490
00:28:18.650 --> 00:28:21.110
it only takes one person
who had a horrible year

491
00:28:21.110 --> 00:28:23.780
to be reminded of a
death. And then that needs

492
00:28:23.780 --> 00:28:26.180
to stop those, you know,
that kind of, Well, in

493
00:28:26.180 --> 00:28:29.510
fact, it's, it's one of
the reasons why Facebook actually

494
00:28:29.510 --> 00:28:31.970
does have an option for
you to turn that off

495
00:28:32.000 --> 00:28:34.430
so that you don't get
those reminders. Yeah. Yeah. But

496
00:28:34.540 --> 00:28:36.350
certainly something that you would
only want to turn off

497
00:28:36.350 --> 00:28:39.620
once it had happened once
and been bad. Right. Well,

498
00:28:39.620 --> 00:28:43.220
so for example, I, I
have, I still have the

499
00:28:43.220 --> 00:28:46.070
option turned on to get
those end of year videos,

500
00:28:46.850 --> 00:28:49.400
but what I don't like
are all of the frequent

501
00:28:50.150 --> 00:28:54.200
like memories that they surface,
because I have quite a

502
00:28:54.200 --> 00:28:55.790
few of them that I
don't want to relive over

503
00:28:55.790 --> 00:28:58.760
and over again, they're an
important part of my personal

504
00:28:58.760 --> 00:29:01.010
history and I want them
to be documented and they

505
00:29:01.010 --> 00:29:03.650
were important experiences, but I
don't need to be reminded

506
00:29:03.650 --> 00:29:06.730
every day. Yeah. It's certainly
not on the anniversary. Right.

507
00:29:06.740 --> 00:29:13.010
Exactly. And so, you know,
Facebook did respond to some

508
00:29:13.010 --> 00:29:15.920
of that criticism for exactly
what you're bringing up. I

509
00:29:15.920 --> 00:29:18.620
think that the challenge is
that it's, it's still like

510
00:29:18.620 --> 00:29:22.340
an opt out experience. Right.
So, so you first have

511
00:29:22.340 --> 00:29:25.370
to have that really bad
experience in order to know

512
00:29:25.730 --> 00:29:28.220
that, Oh, I'm going to
go turn this off. Right.

513
00:29:28.640 --> 00:29:31.010
They don't, they don't ask
you if you would like

514
00:29:31.010 --> 00:29:33.230
to have this before they
show you the first one.

515
00:29:34.340 --> 00:29:36.830
And they argument that the
MBA would make. Cause I've

516
00:29:36.830 --> 00:29:39.110
worked with folks that think
like, this is like, well,

517
00:29:39.110 --> 00:29:42.340
but only 5% or single
digit percent of people will

518
00:29:42.340 --> 00:29:45.400
opt in and we can
get 90% if it's opt

519
00:29:45.400 --> 00:29:49.200
out. So clearly it's opt
in by default. And it's

520
00:29:49.260 --> 00:29:52.350
yeah. The, the fact that
we're designing for like the

521
00:29:52.350 --> 00:29:55.800
99% I think is, is
a problem. And it, again,

522
00:29:55.800 --> 00:29:58.060
just goes back to, you
know, like you said, that

523
00:29:58.060 --> 00:30:01.080
the MBAs here where you're
the people who are at

524
00:30:01.110 --> 00:30:04.110
the most risk from both
a security perspective and the

525
00:30:04.110 --> 00:30:06.510
privacy perspective are not the
people who are the most

526
00:30:06.510 --> 00:30:12.690
paranoid. The people who have
the most to lose from

527
00:30:12.870 --> 00:30:16.560
insufficient security or privacy protections
are not the people in

528
00:30:16.560 --> 00:30:20.190
the middle of your bell
curve. And so, you know,

529
00:30:20.440 --> 00:30:24.360
when we teach, you know,
business school or product managers,

530
00:30:25.500 --> 00:30:28.440
you know, to, to build
based on the volume of

531
00:30:28.440 --> 00:30:32.170
people who are going to
use your product, it, it

532
00:30:32.190 --> 00:30:35.490
conditions them to think about,
you know, this is only

533
00:30:35.490 --> 00:30:38.280
valuable if a lot of
people will use it. And

534
00:30:38.280 --> 00:30:41.760
you know what, sometimes the
products that have the biggest

535
00:30:41.820 --> 00:30:45.870
impact in terms of contributing
to making the world a

536
00:30:45.870 --> 00:30:49.380
better place and actually protecting
people are the tools and

537
00:30:49.380 --> 00:30:54.060
products that are used by
small, you know, groups, it

538
00:30:54.300 --> 00:30:56.730
perhaps, you know, on the
margins of your bell curve,

539
00:30:57.150 --> 00:30:59.940
but it has such a
massive impact on their life

540
00:30:59.970 --> 00:31:04.470
because of their risk profile.
And, you know, these are

541
00:31:04.470 --> 00:31:07.440
the ways that security and
privacy professionals think about it,

542
00:31:07.440 --> 00:31:11.220
where, you know, we, we
envisioned the risk profile of

543
00:31:11.220 --> 00:31:13.440
all of these different groups
of people who use our

544
00:31:13.440 --> 00:31:16.890
products. And we say, where
can we have the most

545
00:31:16.890 --> 00:31:21.960
impact in driving down risk?
And that usually happens at

546
00:31:22.110 --> 00:31:24.810
the edge case, not necessarily
in the middle where the

547
00:31:24.810 --> 00:31:29.010
most people are Very, very
powerful perspective. I appreciate you

548
00:31:29.010 --> 00:31:31.680
taking the time to chat
with me today. Yeah, of

549
00:31:31.680 --> 00:31:34.470
course. Thanks again so much
for, for the discussion. I'm

550
00:31:34.890 --> 00:31:38.310
always happy to talk about
these topics. Cause I'm, I'm

551
00:31:38.310 --> 00:31:41.520
excited to see this paradigm
shifting across the industry. We

552
00:31:41.520 --> 00:31:44.910
will include a link to
Melanie's Twitter on the show

553
00:31:44.910 --> 00:31:47.430
notes. We've been talking to
Melanie and sign the security

554
00:31:47.430 --> 00:31:51.660
and privacy communication lead for
Uber and a frequent tweeter

555
00:31:51.660 --> 00:31:55.740
about sharks. This has been
another episode of Hanselminutes and

556
00:31:55.740 --> 00:31:56.910
we'll see you again next
week.

