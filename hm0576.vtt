WEBVTT FILE

1
00:00:00.360 --> 00:00:03.510
Hi, this is Scott. I
really appreciate our sponsors because

2
00:00:03.510 --> 00:00:06.480
they make the show possible.
Today's show is sponsored by

3
00:00:06.480 --> 00:00:10.680
developer express, become a UI
superhero with dev express controls

4
00:00:10.680 --> 00:00:15.210
and libraries. Deliver elegant.net solutions
that address customer needs today

5
00:00:15.570 --> 00:00:19.650
by leveraging your existing knowledge,
you can build next generation

6
00:00:19.680 --> 00:00:23.760
touch enabled solutions for tomorrow,
you can download your free

7
00:00:23.880 --> 00:00:45.920
30 day trial@dxdothanselminutes.com. That's dx.hanselminutes.com.
Hi, this is Scott Hanselman.

8
00:00:45.920 --> 00:00:48.890
This is another episode of
Hansel minutes today. I'm talking

9
00:00:48.890 --> 00:00:51.920
with at Ana Salinas Hasso
who is a member of

10
00:00:51.920 --> 00:00:55.460
the knowledge technologies group at
Microsoft research. She's got a

11
00:00:55.460 --> 00:00:58.370
master's degree in computer science
from the university of Texas.

12
00:00:58.430 --> 00:01:01.970
How are you? Good. How
are you, Scott? I'm great.

13
00:01:01.970 --> 00:01:05.510
Thanks for, for coming on.
You're also a podcaster and

14
00:01:05.510 --> 00:01:09.620
you've done a lot of
different shows. Yes, that's correct.

15
00:01:09.620 --> 00:01:12.140
I host the women in
tech show, which is a

16
00:01:12.140 --> 00:01:16.760
weekly podcast And you've been
guests on like software engineering

17
00:01:16.760 --> 00:01:18.590
daily and stuff like that.
And people can check out

18
00:01:18.590 --> 00:01:23.480
the women in tech show.com.
Great URL. Thank you. Did

19
00:01:23.480 --> 00:01:26.110
you want to know a
bit about the show? Yeah.

20
00:01:26.110 --> 00:01:28.100
Yeah. I mean, I assume
it's about women in tech.

21
00:01:28.190 --> 00:01:33.260
Yes. So as a software
engineer, when I come to

22
00:01:33.260 --> 00:01:37.430
work everyday, I still see
that there are not enough

23
00:01:37.430 --> 00:01:41.960
women working in technology. However,
there are a few women

24
00:01:41.960 --> 00:01:45.980
who are working on fantastic
projects and defining the future

25
00:01:45.980 --> 00:01:50.030
of technology. And what I
have found is that these

26
00:01:50.030 --> 00:01:54.500
women often get invited to
give the diversity talk at

27
00:01:54.500 --> 00:01:58.580
conferences, to the extent that
on their website, that they

28
00:01:58.580 --> 00:02:02.330
specify that they do talks,
but to not ask them

29
00:02:02.330 --> 00:02:05.980
to give the diversity talk.
So the goal of the

30
00:02:06.020 --> 00:02:09.500
women in tech show is
to provide a platform where

31
00:02:09.620 --> 00:02:12.530
they talk about technology and
the products that they are

32
00:02:12.530 --> 00:02:16.640
building instead of the women
in tech concept and what

33
00:02:16.640 --> 00:02:19.940
it feels like to be
a women in tech. I

34
00:02:19.940 --> 00:02:23.030
see. So people are getting,
you know, like when people

35
00:02:23.030 --> 00:02:25.670
like me might be calling
their, the women that they

36
00:02:25.670 --> 00:02:27.470
know in tech and say,
Oh yeah, come and talk

37
00:02:27.470 --> 00:02:29.900
about diversity in technology. And,
and it's like, well, no,

38
00:02:29.900 --> 00:02:31.880
I want to talk about
cool stuff. I'm building. Cause

39
00:02:31.880 --> 00:02:37.250
it's cool. Exactly. That's what's
happening in the diversity talk.

40
00:02:37.880 --> 00:02:41.090
I'm not against that talk,
but I think we have

41
00:02:41.960 --> 00:02:45.140
been overdoing it a bit.
And my thought is that

42
00:02:45.590 --> 00:02:49.310
if we have more women
talking about technology, more people

43
00:02:49.310 --> 00:02:51.770
are going to learn from
them. And it's not going

44
00:02:51.770 --> 00:02:56.060
to be a rare thing
to see a women talking

45
00:02:56.060 --> 00:03:01.030
about tech. So it'll help.
Yeah. So this will help

46
00:03:01.270 --> 00:03:05.910
tackle the unconscious bias problems
and things like that. Yeah.

47
00:03:06.060 --> 00:03:08.280
Well I hope it will
that we're doing a small,

48
00:03:08.310 --> 00:03:10.950
I mean, I'm doing a
small part as well. We've

49
00:03:10.950 --> 00:03:13.740
had a lot of different
women and people of color

50
00:03:13.740 --> 00:03:17.160
on the show because they
are doing cool stuff. Not

51
00:03:17.160 --> 00:03:21.000
because we're trying to fill
some kind of, some kind

52
00:03:21.000 --> 00:03:23.850
of quota. I don't want
to create a world of

53
00:03:24.060 --> 00:03:26.940
what I call power Rangers.
Diversity. Have you ever seen

54
00:03:26.940 --> 00:03:30.360
the power Rangers team? Yes
I have. Yeah. So with

55
00:03:30.360 --> 00:03:33.630
the power Rangers, whenever they
build a team, there's one

56
00:03:33.630 --> 00:03:36.750
of each and you can't
have two of one kind

57
00:03:36.750 --> 00:03:38.760
because you know, it wouldn't
be the power Rangers. If

58
00:03:38.760 --> 00:03:41.310
it didn't look like a
weird pie chart of diversity.

59
00:03:41.550 --> 00:03:44.820
So instead we want them
doing cool stuff and talking

60
00:03:44.820 --> 00:03:46.770
about cool stuff. So what
cool stuff are you working

61
00:03:46.770 --> 00:03:48.920
on? Cause you've changed your
career a little bit from,

62
00:03:48.920 --> 00:03:54.300
from one technology to another.
Yes. So I joined Microsoft

63
00:03:54.300 --> 00:03:56.940
full time. It was my
first full time job back

64
00:03:56.940 --> 00:04:02.790
in June, 2014. And for two
years I was working on

65
00:04:03.180 --> 00:04:10.560
web applications on services. And
when I went to velocity

66
00:04:11.010 --> 00:04:16.800
last year, which is a
conference about web performance and

67
00:04:16.860 --> 00:04:20.370
dev ops, there was this
talk that really caught my

68
00:04:20.370 --> 00:04:24.630
attention from Patrick Meenan, who
is the creator of the

69
00:04:24.630 --> 00:04:28.230
webpage test and works on
Chrome and dev tools. And

70
00:04:28.230 --> 00:04:31.650
Tammy Everts our research at
SOASTA and their talk was

71
00:04:31.920 --> 00:04:37.200
using machine learning to predict
bounce rate and conversions. So

72
00:04:37.200 --> 00:04:40.260
what I liked about this
talk is that Patrick Meenan

73
00:04:40.260 --> 00:04:44.340
was saying, I don't have
a background in machine learning.

74
00:04:44.490 --> 00:04:47.970
I don't have a PhD
in AI, but the tools

75
00:04:47.970 --> 00:04:52.560
are very accessible. Now with
this library from Python called

76
00:04:52.560 --> 00:04:55.860
psychic learn. So he was
showing the code and he

77
00:04:55.860 --> 00:04:59.910
said, here it is eight
lines of Python and we're

78
00:04:59.910 --> 00:05:03.180
running a machine learning algorithm.
So at that moment, it

79
00:05:03.450 --> 00:05:06.750
clicked to me that this
is going to be very

80
00:05:06.750 --> 00:05:12.900
big. And it's the next
revolution in technology. Did this

81
00:05:12.900 --> 00:05:15.660
make you want to just
pivot your career and change

82
00:05:15.660 --> 00:05:19.380
direction right away? That was
the moment It started. It

83
00:05:19.380 --> 00:05:23.760
made me think certainly. And
then right after that, Microsoft

84
00:05:23.760 --> 00:05:29.310
announced their big investment in
artificial intelligence and bringing AI

85
00:05:29.310 --> 00:05:33.540
into our products and there's
even an AI school now.

86
00:05:34.350 --> 00:05:38.010
So that's when I started
thinking it's right now, when

87
00:05:38.010 --> 00:05:42.840
it's happening, the tools are
very accessible. There are lots

88
00:05:42.840 --> 00:05:47.910
of resources online to learn
this topic. So I that's

89
00:05:47.910 --> 00:05:51.360
when I decided to focus
on this because it's the

90
00:05:51.360 --> 00:05:55.410
next wave of technology and
the impact that it's going

91
00:05:55.410 --> 00:05:57.090
to have in our lives
is going to be so,

92
00:05:58.940 --> 00:06:01.670
And you were doing, you're
doing web dev at Microsoft

93
00:06:01.670 --> 00:06:04.750
at that point when you
decided to switch. Yes. Yes.

94
00:06:04.750 --> 00:06:10.300
So I was working on
previously power apps in the

95
00:06:10.300 --> 00:06:14.290
web portion of it. So
it was services and C-sharp

96
00:06:14.320 --> 00:06:19.180
and the front end type
script, HTML knockout. And before

97
00:06:19.180 --> 00:06:23.860
that I was working on
an internal project, which was

98
00:06:23.860 --> 00:06:29.620
angular 1.0 and similar stock
type script C-sharp. So, so

99
00:06:29.620 --> 00:06:32.650
what I had switched from
was from working on an

100
00:06:32.650 --> 00:06:36.490
internal project to our consumer
facing project, because I wanted

101
00:06:36.490 --> 00:06:41.680
to experience that side of
a product. And it's very

102
00:06:41.680 --> 00:06:45.970
different because the first one
goes from being a hundred

103
00:06:45.970 --> 00:06:49.720
users or 200 and the
other one, two thousands. And

104
00:06:49.720 --> 00:06:54.850
then seeing that growth Did,
when I think of Microsoft

105
00:06:54.850 --> 00:06:58.960
research, I think of people
with advanced degrees and PhDs,

106
00:06:59.260 --> 00:07:01.180
and I have to say,
and this is my own

107
00:07:01.180 --> 00:07:04.210
internal kind of bias because
I went to a community

108
00:07:04.240 --> 00:07:06.670
college, which is like a
technical school. I don't have

109
00:07:06.670 --> 00:07:09.550
a advanced degree. I have
a bachelor's degree. I feel,

110
00:07:09.580 --> 00:07:14.020
I feel intimidated because I
think everyone is doctor this

111
00:07:14.020 --> 00:07:16.870
and doctor that now you
have a master's degree, which

112
00:07:16.870 --> 00:07:19.000
is an advanced degree. Are
you working on your PhD?

113
00:07:19.000 --> 00:07:21.190
And are you, how do
you feel? Is everyone on

114
00:07:21.190 --> 00:07:24.280
the team, a PhD type
person? I'm not working on

115
00:07:24.280 --> 00:07:28.060
a PhD. I did do
a master's in computer science.

116
00:07:28.570 --> 00:07:33.580
My, my team is very
mixed. There are several PhD

117
00:07:33.670 --> 00:07:36.790
people, but I've been on
the industry 15 or 20

118
00:07:36.790 --> 00:07:41.410
years. One of them studied
linguistics and he's from Germany

119
00:07:41.410 --> 00:07:47.440
and built the, the auto,
correct? Not the auto, correct.

120
00:07:47.470 --> 00:07:51.850
The language checks in word,
you build that. And then

121
00:07:52.570 --> 00:07:57.220
there's another PhD who focused
on NLP. But what I'm

122
00:07:57.220 --> 00:08:02.350
seeing now after Microsoft's decision
to democratize AI throughout the

123
00:08:02.350 --> 00:08:08.650
company, is that we're bringing
highly experienced software engineers and

124
00:08:08.650 --> 00:08:15.460
developers who can help integrate
research technologies into products while

125
00:08:15.460 --> 00:08:20.200
also getting exposed to things
like machine learning and deep

126
00:08:20.200 --> 00:08:26.830
learning, because these are now
so accessible. Even several people

127
00:08:26.830 --> 00:08:30.190
say you don't need a
PhD. Some Aldman had a

128
00:08:30.190 --> 00:08:32.470
QA and he said, no,
you don't need a PhD

129
00:08:32.470 --> 00:08:36.220
to do this. You just
need to be motivated and

130
00:08:36.280 --> 00:08:38.800
start getting exposed to it.
We have a lot of

131
00:08:38.980 --> 00:08:43.180
tools right now, like toggle,
which makes a diverse set

132
00:08:43.180 --> 00:08:45.880
of data sets available that
you can play with. Can

133
00:08:45.880 --> 00:08:49.720
you spell that tool? So
Cabo is K a G

134
00:08:49.720 --> 00:08:55.000
G L E Huh. Okay.
And this is a kaggle.com

135
00:08:55.120 --> 00:08:57.150
a whole website around it.
There's a whole, it's a

136
00:08:57.150 --> 00:09:02.090
whole world of data science.
Yes. And we see people

137
00:09:02.090 --> 00:09:08.390
contributing Leora that have no
backgrounds and PhDs and in

138
00:09:08.390 --> 00:09:12.560
AI and things like that
because you, you can divide

139
00:09:12.560 --> 00:09:16.310
it in two areas. For
example, one is creating those

140
00:09:16.310 --> 00:09:21.950
algorithms and optimizing them. And
the other area is using

141
00:09:21.950 --> 00:09:26.360
those algorithms and tweaking some
things like the parameters to

142
00:09:26.360 --> 00:09:31.640
solve problems like detecting cancer
in x-ray images and things

143
00:09:31.640 --> 00:09:35.540
like that. Interesting. So when
I think of kind of

144
00:09:35.540 --> 00:09:39.410
the multiple layers, there's, there's
artificial intelligence in the general

145
00:09:39.410 --> 00:09:42.130
sense, there's machine learning. As
we get more specifically in

146
00:09:42.130 --> 00:09:44.090
what we think of as
machine learning right now, and

147
00:09:44.090 --> 00:09:47.270
then there's this concept of
deep learning for, for many

148
00:09:47.270 --> 00:09:51.020
of us, myself, I'll speak
for myself. It's a, it's

149
00:09:51.020 --> 00:09:53.270
a black box like you're
describing, you know, I can

150
00:09:53.270 --> 00:09:55.610
write a couple of lines
of Python. I put it

151
00:09:55.610 --> 00:09:58.280
into a black box. And
then from my perspective, a

152
00:09:58.280 --> 00:10:04.280
miracle happens and someone with
a PhD understands that, should

153
00:10:04.280 --> 00:10:07.190
I strive to understand the
internals or how deep does

154
00:10:07.190 --> 00:10:08.780
one need to go? How
deep do you have to

155
00:10:08.780 --> 00:10:11.300
go before you say, all
right. And then a miracle

156
00:10:11.300 --> 00:10:13.610
happens, like how many layers
of abstraction down do you

157
00:10:13.610 --> 00:10:16.980
go? So the first layer
of that, I think of

158
00:10:17.390 --> 00:10:22.400
as the big umbrella is
artificial intelligence, which is making

159
00:10:22.400 --> 00:10:28.370
machines intelligent. And under this
umbrella, we have the machine

160
00:10:28.370 --> 00:10:35.810
learning umbrella, which is a
way of teaching machines to

161
00:10:35.810 --> 00:10:43.070
do intelligent tasks, like recognizing
images, processing, natural language. And

162
00:10:43.070 --> 00:10:48.800
then inside machine learning, we
have deep learning, which is

163
00:10:49.730 --> 00:10:54.170
using several layers of neural
networks. It's just a different

164
00:10:54.440 --> 00:11:03.350
yet another type of teaching
machines, intelligent tasks, but without

165
00:11:03.380 --> 00:11:09.710
having to have expertise on
defining features. So for example,

166
00:11:09.710 --> 00:11:13.010
in machine learning, if you're
trying to solve a problem,

167
00:11:13.700 --> 00:11:17.990
like detecting spam, if I'm
doing it, the machine learning

168
00:11:17.990 --> 00:11:22.160
way, I specify features, which
are characteristics that will help

169
00:11:22.160 --> 00:11:24.530
me know if something is
spam. For example, if it

170
00:11:24.530 --> 00:11:32.000
contains a certain word like
free food or something, or

171
00:11:33.560 --> 00:11:36.500
No free money, right. Pay
for help. So, so a

172
00:11:36.500 --> 00:11:42.950
person is specifying those characteristics
while in deep learning, why

173
00:11:42.950 --> 00:11:47.240
it's become very big and
accessible is that those features

174
00:11:47.570 --> 00:11:53.030
are automatically detected by an
algorithm. So to answer your

175
00:11:53.050 --> 00:11:57.280
question, how did you need
to go? You don't need

176
00:11:57.280 --> 00:12:03.460
to go really deep as
to understanding the mathematics in

177
00:12:03.460 --> 00:12:08.680
the models, but you need
to have some depth as

178
00:12:08.680 --> 00:12:12.940
to what the algorithm is
doing. And some implications like,

179
00:12:14.680 --> 00:12:18.370
do they perform well with
more data? So I think

180
00:12:18.570 --> 00:12:23.140
there's a middle ground. There's
definitely aligned between the PhD

181
00:12:23.830 --> 00:12:28.740
and more about applied machine
learning. I like that. That's

182
00:12:28.740 --> 00:12:30.300
a good point. And you
know, that's interesting that you

183
00:12:30.300 --> 00:12:33.630
point that out because you
said applied machine learning, which

184
00:12:33.630 --> 00:12:35.910
seems like the thing I
would be interested in. So

185
00:12:35.910 --> 00:12:37.470
let me see if I
can parse this a little

186
00:12:37.470 --> 00:12:39.780
bit and we'll come back
to that. So, so machine

187
00:12:39.780 --> 00:12:44.280
learning you said is, you
know, it's an approach to

188
00:12:44.280 --> 00:12:47.730
go towards artificial intelligence. So
you take algorithms, you parse

189
00:12:47.730 --> 00:12:49.980
data, you learn from it
and you train it. You

190
00:12:49.980 --> 00:12:55.260
train the system while deep
learning is a different algorithmic

191
00:12:55.260 --> 00:12:59.310
approach. We're using neural networks
where you can weigh input

192
00:12:59.310 --> 00:13:01.560
and then decide whether or
not something is correct. Based

193
00:13:01.560 --> 00:13:04.500
on that, on that weighing,
I don't necessarily need to

194
00:13:04.500 --> 00:13:07.380
understand the advanced math, but
I do need to understand

195
00:13:07.800 --> 00:13:09.960
what's good. Like if I
decide to do a face

196
00:13:09.960 --> 00:13:14.640
tracking system, I shouldn't just
blindly throw faces at, at

197
00:13:14.640 --> 00:13:16.680
a, at an API somewhere
and hope it works out.

198
00:13:16.680 --> 00:13:18.450
I should do a little
research and understand if I

199
00:13:18.690 --> 00:13:22.920
am I using an appropriate
tool for the job. And

200
00:13:22.920 --> 00:13:26.460
this is always open. If,
if you have more questions

201
00:13:26.460 --> 00:13:30.000
about why a certain function
is working, you can always

202
00:13:30.060 --> 00:13:34.410
decide to read about it
in more detail about the

203
00:13:34.410 --> 00:13:37.860
math, if you're interested in
it, but for applying machine

204
00:13:37.860 --> 00:13:42.840
learning to solve interesting problems,
you don't need to go

205
00:13:42.840 --> 00:13:49.320
really deep. Okay, there's this,
there's this really helpful website

206
00:13:49.830 --> 00:13:56.340
called fast.ai. And they're, they're
actually offering a free seven

207
00:13:56.340 --> 00:14:00.210
lesson course on deep learning.
And this is done by

208
00:14:00.810 --> 00:14:06.510
Jeremy Howard and Rachel Thomas. Jeremy is
one of the top contributors

209
00:14:06.630 --> 00:14:09.810
at Kaggle. And I think
he, he was ahead data

210
00:14:09.810 --> 00:14:14.490
scientists there and their philosophy
is to make deep learning

211
00:14:14.700 --> 00:14:20.070
on cool again, by, by
this, they mean that a

212
00:14:20.070 --> 00:14:24.030
lot of people and experts,
like you said, there are

213
00:14:24.030 --> 00:14:26.280
a lot of buzzwords and
they make you think you

214
00:14:26.280 --> 00:14:30.840
need thousands of GPU and
millions of data. When in

215
00:14:30.840 --> 00:14:36.780
reality, that is not true.
So this opens the door

216
00:14:36.810 --> 00:14:41.700
to people in developing countries
to consider using this tools

217
00:14:41.700 --> 00:14:46.800
to solve problems in their
communities. Because what Jeremy and

218
00:14:46.800 --> 00:14:51.030
Rachel say is the people
in Silicon Valley. They're not

219
00:14:51.170 --> 00:14:53.690
to be thinking about some
village in a third world

220
00:14:53.690 --> 00:14:58.790
country to solve their problem.
They're using AI. So that's

221
00:14:58.790 --> 00:15:02.870
why everybody should start getting
exposed to this because it's

222
00:15:02.870 --> 00:15:08.440
a very powerful tool. Wow.
Yeah. So fast.ai. It's a

223
00:15:08.440 --> 00:15:13.300
seven week course in part
one practical, deep learning for

224
00:15:13.300 --> 00:15:15.310
coders is really great. I
liked the way that they

225
00:15:15.310 --> 00:15:17.290
said this, because this is,
this is literally talking to

226
00:15:17.290 --> 00:15:20.260
me here. It says, learn
how to build models without

227
00:15:20.260 --> 00:15:25.030
needing graduate level math, but
without dumbing, anything down and

228
00:15:25.030 --> 00:15:27.970
it's free, you can actually
go to fast.ai, go to

229
00:15:27.970 --> 00:15:30.580
chorus.fast AI and check that
out. What a great resource.

230
00:15:30.580 --> 00:15:33.490
Thank you. Yeah. It's a
great resources. One of the

231
00:15:33.490 --> 00:15:37.540
best ones that I've found.
Okay. So then you found

232
00:15:37.780 --> 00:15:42.730
yourself in Microsoft research and
you found that balance between,

233
00:15:43.210 --> 00:15:45.220
you know, the internals of
AI and then what you

234
00:15:45.220 --> 00:15:49.810
called applied machine learning, which
is you understand enough, and

235
00:15:49.810 --> 00:15:52.000
then you can start applying
it practically. And what I

236
00:15:52.000 --> 00:15:54.550
liked about that word applied,
when I think about it

237
00:15:54.580 --> 00:15:58.000
in my own experiences, I
have a degree in software

238
00:15:58.000 --> 00:16:01.990
engineering and you have a
degree in computer science. Have

239
00:16:01.990 --> 00:16:04.900
you ever thought about the
difference between those two practices

240
00:16:04.900 --> 00:16:11.050
as being distinct? Yes, definitely.
Computer science can be more

241
00:16:11.050 --> 00:16:17.560
theoretical, more, more about the
math and proving theorems, specially

242
00:16:17.560 --> 00:16:24.250
in areas like cryptography and
theory of computation while software

243
00:16:24.250 --> 00:16:34.150
engineering is about building high
quality software. That's maintainable understandable

244
00:16:34.510 --> 00:16:39.790
extendable, and it's a really
important set of skills to

245
00:16:39.790 --> 00:16:44.140
have sovereign engineering. Certainly. Yeah.
I've been talking with a

246
00:16:44.140 --> 00:16:47.830
number of people about how
young people learn about how

247
00:16:47.830 --> 00:16:51.700
new software people learn and
Maria on my team. And

248
00:16:51.700 --> 00:16:54.790
I both feel that in
some instances, and this is

249
00:16:54.790 --> 00:16:57.670
a general, a general statement.
So not trying to pick

250
00:16:57.670 --> 00:17:00.670
on anyone who has a
particular degree, but in some

251
00:17:00.670 --> 00:17:03.160
instances, people can come out
of school with a computer

252
00:17:03.160 --> 00:17:08.020
science degree, but not understand
how to write software. Well,

253
00:17:08.590 --> 00:17:10.180
you know, and that can
be a real problem. It's

254
00:17:10.180 --> 00:17:13.600
where we can be setting
people up for failure. I

255
00:17:13.600 --> 00:17:16.750
totally agree with that. And
if you're coming from a

256
00:17:16.930 --> 00:17:23.020
traditional university background, for example,
I did a study abroad

257
00:17:23.200 --> 00:17:27.220
in UT Austin. And also
my undergrad is from Mexico.

258
00:17:28.030 --> 00:17:33.580
There are topic courses which
allow you to choose from

259
00:17:35.320 --> 00:17:42.190
several options and they have
some software engineering courses. But

260
00:17:43.120 --> 00:17:46.810
when I was studying, at
least they're not mandatory. So

261
00:17:46.810 --> 00:17:49.920
you get to choose them.
But in this course, as

262
00:17:49.920 --> 00:17:56.760
they, I learned using get
pair programming, test driven development.

263
00:17:57.660 --> 00:18:02.370
So there are certainly ways
to learn software engineering principles

264
00:18:02.370 --> 00:18:06.900
while you're at school, but
sometimes you have to choose

265
00:18:06.900 --> 00:18:11.940
them for yourself. And another
thing that you can do

266
00:18:11.940 --> 00:18:16.950
is do an internship before
coming full time, because it's

267
00:18:16.950 --> 00:18:20.610
a good 12 week program
where you can learn these

268
00:18:20.610 --> 00:18:25.580
principles before you graduate. Yeah.
It's, it's, it's difficult if

269
00:18:25.580 --> 00:18:28.880
you find yourself with a
deep understanding of the internals

270
00:18:28.880 --> 00:18:32.390
of computers, but most I
am in my experience, most

271
00:18:32.390 --> 00:18:35.060
people then they, when they
do their work at, in

272
00:18:35.060 --> 00:18:38.870
college, it's not always in
groups. So computers is as

273
00:18:38.870 --> 00:18:41.840
much about how they work
as it is about how

274
00:18:41.840 --> 00:18:45.020
to work with other humans.
And if you take someone

275
00:18:45.020 --> 00:18:47.600
who's done six years of
deep research alone in the

276
00:18:47.600 --> 00:18:50.000
library and then drop them
on a, on a diverse

277
00:18:50.000 --> 00:18:52.850
team of 15 people from
all over the world, it

278
00:18:52.850 --> 00:18:55.400
can be an overwhelming experience
to learn how to actually

279
00:18:55.400 --> 00:19:00.080
ship. And this is why
I think we're seeing an

280
00:19:00.080 --> 00:19:05.450
interesting balance at Microsoft research
where more experienced software engineers

281
00:19:05.450 --> 00:19:09.920
and developers are collaborating with
researchers because there's an X,

282
00:19:09.980 --> 00:19:14.540
there's an exchange of knowledge
happening, researchers, all learning about

283
00:19:15.050 --> 00:19:21.830
programming practices and they software
engineers are learning about the

284
00:19:21.830 --> 00:19:24.920
research side of things like
artificial intelligence and things like

285
00:19:24.920 --> 00:19:29.870
that. Interesting. So when I
think of Microsoft research though,

286
00:19:29.960 --> 00:19:32.480
I assume that you're, I
dunno, you're all off making

287
00:19:32.480 --> 00:19:35.300
deep thoughts, right. But you
had said something about the

288
00:19:35.300 --> 00:19:39.230
democratization of AI. Is there
a change of Microsoft research

289
00:19:39.260 --> 00:19:43.940
that has you shipping on
a regular basis? Yeah. So

290
00:19:44.000 --> 00:19:49.880
Santiago recently announced this democratize
AI initiative. What he means

291
00:19:49.880 --> 00:19:53.780
by this is let's take
some of the research that

292
00:19:53.780 --> 00:19:57.800
has been produced at Microsoft
research and embedded in our

293
00:19:57.800 --> 00:20:02.330
products where, where it makes
sense, obviously. So for example,

294
00:20:02.330 --> 00:20:10.610
my team successfully added machine
learning models to outlook and

295
00:20:10.610 --> 00:20:14.420
dynamics. And one example of
where we see artificial intelligence

296
00:20:14.450 --> 00:20:19.160
is if you receive a
message we can detect, Oh,

297
00:20:19.160 --> 00:20:22.670
is this a meeting request?
Or for example, in the

298
00:20:22.670 --> 00:20:28.190
dynamics CRM, you can have
intelligence in, will this lead

299
00:20:28.190 --> 00:20:31.070
me to a sale or
not. And based on this,

300
00:20:31.070 --> 00:20:35.000
you can prioritize which customers
to focus on if you

301
00:20:35.000 --> 00:20:41.720
want. So this is the
whole democratize AI push it's

302
00:20:41.720 --> 00:20:46.340
allowed Merck, not merging two
organizations, but certainly have a

303
00:20:46.840 --> 00:20:51.730
better collaboration to build more
intelligent, more intelligent tools that

304
00:20:51.820 --> 00:20:55.530
can enhance productivity. Does that
make you feel like you

305
00:20:55.530 --> 00:20:59.070
have to find a balance
between the pressure of shipping

306
00:20:59.070 --> 00:21:02.370
something and the time required
to really deeply think about

307
00:21:02.370 --> 00:21:06.720
something and research something? Yes.
There's definitely a balance there

308
00:21:07.020 --> 00:21:10.890
that needs to be taken
into account. I would feel

309
00:21:10.890 --> 00:21:13.050
a little overwhelmed. I would
feel like, wait a second.

310
00:21:13.050 --> 00:21:15.180
I thought I was supposed
to go off and think

311
00:21:15.180 --> 00:21:17.490
deeply and research this, and
now you're making me ship.

312
00:21:19.080 --> 00:21:22.200
There's actually a balance there
because like I mentioned is

313
00:21:22.680 --> 00:21:27.960
finding research that has been
produced and just see if

314
00:21:27.960 --> 00:21:31.260
it's useful in a product.
So there, there's certainly still

315
00:21:31.260 --> 00:21:38.460
a focus in doing research
without the pressure of shipping

316
00:21:38.490 --> 00:21:45.570
something so that there are
the different sites still. And

317
00:21:45.570 --> 00:21:47.460
this hasn't been a problem
for you in your career.

318
00:21:47.520 --> 00:21:49.500
You found it to be
a, you know, a flexible

319
00:21:49.500 --> 00:21:53.310
enough arrangement. Yes, yes. I
found, I found it to

320
00:21:53.310 --> 00:21:56.580
be flexible enough. And what
I like right now is

321
00:21:56.580 --> 00:22:00.180
that a lot of teams
are starting to use deep

322
00:22:00.180 --> 00:22:03.510
learning. So right now what
I'm doing is I'm learning

323
00:22:03.510 --> 00:22:09.930
about this technology and then
trying it out before building

324
00:22:09.930 --> 00:22:14.220
something. So, so I have
a balance time to do

325
00:22:14.220 --> 00:22:17.430
both things. Knowledge technologies group
is the group that you're

326
00:22:17.430 --> 00:22:20.940
working on. And is this
one of many AI groups

327
00:22:20.940 --> 00:22:23.610
within Microsoft research or they're
like a whole splinter groups

328
00:22:23.610 --> 00:22:28.560
of different AIS? Yes. There's
a, a group for machine

329
00:22:28.560 --> 00:22:32.550
translation, which is the people
that worked on the translator

330
00:22:32.550 --> 00:22:36.180
app. And this app allows
you to, to speak to

331
00:22:36.180 --> 00:22:41.790
the phone. And there's another
person that gets the translation

332
00:22:42.000 --> 00:22:44.130
of what you're saying. So
you can be speaking in

333
00:22:44.130 --> 00:22:46.260
English and then I can
see it on my phone

334
00:22:47.040 --> 00:22:51.300
in Spanish. So that that's
one team. There's also the

335
00:22:51.720 --> 00:22:56.490
machine teaching group, which is
about making machine learning tools

336
00:22:56.490 --> 00:23:01.890
more accessible. So there are,
there are several groups and

337
00:23:01.890 --> 00:23:04.890
you can find these groups
and the research areas. If

338
00:23:04.890 --> 00:23:07.710
you go to the Microsoft
research website, it's very neatly

339
00:23:07.710 --> 00:23:14.070
organized the areas of focus
that are available there Since

340
00:23:14.070 --> 00:23:16.920
you work in close to
that, that team I'm as

341
00:23:16.920 --> 00:23:19.260
a, as a person that
is interested in languages, but

342
00:23:19.260 --> 00:23:22.290
never happy with my own
abilities with other languages. I'm

343
00:23:22.290 --> 00:23:25.170
curious if you've noticed a
difference in the last five

344
00:23:25.170 --> 00:23:28.290
or 10 years around machine
translation, because I think we've

345
00:23:28.290 --> 00:23:30.630
all had that experience where
someone sent us something in

346
00:23:30.630 --> 00:23:33.390
a different language and we
copy paste it into being

347
00:23:33.390 --> 00:23:38.610
translator, Google translate, and someone,
someone, some native speaker could

348
00:23:38.610 --> 00:23:42.180
smell that it wasn't right.
They could understand it, but

349
00:23:42.180 --> 00:23:46.070
they go, that's not exactly
how you would say it.

350
00:23:46.750 --> 00:23:48.650
Is it getting better? Have
we been sitting at like

351
00:23:48.650 --> 00:23:51.260
80% and now we're getting
towards a hundred percent? Cause

352
00:23:51.260 --> 00:23:53.210
it seems like it's getting
better, but at the same

353
00:23:53.210 --> 00:23:57.880
time a native speaker can
always smell translation. Yes. I

354
00:23:57.880 --> 00:24:01.840
think that's corrected. It's getting
better, but we still need

355
00:24:01.840 --> 00:24:05.140
a little push on this
area because there are things

356
00:24:05.140 --> 00:24:10.060
like slang and sarcasm that
are still difficult to detect.

357
00:24:10.780 --> 00:24:14.830
But what I've been reading
about is that when the

358
00:24:14.830 --> 00:24:19.390
approach was switched to deep
learning, there are better results,

359
00:24:19.480 --> 00:24:23.680
but I haven't seen examples
of these results yet. So

360
00:24:23.680 --> 00:24:25.630
I'll definitely take a look
at that. But what I

361
00:24:25.630 --> 00:24:30.880
see from Google translate been
translated is that it's good.

362
00:24:30.910 --> 00:24:34.690
But yes I can detect
it was not translated by

363
00:24:34.690 --> 00:24:38.890
a human One day, one
day, we'll have the universal

364
00:24:38.890 --> 00:24:41.890
translator where we can Skype
each speak our native language

365
00:24:41.890 --> 00:24:44.890
and then it'll come out
nicely. Yes. Yes, I hope

366
00:24:44.890 --> 00:24:48.490
so. What about the, we
talked about the different buzzwords,

367
00:24:48.490 --> 00:24:52.120
artificial intelligence machine learning and
deep learning, and we kind

368
00:24:52.120 --> 00:24:54.430
of broken them down so
that they aren't buzzwords anymore.

369
00:24:54.730 --> 00:24:58.240
But what about the term
data science? It's a giant

370
00:24:58.240 --> 00:25:01.180
field. It's a very vast
field. If someone had data

371
00:25:01.180 --> 00:25:03.640
scientist on their resume, I
wouldn't know what that meant.

372
00:25:04.870 --> 00:25:09.880
Yes. When I think data
scientist, I think a lot

373
00:25:09.880 --> 00:25:16.840
of visualizations, but maybe that's
from the, the little exposure

374
00:25:16.840 --> 00:25:20.920
that I've had here at
Microsoft. So it's more about

375
00:25:23.290 --> 00:25:28.360
visualizing the data, but I
might be wrong and about

376
00:25:29.620 --> 00:25:34.240
taking decisions based on this.
So I think it's a

377
00:25:34.240 --> 00:25:39.040
little higher level than deep
learning and machine learning, but

378
00:25:39.040 --> 00:25:42.130
I think there's some machine
learning involved in data science,

379
00:25:42.130 --> 00:25:45.820
for example, clustering, I would
say. So I think it's

380
00:25:45.820 --> 00:25:50.200
a little higher level. Yeah.
It seems very big. It

381
00:25:50.200 --> 00:25:54.520
seems like the big tent,
because it's, it's, data-driven science,

382
00:25:54.880 --> 00:25:59.590
it's trying to take other
giant, giant worlds like statistics

383
00:25:59.590 --> 00:26:04.090
and data analysis in a
general term and saying, and

384
00:26:04.090 --> 00:26:05.890
then now you just wrap
it all up. You wrap

385
00:26:05.890 --> 00:26:09.850
up all these different things.
I'm a data scientist, but

386
00:26:09.850 --> 00:26:12.310
I mean, I assume you've
got statistical learning. You've got

387
00:26:12.310 --> 00:26:15.010
data mining, you've got machine
learning, you'd plug it into

388
00:26:15.460 --> 00:26:19.180
it. Is it maybe it's
just being a statistician except

389
00:26:19.180 --> 00:26:23.080
it's more fun to say
data scientists. Yes. That could

390
00:26:23.080 --> 00:26:26.650
be. And since you brought
up data science, what I'm

391
00:26:26.650 --> 00:26:31.090
thinking about now is an
important concept in all this

392
00:26:31.090 --> 00:26:35.140
artificial intelligence and why diversity
is important is because we

393
00:26:35.140 --> 00:26:43.090
have this data sense that
are gathered, but sometimes they

394
00:26:43.800 --> 00:26:48.030
include diversity. And when I
was at Mtech a few

395
00:26:48.030 --> 00:26:52.350
weeks ago at this AI
conference, there was a woman

396
00:26:52.440 --> 00:26:57.510
from the film industry saying
I'm a minority, I'm a

397
00:26:57.510 --> 00:27:01.290
black woman. So I am
a minority in the dataset

398
00:27:01.680 --> 00:27:06.240
like who is who's building
this data sets because her

399
00:27:06.240 --> 00:27:11.520
concern was this. This was
just an example. She brought

400
00:27:11.520 --> 00:27:12.960
up. I don't know if
this is how it works,

401
00:27:12.990 --> 00:27:16.860
but she's saying in LinkedIn,
if my data sets say

402
00:27:16.860 --> 00:27:21.570
that white men get a
job in tech, what is

403
00:27:21.570 --> 00:27:24.960
the recommendation system going to
be always, you're going to

404
00:27:24.960 --> 00:27:28.620
recommend white men for those
positions. So she had a

405
00:27:28.620 --> 00:27:34.230
concern about this. So a
lot of work needs to

406
00:27:34.230 --> 00:27:38.880
be done, to think about
the data sense. And another

407
00:27:38.880 --> 00:27:44.310
example we've seen is Snapchat.
Wasn't recognizing some people in

408
00:27:44.310 --> 00:27:48.870
their filters. So this is
also why we should have

409
00:27:48.870 --> 00:27:52.200
more diverse teams working in
AI because it's going to

410
00:27:52.200 --> 00:27:56.070
be a pretty big area.
And we need people with

411
00:27:56.070 --> 00:27:59.850
diverse backgrounds to raise these
questions and prove that the

412
00:27:59.850 --> 00:28:04.460
algorithms don't work for certain
minorities. Yeah. This is a

413
00:28:04.460 --> 00:28:08.480
really important thing. I think
that from a scientist scientific

414
00:28:08.480 --> 00:28:10.640
perspective, this is important. And
the, the word that I

415
00:28:10.640 --> 00:28:13.070
like to use diversity is
a great word, but I

416
00:28:13.070 --> 00:28:16.610
really like the word inclusive.
If you include everybody, then

417
00:28:16.610 --> 00:28:18.590
much better. Things are going
to happen. And we see

418
00:28:18.590 --> 00:28:22.100
this as users. If you
go and Google, like, you

419
00:28:22.100 --> 00:28:25.100
know, white teenager on Google,
and then look at the

420
00:28:25.100 --> 00:28:28.400
images, you'll see one thing.
And if you Google black

421
00:28:28.400 --> 00:28:31.640
teenager, you might see another
thing and you, you know,

422
00:28:31.640 --> 00:28:35.090
you might want to understand,
well, why are those images

423
00:28:35.090 --> 00:28:37.610
being chosen? But they're being
chosen because of the way

424
00:28:37.610 --> 00:28:42.590
that the, the weighing system
of the artificial intelligence that

425
00:28:42.680 --> 00:28:46.070
powers Google, right? And your
point about having the correct

426
00:28:46.070 --> 00:28:48.920
dataset. When, when remember when
the connect came out very,

427
00:28:48.920 --> 00:28:53.600
very early on, on the
Xbox, it couldn't see my,

428
00:28:53.870 --> 00:28:56.750
my black wife. And she
was very frustrated that she

429
00:28:56.750 --> 00:28:59.360
couldn't be seen by the
connect. And then for a

430
00:28:59.360 --> 00:29:02.570
couple of firmware updates in
some beta testing later, when

431
00:29:02.570 --> 00:29:05.630
they went and had all
different people from, you know,

432
00:29:05.630 --> 00:29:08.750
different skin tones get looked
at, then suddenly she could

433
00:29:08.750 --> 00:29:10.940
be seen by connect and
felt much happier about it.

434
00:29:11.210 --> 00:29:13.730
But you can catch those
things early. I'm hearing you

435
00:29:13.730 --> 00:29:17.240
say with a, with a
team that's inclusive. Yes. Because

436
00:29:17.810 --> 00:29:20.570
if a team that's inclusive
is working on this, they'll,

437
00:29:21.230 --> 00:29:24.050
they'll all have tried it
out and figured out, Oh,

438
00:29:24.050 --> 00:29:28.520
it doesn't work for me.
Why? Exactly, exactly. That's a

439
00:29:28.520 --> 00:29:33.590
really important thing to know.
So people should check out.

440
00:29:33.710 --> 00:29:38.120
I'm hearing you say the
fast.ai, which is the free

441
00:29:38.120 --> 00:29:43.090
course, and that's actually by
the, one of the individuals

442
00:29:43.090 --> 00:29:46.630
in that team was number
two on the Kaggle contest,

443
00:29:46.630 --> 00:29:50.800
Jeremy Howard. And then Rachel Thomas has
a PhD from Duke and

444
00:29:50.800 --> 00:29:53.860
she is involved as on
F fast AI in those

445
00:29:53.860 --> 00:29:57.180
courses as well. And that's
all free. Yes. And if

446
00:29:57.180 --> 00:30:01.020
you want to start learning
machine learning, the one that

447
00:30:01.020 --> 00:30:04.800
I highly recommend is intro
to machine learning from your

448
00:30:04.800 --> 00:30:09.180
Udacity, because it's very hands
on throughout the course. And

449
00:30:09.480 --> 00:30:12.810
the lectures are very short.
They're two minute videos, and

450
00:30:12.810 --> 00:30:16.140
then you get to type
a code snippet and Python.

451
00:30:17.520 --> 00:30:20.550
Plus you get to use
the Enron data set. So

452
00:30:20.670 --> 00:30:24.180
it's very hands on, and
it's a really good intro

453
00:30:24.210 --> 00:30:26.310
to this area for those
that are coming from a

454
00:30:26.820 --> 00:30:31.140
software engineering background. Very cool.
And then people can check

455
00:30:31.140 --> 00:30:37.560
out your podcast@thewomenintechshow.com and subscribe
on iTunes. Yes, that's correct.

456
00:30:38.160 --> 00:30:40.050
Fantastic. Well, thank you so
much for chatting with me

457
00:30:40.050 --> 00:30:42.600
today. Thank you for having
me, Scott. It was great

458
00:30:42.600 --> 00:30:46.110
talking to you. This has
been another episode of Hansel

459
00:30:46.110 --> 00:30:48.270
minutes, and we'll see you
again next week.

