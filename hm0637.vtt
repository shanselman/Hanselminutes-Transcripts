WEBVTT FILE

1
00:00:00.480 --> 00:00:04.230
Hey folks. Today's episode was
sponsored by Datadog it's. A

2
00:00:04.230 --> 00:00:08.460
cloud scale monitoring and analytics
platform. Datadog was built to

3
00:00:08.460 --> 00:00:12.510
bring clarity to complex dynamic
applications in the cloud, on

4
00:00:12.510 --> 00:00:16.890
premises in containers or wherever
they run with beautiful dashboards,

5
00:00:16.980 --> 00:00:21.360
distributed, tracing, and seamless integrations
with more than 200 technologies,

6
00:00:21.780 --> 00:00:26.190
including Azure and Docker and
PagerDuty Datadog provides deep end

7
00:00:26.190 --> 00:00:28.860
to end visibility into the
health and performance of modern

8
00:00:28.860 --> 00:00:33.300
apps. Visualize key metrics, set
alerts to identify anomalies and

9
00:00:33.300 --> 00:00:36.570
collaborate with your team to
troubleshoot and fix issues fast.

10
00:00:37.110 --> 00:00:39.330
I'd like you to try
it yourself by starting a

11
00:00:39.330 --> 00:00:43.290
free 14 day trial today,
listeners of this podcast will

12
00:00:43.290 --> 00:00:49.530
also get a free Datadog
t-shirt get started at bitly.com/data

13
00:00:49.530 --> 00:01:08.670
dog shirt. That's bitly.com/datadog shirt.
Hi, this is Scott Hanselman.

14
00:01:08.670 --> 00:01:11.400
This is another episode of
Hanselminutes today. I'm talking with

15
00:01:11.490 --> 00:01:14.220
April Wensel. He's a founder
of compassionate coding, and you

16
00:01:14.220 --> 00:01:16.410
can check her out at
April Wensel, w E N

17
00:01:16.410 --> 00:01:20.480
S E l.com. Thanks for
chatting with me. Hello. Hi,

18
00:01:20.480 --> 00:01:23.270
happy to be here. I'm
looking at your website. I've

19
00:01:23.270 --> 00:01:27.050
seen you online and you
give lots of talks about

20
00:01:27.320 --> 00:01:32.210
this concept of, of compassionate
coding. And I, I, when

21
00:01:32.210 --> 00:01:34.730
I was talking to you
earlier, I, I said, well,

22
00:01:34.730 --> 00:01:38.030
is this like soft skills
or squishy sales skills? And

23
00:01:38.030 --> 00:01:41.000
you checked me and, and
I appreciated the check. Why,

24
00:01:41.030 --> 00:01:44.090
why is that a wrong
word to use? Yeah. So

25
00:01:44.090 --> 00:01:47.720
when we use terms like
soft or squishy, that doesn't

26
00:01:47.720 --> 00:01:49.850
sound like something that people
would care about, right? I

27
00:01:49.850 --> 00:01:53.960
mean, especially because the connotations
with soft, our weakness, we

28
00:01:53.960 --> 00:01:59.210
say things like he's gone
soft and it's also associated,

29
00:01:59.210 --> 00:02:01.970
I think, to a certain
extent with women. And to

30
00:02:01.970 --> 00:02:03.890
be honest, not that there's
anything wrong with being associated

31
00:02:03.890 --> 00:02:07.400
with women, but soft quote,
soft skills are important for

32
00:02:07.400 --> 00:02:10.250
everyone to develop, not just
women. And so I think

33
00:02:10.250 --> 00:02:14.360
that this association is, is
harmful because it keeps people

34
00:02:14.360 --> 00:02:17.960
from giving these skills, the
importance that, that they really

35
00:02:17.960 --> 00:02:20.540
should have. And on top
of that, I mean, what

36
00:02:20.540 --> 00:02:24.320
we're talking about here are
a complex kind of psychological

37
00:02:24.830 --> 00:02:28.940
phenomenon that really aren't really
soft, even in that, you

38
00:02:28.940 --> 00:02:33.140
know, area they're very complicated,
there's science behind it. So

39
00:02:33.440 --> 00:02:36.560
in that sense too, there
are definitely hard in terms

40
00:02:36.560 --> 00:02:41.210
of difficulty and complexity as
well. Yeah. You've also talked

41
00:02:41.210 --> 00:02:44.720
a lot online about the
idea of emotional intelligence and

42
00:02:44.720 --> 00:02:47.960
I've always heard, you know,
IQ and E Q and

43
00:02:48.170 --> 00:02:50.630
it seems like we, we
really promote this idea that

44
00:02:50.630 --> 00:02:53.330
people should be smart and
IQ is so important, but

45
00:02:53.420 --> 00:02:56.570
I've never taken a class
on EPQ or learning about

46
00:02:56.600 --> 00:02:59.980
emotional intimacy. Yeah. Which is,
I feel like that's the

47
00:02:59.980 --> 00:03:02.710
case for most people, we
are starting to see some

48
00:03:03.310 --> 00:03:08.380
social, emotional learning happening in,
in school curriculum. But I

49
00:03:08.380 --> 00:03:10.450
think right now, only out
in California and places like

50
00:03:10.450 --> 00:03:12.940
that, and not, not kind
of in the mainstream, but,

51
00:03:13.240 --> 00:03:16.300
but empathy, you know, that's
one, one aspect of emotional

52
00:03:16.300 --> 00:03:19.120
intelligence. Like it's definitely a
skill that can be learned.

53
00:03:19.120 --> 00:03:21.130
It's not just something you're
either born with or not

54
00:03:21.280 --> 00:03:24.570
born with. You know, That's
very interesting. Why is it

55
00:03:24.570 --> 00:03:26.790
that we seem to think
that people are born smart

56
00:03:26.790 --> 00:03:29.850
or not smart, or they're
born with an emotional, you

57
00:03:29.850 --> 00:03:32.640
know, core emotional intelligence. It
seems like everything can be

58
00:03:32.640 --> 00:03:36.150
learned and we should be
promoting that. Yeah, definitely. I'm

59
00:03:36.150 --> 00:03:39.240
a big fan of Carol
Dweck work on growth mindset,

60
00:03:39.270 --> 00:03:42.360
which is this idea just
like you said, that whatever

61
00:03:42.360 --> 00:03:44.520
skill it is that we
want to develop, it doesn't

62
00:03:44.520 --> 00:03:46.680
matter where we're starting from.
We can get better. We

63
00:03:46.680 --> 00:03:50.610
may not, you know, there
may be an eventual limit

64
00:03:50.610 --> 00:03:52.380
that we hit or, you
know, it may take us

65
00:03:52.380 --> 00:03:55.140
longer than other people, but
all of us are capable

66
00:03:55.140 --> 00:03:58.740
of growing whatever skills we
decide to. Yeah. That's interesting

67
00:03:58.740 --> 00:04:01.500
that you bring up the
growth growth mindset, because I'm

68
00:04:01.500 --> 00:04:04.650
thinking about my two kids
while I just made an

69
00:04:04.650 --> 00:04:08.220
assertion that they're not learning
about emotional intelligence. In fact,

70
00:04:08.220 --> 00:04:10.320
the growth mindset has been
a part of the curriculum

71
00:04:10.320 --> 00:04:12.750
at their school. So that's
cool that you brought up

72
00:04:12.750 --> 00:04:15.000
Carol Dweck. Yeah. Yeah. I'm
a big fan of hers

73
00:04:15.170 --> 00:04:17.040
and her work. And, you
know, cause I think there's

74
00:04:17.040 --> 00:04:21.390
this idea in tech, especially
that, you know, you're either

75
00:04:21.390 --> 00:04:24.270
born to be a coder
or, or not, you know,

76
00:04:24.270 --> 00:04:27.420
and that either I think
Jeff Atwood, hadn't this blog

77
00:04:27.420 --> 00:04:29.370
post where you said that
the computers chose him or

78
00:04:29.370 --> 00:04:31.410
something like that. And it's
like, you know, for some

79
00:04:31.410 --> 00:04:33.630
people that's the case, that's
their origin story. You know,

80
00:04:33.630 --> 00:04:35.430
that's, that's how they feel.
That's how they frame it.

81
00:04:35.700 --> 00:04:38.850
But that's not the case
for everyone. Some people, you

82
00:04:38.850 --> 00:04:41.280
know, were interested in what
they could build, interested in

83
00:04:41.280 --> 00:04:43.500
solving a specific problem. And
so they learn to code,

84
00:04:43.770 --> 00:04:45.630
or we have a lot
of people coming to coding

85
00:04:45.630 --> 00:04:48.540
now later in life, you
know, taking bootcamps or being

86
00:04:48.540 --> 00:04:51.650
self-taught or whatever. And those
people are, you know, I

87
00:04:51.690 --> 00:04:53.820
believe that we should welcome
them just as much as

88
00:04:53.820 --> 00:04:56.820
we would welcome anybody who
thinks that they were born,

89
00:04:56.850 --> 00:04:59.400
you know, to be a
coder. Yeah. That's a really,

90
00:04:59.400 --> 00:05:01.080
I really liked the way
you phrased that this idea

91
00:05:01.080 --> 00:05:05.670
of the programmers origin story
really resonates with me because

92
00:05:06.060 --> 00:05:09.120
you know, there are tropes
in the superhero origin story.

93
00:05:09.120 --> 00:05:11.190
As somebody gets bit by
a radioactive spider or they'd

94
00:05:11.190 --> 00:05:14.580
fall into a VAT of
whatever. And there's the origin

95
00:05:14.580 --> 00:05:16.680
story of, I was given
an Apple two at age

96
00:05:16.710 --> 00:05:20.370
12, but why is that
any more interesting or more

97
00:05:20.370 --> 00:05:22.740
valuable than I was a
teacher for 20 years? And

98
00:05:22.740 --> 00:05:24.150
I decided I want to
get into tech and I

99
00:05:24.150 --> 00:05:26.130
quit my job. And at
45 decided to become a

100
00:05:26.130 --> 00:05:28.710
programmer. That's a pretty cool
origin story too, isn't it?

101
00:05:28.950 --> 00:05:32.520
Yeah. It really is. You
did an interview with a

102
00:05:32.580 --> 00:05:36.120
building programmers and I liked
that idea that the programmers

103
00:05:36.120 --> 00:05:40.560
are built. They're not, they're
not, they're not born. Yeah.

104
00:05:40.620 --> 00:05:43.870
I think that's really, that's
really true. So when, when

105
00:05:43.870 --> 00:05:46.380
we talk about these terms
and, and now I'm not

106
00:05:46.380 --> 00:05:49.140
going to use terms like
soft and squishy, what should

107
00:05:49.140 --> 00:05:51.480
I say? Cause I want
to make a compassionate environment.

108
00:05:51.480 --> 00:05:55.230
I want everyone to feel
that they are they're valued,

109
00:05:55.230 --> 00:05:57.800
but you know, they say
in programming, naming is hard,

110
00:05:57.890 --> 00:06:00.760
right? So here we are
looking to name stuff. Yeah.

111
00:06:00.790 --> 00:06:03.310
The, the, the interesting thing
about that is one of

112
00:06:03.310 --> 00:06:05.500
the reasons the naming is
hard is that naming actually

113
00:06:05.500 --> 00:06:08.810
requires a great deal of
empathy because naming, and that

114
00:06:08.830 --> 00:06:11.410
includes the naming variables because
the idea is you're not

115
00:06:11.410 --> 00:06:13.300
naming them for the machine.
The machine doesn't care what

116
00:06:13.300 --> 00:06:15.520
you call the variables and
there's no right or wrong.

117
00:06:15.550 --> 00:06:19.480
There's no perfect name in
terms of, you know, actual

118
00:06:19.810 --> 00:06:23.770
correctness naming is really about
who it's for and who

119
00:06:23.770 --> 00:06:26.110
it's going to help. So
in the case of variables,

120
00:06:26.110 --> 00:06:28.360
you're really naming based on,
who's going to be reading

121
00:06:28.360 --> 00:06:31.360
the code, including yourself, your
future self, and also any

122
00:06:31.420 --> 00:06:35.050
of your collaborators. And so
in this case for skills,

123
00:06:36.160 --> 00:06:37.450
I mean, one thing is
just to refer to them

124
00:06:37.450 --> 00:06:40.180
more specifically, like, are you
talking about communication skills or

125
00:06:40.180 --> 00:06:43.720
are you talking about self-awareness
introspection just be more specific

126
00:06:43.720 --> 00:06:46.900
instead of kind of lumping
them together. Another option is,

127
00:06:47.230 --> 00:06:49.990
you know, we talked about
emotional intelligence, emotional intelligence skills

128
00:06:50.020 --> 00:06:53.500
would also be good. Some
people use something like professional

129
00:06:53.500 --> 00:06:55.630
skills, but again, that's kind
of vague and it's hard

130
00:06:55.630 --> 00:06:58.540
for people to know exactly
what that means. I gave

131
00:06:58.540 --> 00:07:01.000
a talk and I talked
about, well, it's a talk

132
00:07:01.000 --> 00:07:02.800
that I do frequently. And
I, I refer to them

133
00:07:02.800 --> 00:07:05.740
as catalytic skills. It's a
term I borrowed from Daniel

134
00:07:05.740 --> 00:07:08.290
Goldman, who is pretty big
in the emotional intelligence world.

135
00:07:08.740 --> 00:07:12.340
And this idea of catalytic
skills is that they help

136
00:07:12.340 --> 00:07:15.850
you catalyze your other skills.
So for example, they may

137
00:07:15.850 --> 00:07:19.540
help you learn new technical
skills. They may help you

138
00:07:19.540 --> 00:07:22.750
decide which technical skills to
use, you know, which coding,

139
00:07:22.780 --> 00:07:25.150
which coding framework to use
or what, not that that

140
00:07:25.150 --> 00:07:29.200
requires certain amount of problem
solving ability. That's not actually

141
00:07:29.200 --> 00:07:32.530
directly a coding skill. It's
more sort of a meta

142
00:07:32.530 --> 00:07:35.290
skill, so to speak. So
there's just a whole range

143
00:07:35.290 --> 00:07:37.630
of things you could call
them other than soft skills.

144
00:07:37.630 --> 00:07:40.300
And I think it depends
on your audience. Really. Yeah.

145
00:07:40.540 --> 00:07:42.550
I think my initial reaction
is that it might be

146
00:07:42.550 --> 00:07:45.640
hard to sell a more
complicated word, but what the

147
00:07:45.640 --> 00:07:48.970
great thing about catalytic or
catalyst is is that it

148
00:07:49.090 --> 00:07:51.940
is, it's more precise. It
says exactly like this is

149
00:07:51.940 --> 00:07:54.370
a thing that precipitates a
thing and it, it, it

150
00:07:54.370 --> 00:07:56.770
causes it a better thing
to happen. It's like it

151
00:07:56.780 --> 00:07:59.470
evokes the sense of change
and chemical reaction in the

152
00:07:59.470 --> 00:08:02.410
body. Yeah. That's, that's what
I like about it too.

153
00:08:02.920 --> 00:08:06.610
It's more, it's more yeah.
Precise as you said. Yeah.

154
00:08:06.630 --> 00:08:09.730
That's interesting. I like that.
So as we try to

155
00:08:09.730 --> 00:08:11.770
promote these kinds of things,
how do we push back

156
00:08:11.770 --> 00:08:13.660
on people that are like,
Oh yeah, that's just a

157
00:08:13.660 --> 00:08:15.310
word. It's not, it's not
that big of a deal

158
00:08:15.310 --> 00:08:18.760
who cares? Yeah, yeah, no,
I hear that a lot.

159
00:08:18.830 --> 00:08:22.480
And it's the power of
language is, is surprising. I

160
00:08:22.480 --> 00:08:23.980
mean, it's, it does sound
like, Oh, what does it

161
00:08:23.980 --> 00:08:25.660
matter? What we call it?
But the thing is that

162
00:08:26.770 --> 00:08:29.530
we have so many associations
in our head with different

163
00:08:29.530 --> 00:08:32.830
words, you know? Like, and,
and so that can have

164
00:08:32.830 --> 00:08:35.260
a huge impact on how
we actually think about things.

165
00:08:35.260 --> 00:08:38.800
I mean, I think that
that's just, that's just undeniable.

166
00:08:38.800 --> 00:08:40.450
I mean, cause if you
think about the word soft,

167
00:08:40.450 --> 00:08:42.220
I mean, what comes to
mind? Like I think of

168
00:08:42.220 --> 00:08:45.250
like pillows, I think of
Sears had these commercials in

169
00:08:45.250 --> 00:08:48.160
the nineties, the softer side
of Sears. Like I think

170
00:08:48.160 --> 00:08:50.230
of all these things that
just, you know, are nothing

171
00:08:50.230 --> 00:08:54.340
that sound, you know, important
or like something that I

172
00:08:54.340 --> 00:08:57.300
would need in the, whereas,
you know, I hear catalytic

173
00:08:57.300 --> 00:09:00.570
and I think, you know,
science and chemical reactions like

174
00:09:00.570 --> 00:09:03.180
you were talking about, I
think of, you know, making

175
00:09:03.180 --> 00:09:05.520
things more efficient, faster, all
that stuff. And so it

176
00:09:05.520 --> 00:09:07.050
sounds, you know what I
mean? So I think if

177
00:09:07.050 --> 00:09:08.970
we look at the associations
we have in our head,

178
00:09:09.720 --> 00:09:12.050
that's how we can see
the power of language. That's

179
00:09:12.050 --> 00:09:13.880
a really good point. And
if, if someone were trying

180
00:09:13.880 --> 00:09:16.160
to do a soft skills
workshop and sell it to

181
00:09:16.160 --> 00:09:19.790
like GE or some giant
corporation, that might be challenging,

182
00:09:19.790 --> 00:09:22.610
but if you put together
a catalytic skills workshop, you

183
00:09:22.610 --> 00:09:24.770
know, there's a certain personality
type at a giant company

184
00:09:24.770 --> 00:09:26.390
that might be like, yeah,
that's exactly what we need

185
00:09:26.390 --> 00:09:30.440
here. Yeah, exactly. I'm doing
a workshop at node summit

186
00:09:30.440 --> 00:09:32.990
coming up in the summer
and I called it working

187
00:09:32.990 --> 00:09:35.660
with humans and that's really
cause that's what it's about

188
00:09:35.660 --> 00:09:37.340
is like how to, how
to work well with humans

189
00:09:37.370 --> 00:09:40.100
as a human. And so
that's kind of how I

190
00:09:40.100 --> 00:09:43.370
described it there. That is
funny though, we are working

191
00:09:43.370 --> 00:09:46.190
with computers that are supposed
to be deterministic and they

192
00:09:46.190 --> 00:09:49.310
follow instructions really well. And
then of course the exact

193
00:09:49.310 --> 00:09:52.520
opposite of that are people
which are don't follow instructions

194
00:09:52.520 --> 00:09:55.460
particularly well and are hard
to understand sometimes, but it

195
00:09:55.460 --> 00:09:58.160
is the, the, the, the
coming together of those two

196
00:09:58.160 --> 00:10:01.610
things that we're we're software
happens. Right. Exactly. Yeah. And

197
00:10:01.610 --> 00:10:03.920
I think for too long,
we've sort of been neglecting

198
00:10:03.950 --> 00:10:05.870
the human side of things.
And I think you see

199
00:10:05.870 --> 00:10:08.990
this in the fact that,
you know, most software projects

200
00:10:09.050 --> 00:10:11.750
are not failing because of
technical reasons really they're, they're

201
00:10:11.750 --> 00:10:15.890
failing for human reasons. It's,
you know, people not communicating

202
00:10:15.890 --> 00:10:18.980
requirements, they're not prioritizing them
well or not designing for

203
00:10:18.980 --> 00:10:21.050
the customer with the customers
in mind and things like

204
00:10:21.050 --> 00:10:24.740
that. So I think this
gap, this, you know, empathy

205
00:10:24.740 --> 00:10:28.280
gap and just emotional intelligence
gap in general, that's been

206
00:10:28.280 --> 00:10:32.600
in tech for, you know,
forever has really been, we

207
00:10:32.600 --> 00:10:34.520
can see the evidence of
it in, in what the

208
00:10:34.520 --> 00:10:37.070
technology that's been produced. So
I think it's not just,

209
00:10:37.070 --> 00:10:39.020
you know, Oh, let's just,
you know, be nice to

210
00:10:39.020 --> 00:10:40.820
each other or whatever. It's
more like, no, let's be

211
00:10:40.820 --> 00:10:44.120
more efficient and build better
products by building empathy for

212
00:10:44.120 --> 00:10:46.760
each other. And the people
we're building for, That's a

213
00:10:46.760 --> 00:10:50.780
really like actionable thing that
you just said, like, you're

214
00:10:50.780 --> 00:10:53.780
not out promoting, come on
everyone. Let's be nice. Let's

215
00:10:53.780 --> 00:10:55.700
all get along. Like, Hey,
listen, if we want this

216
00:10:55.700 --> 00:10:58.970
to be a successful project,
you working effectively with the

217
00:10:58.980 --> 00:11:01.430
human next to you or
the cube over from you

218
00:11:01.790 --> 00:11:05.780
is as important as your
code compiling. Yeah, absolutely. And

219
00:11:05.780 --> 00:11:08.510
you've said before that, when
people, when you tell people

220
00:11:08.510 --> 00:11:11.180
that your company is compassionate
coding, they think that's like

221
00:11:11.180 --> 00:11:14.090
an oxymoron. You can't have
one with the other. Yeah,

222
00:11:14.120 --> 00:11:17.380
they definitely, they do, but
I've gotten that feedback. And,

223
00:11:17.380 --> 00:11:19.100
and I've said that in
what I would like is

224
00:11:19.100 --> 00:11:20.990
for people to think it's
redundant. Like I would like

225
00:11:20.990 --> 00:11:23.510
it to be the norm
that, you know, people in

226
00:11:23.510 --> 00:11:27.170
software are known to be
empathetic, that they have this

227
00:11:27.170 --> 00:11:30.470
versatile ability to work both
on machines and with human

228
00:11:30.470 --> 00:11:32.570
beings, you know? Cause I
think that's where the real

229
00:11:32.570 --> 00:11:36.200
potential lies. That would be
amazing if we could make

230
00:11:36.200 --> 00:11:40.580
it. So that, that tech
felt like people knew like,

231
00:11:40.610 --> 00:11:44.030
Oh yeah, computer person, that's,
that's so great. Like this

232
00:11:44.030 --> 00:11:46.040
person that, you know, they're,
they're a great person. They're

233
00:11:46.040 --> 00:11:48.890
a kind person, they're a
computer person. Right. Wouldn't that

234
00:11:48.890 --> 00:11:51.560
be fantastic. That'd be great.
But right now I just

235
00:11:51.560 --> 00:11:53.770
feel like, ah, I don't
want to fix my dad's

236
00:11:53.770 --> 00:11:55.990
computer. You know, I think
we're all just going to

237
00:11:55.990 --> 00:11:58.750
burned out and we are
having enough trouble being compassionate

238
00:11:58.750 --> 00:12:01.780
with ourselves and our family.
It's difficult sometimes to be

239
00:12:01.780 --> 00:12:04.140
patient and kind at work.
Well, that's a really good

240
00:12:04.140 --> 00:12:07.020
point too, which is that
the self compassion is just

241
00:12:07.020 --> 00:12:09.570
as important. I mean, everything
really starts with self compassion.

242
00:12:09.570 --> 00:12:11.430
I mean, I don't think
I could be doing the

243
00:12:11.430 --> 00:12:13.560
work that I'm doing. If
I didn't take enough time

244
00:12:13.560 --> 00:12:15.240
to take care of myself,
take care of my own

245
00:12:15.240 --> 00:12:18.510
health, do the things that
make me happy outside of

246
00:12:18.510 --> 00:12:22.590
work. And I think that's
actually, it's hard to be

247
00:12:22.590 --> 00:12:24.630
empathetic to other people. It's
hard to be compassionate with

248
00:12:24.630 --> 00:12:26.940
other people if you're, if
you're not taking care of

249
00:12:26.940 --> 00:12:28.920
yourself in these ways. So
that's actually a really important

250
00:12:28.920 --> 00:12:32.340
point. Yeah. We hear a
lot online about self care

251
00:12:32.340 --> 00:12:34.680
and I really like communities
where we're telling each other

252
00:12:34.680 --> 00:12:36.570
and helping each other say
like, Hey, listen, take, they

253
00:12:36.570 --> 00:12:38.700
take the time. If you
need a, you know, a

254
00:12:38.730 --> 00:12:40.950
break, you need a mental
health afternoon. You know, if

255
00:12:40.950 --> 00:12:42.990
you're, if, if it's just
not working for you today,

256
00:12:42.990 --> 00:12:45.840
if you can take the
time for yourself because you're

257
00:12:45.840 --> 00:12:49.530
no good to us stressed
out. Definitely. Yeah. I think

258
00:12:49.530 --> 00:12:51.990
that's really important. And I
would also say that, you

259
00:12:51.990 --> 00:12:55.010
know, this idea of, of,
you know, with the, the

260
00:12:55.110 --> 00:12:58.080
programmers being kind to people
it's not so far fetched,

261
00:12:58.080 --> 00:13:00.840
because if you think about
it, we're problem solvers, right?

262
00:13:00.840 --> 00:13:03.870
So we look for pain
points and we solve them.

263
00:13:03.870 --> 00:13:06.360
And that's really what compassion
is about is about alleviating

264
00:13:06.360 --> 00:13:09.150
suffering. Right? So reducing these
pain points. So to a

265
00:13:09.150 --> 00:13:12.090
certain extent, technologists are kind
of like primed to do

266
00:13:12.090 --> 00:13:14.760
this. I mean, we, although
you're right. Sometimes we're, we're

267
00:13:14.760 --> 00:13:18.750
burnt out. Understandably. So I
really do think that, you

268
00:13:18.750 --> 00:13:20.790
know, when you see things
to like open source, like,

269
00:13:20.790 --> 00:13:25.020
and people who kindly contribute
on stack overflow, that's a

270
00:13:25.020 --> 00:13:27.420
certain subset, hopefully a large
subset, but it is a

271
00:13:27.420 --> 00:13:31.860
subset. Those people are generous,
right? That's generosity. They're not

272
00:13:31.860 --> 00:13:34.530
getting paid for this and
you know, some of them

273
00:13:34.590 --> 00:13:38.010
and at least it opensource.
And so, you know, I

274
00:13:38.010 --> 00:13:40.530
think that that's important too,
is that we have these

275
00:13:40.530 --> 00:13:43.020
seeds of compassion already in
tech. And that's why I

276
00:13:43.020 --> 00:13:45.420
really have faith that we
can help those grow and

277
00:13:45.420 --> 00:13:49.320
blossom into beautiful completion flowers.
Yeah, indeed. I like the

278
00:13:49.320 --> 00:13:53.880
idea of direct of, of
directly tying compassion and empathy

279
00:13:53.880 --> 00:13:56.670
and, and working well as
a team to, to code

280
00:13:56.670 --> 00:14:00.810
quality that we're, you're not
just saying your customers, your

281
00:14:00.810 --> 00:14:04.740
employees, your users will feel
better. Your, you know, your

282
00:14:04.740 --> 00:14:06.990
employees might not quit or
whatever. You're literally saying the

283
00:14:06.990 --> 00:14:10.020
code will be higher quality.
And the shipping product will

284
00:14:10.020 --> 00:14:13.230
be better if, if everyone
is, is focused on empathy

285
00:14:13.230 --> 00:14:17.100
and compassion. Yeah, absolutely. Because
really for me, I've looked

286
00:14:17.100 --> 00:14:19.290
at compassion as an optimization
problem where you're trying to

287
00:14:19.290 --> 00:14:22.500
minimize suffering. And so when
you're talking about, you know,

288
00:14:22.500 --> 00:14:25.260
should we, should we test
the code? Like, should we,

289
00:14:25.260 --> 00:14:27.840
you know, how, how should
we build this being compassionate

290
00:14:27.840 --> 00:14:29.910
towards yourself and the people
you're working with and the

291
00:14:29.910 --> 00:14:32.370
customer, right? All of these,
if you take those into

292
00:14:32.370 --> 00:14:33.720
account, I mean, that's how
you're going to get the

293
00:14:33.720 --> 00:14:36.480
best code, right? This might
be a bit of a

294
00:14:36.480 --> 00:14:39.180
stretch of an analogy. But
I remember when I took

295
00:14:39.210 --> 00:14:42.570
my very first driving course
when I was 16 and

296
00:14:42.570 --> 00:14:44.550
they did this thing where
they were like, two people

297
00:14:44.550 --> 00:14:46.800
are going to drive in
this simulator, one's going to

298
00:14:46.800 --> 00:14:49.350
drive as aggressively and as
fast as possible. And see

299
00:14:49.350 --> 00:14:51.650
if they get there first
and the one will simply

300
00:14:51.650 --> 00:14:53.930
follow the rules and stop
and, you know, be a

301
00:14:53.930 --> 00:14:57.110
good driver. And you drive
for like 15, 20 minutes.

302
00:14:57.110 --> 00:14:59.600
And the person who drives
aggressively and cuts people off

303
00:14:59.840 --> 00:15:04.310
ends up getting there about
90 seconds, early 90 seconds,

304
00:15:04.430 --> 00:15:07.370
you know, out of 20
minutes. Now, certainly if they

305
00:15:07.370 --> 00:15:09.260
were trying to get to
the hospital, maybe that's necessary,

306
00:15:09.290 --> 00:15:12.020
but they were trying to
make the point that this

307
00:15:12.020 --> 00:15:17.030
person burned a half dozen
bridges was extraordinarily unsafe, pissed

308
00:15:17.030 --> 00:15:20.150
off half the road and
made it there with 15%

309
00:15:20.150 --> 00:15:25.580
more speed. So applying that
to, to code reviews, to

310
00:15:25.580 --> 00:15:30.170
an experience at work, minimal
effort of being kind, being

311
00:15:30.170 --> 00:15:34.040
pleasant would, would cost you
little if nothing, if not

312
00:15:34.040 --> 00:15:38.020
make, maybe make things better.
Yeah. Actually, I actually love

313
00:15:38.020 --> 00:15:41.230
that analogy because really while
driving is one of the

314
00:15:41.230 --> 00:15:44.830
best times to practice empathy
and compassion is, you know,

315
00:15:44.830 --> 00:15:48.190
when you're stuck behind somebody
or somebody cuts you off

316
00:15:48.190 --> 00:15:50.740
or whatever it is, instead
of just, you know, having

317
00:15:50.740 --> 00:15:53.590
a knee jerk, really angry
reaction, but just taking a

318
00:15:53.590 --> 00:15:56.050
minute, like just taking a
breath and thinking, you know,

319
00:15:56.110 --> 00:15:57.880
I don't know what that
person's going through. You know,

320
00:15:57.880 --> 00:15:59.620
what, what might be going
on in their life. That's

321
00:15:59.620 --> 00:16:01.330
making them act this way.
And that's like a great

322
00:16:01.330 --> 00:16:03.820
thing that all of us
who drive or commute in

323
00:16:03.820 --> 00:16:06.580
any way really have the
option to do is just

324
00:16:06.580 --> 00:16:10.300
to, to really, to think
about in practice, you know,

325
00:16:10.300 --> 00:16:13.840
that, that concern for other
people, Hey folks, I want

326
00:16:13.840 --> 00:16:16.210
to take a moment to
thank our sponsors and remind

327
00:16:16.210 --> 00:16:19.270
you that when you support
them, you're supporting Hansel minutes

328
00:16:19.270 --> 00:16:23.710
because they make this show
possible. Innovative developers help improve

329
00:16:23.710 --> 00:16:26.530
the lives of millions, of
small businesses and accountants that

330
00:16:26.530 --> 00:16:30.190
use QuickBooks worldwide. Your app
could make the difference that

331
00:16:30.190 --> 00:16:33.220
helps them work smarter and
put more money in their

332
00:16:33.220 --> 00:16:37.630
pocket, enter into it. Developer
from tech tools to growing

333
00:16:37.630 --> 00:16:41.350
your customer base, to helping
you get funding. Intuit developer

334
00:16:41.350 --> 00:16:44.980
helps your app go from
concept to company, find out

335
00:16:44.980 --> 00:16:53.740
more at intuit.me/code that's I
N T U I t.me/code.

336
00:16:54.370 --> 00:16:58.120
It makes me wonder when
did things get so, so

337
00:16:58.120 --> 00:17:00.790
angry and so selfish? When
did the idea of compassion

338
00:17:00.790 --> 00:17:05.290
or empathy become synonymous with
some kind of weakness? Yeah,

339
00:17:05.320 --> 00:17:07.510
there's an interesting history there
and I don't know all

340
00:17:07.510 --> 00:17:10.960
of it. There's a great
book called on kindness. And

341
00:17:10.990 --> 00:17:14.320
it's about it kind of
charts the history over time,

342
00:17:14.320 --> 00:17:17.500
because there was a time,
you know, especially cause there's,

343
00:17:17.500 --> 00:17:19.360
you know, it ties into
religion in some ways. I

344
00:17:19.360 --> 00:17:21.730
mean, not for me personally,
but there's definitely a history

345
00:17:21.730 --> 00:17:24.580
there of when religion was
bigger in society, like certain

346
00:17:24.580 --> 00:17:27.910
types of kindness were more
popular and then, you know,

347
00:17:27.910 --> 00:17:31.270
this industrialization happened too. And
then it's like, you know,

348
00:17:31.360 --> 00:17:34.390
than it is all about
efficiency. But the thing is

349
00:17:34.390 --> 00:17:37.450
that kindness really is compatible
with efficiency, which is how

350
00:17:37.450 --> 00:17:40.030
I think this new kind
of resurgence of kindness is

351
00:17:40.030 --> 00:17:42.250
going to happen is that
it's when you speak with

352
00:17:42.250 --> 00:17:44.980
emotional intelligence, when you're speaking
to somebody, they are more

353
00:17:44.980 --> 00:17:49.740
likely to receive the feedback
or suggestions you're giving because

354
00:17:50.160 --> 00:17:52.050
they're not, it's not going
to trigger their threat response.

355
00:17:52.080 --> 00:17:53.940
So if you come at
somebody and you're like yelling

356
00:17:53.940 --> 00:17:57.120
at them, or just even
just being direct with them,

357
00:17:57.570 --> 00:18:01.140
overly direct, they're gonna, they're,
it's going to trigger their

358
00:18:01.140 --> 00:18:03.930
threat response, their fight or
flight. And so they're not

359
00:18:03.930 --> 00:18:06.320
really going to listen to
your message. Right? Right. And

360
00:18:06.320 --> 00:18:09.980
as a manager of people,
a manager of developers, if

361
00:18:09.980 --> 00:18:13.670
I can express an issue,
a concern, whatever, whether it

362
00:18:13.670 --> 00:18:15.710
be a code review or
a managerial issue with someone

363
00:18:16.070 --> 00:18:19.250
in a way that doesn't
trigger their, their, their threat

364
00:18:19.250 --> 00:18:21.080
response or that they know
it is coming from a

365
00:18:21.080 --> 00:18:23.930
place of helpfulness. Even if
I make a mistake or

366
00:18:23.930 --> 00:18:28.130
say something incorrectly, they won't
necessarily flip the bid on

367
00:18:28.130 --> 00:18:30.830
me and assume that I
had no malice of forethought.

368
00:18:31.010 --> 00:18:34.430
Yeah, exactly. Now Jonathan salt
is someone that you have

369
00:18:34.640 --> 00:18:37.070
tweeted about before, and he's
got a book out now

370
00:18:37.070 --> 00:18:40.280
called a new reality human
evolution for a sustainable future.

371
00:18:40.280 --> 00:18:43.160
And you pulled out a
quote, a pull quote out

372
00:18:43.160 --> 00:18:46.070
of his book where you
said that salt believes that

373
00:18:46.460 --> 00:18:48.710
we as a society are
kind of transitioning from a

374
00:18:48.710 --> 00:18:52.610
time of competition and independence
with wind lose strategies to

375
00:18:52.610 --> 00:18:55.790
a focus on cooperation and
interdependence. Sometimes it doesn't feel

376
00:18:55.790 --> 00:18:58.730
like that. D D do
you believe that? I do

377
00:18:58.730 --> 00:19:01.490
believe that. And like part
of what the book is

378
00:19:01.490 --> 00:19:03.980
about is the fact that
we are in this transition

379
00:19:03.980 --> 00:19:07.400
and that's why there's so
much conflict. There's, you know,

380
00:19:07.400 --> 00:19:10.970
whenever you're going through a
transition there's conflict in the

381
00:19:10.970 --> 00:19:13.160
new age world, people talk
about a healing crisis where

382
00:19:13.160 --> 00:19:15.440
when you're, when you're healing,
like sometimes things get worse

383
00:19:15.440 --> 00:19:17.660
before they get better. And
so I think sometimes it

384
00:19:17.660 --> 00:19:19.970
seems like things are getting
worse now and, and everybody,

385
00:19:19.970 --> 00:19:21.890
you know, but I think
really what's happening is we're

386
00:19:21.890 --> 00:19:24.410
talking about these things and,
and we're, we're trying to

387
00:19:24.410 --> 00:19:27.020
make, to make changes and
to make progress. And, and

388
00:19:27.020 --> 00:19:29.480
it's inevitable that in that
time there's going to be

389
00:19:29.480 --> 00:19:31.130
resistance. There's going to be
the people who want to

390
00:19:31.130 --> 00:19:33.140
protect the status quo. Like
I encounter that all the

391
00:19:33.140 --> 00:19:35.210
time and trying to push
for compassion and tech. It's

392
00:19:35.210 --> 00:19:37.520
like the people who have
been getting by without being

393
00:19:37.520 --> 00:19:40.610
compassionate in tech, like really
don't want to be compassionate

394
00:19:40.610 --> 00:19:43.910
because it's, you know, it
does take effort and they're,

395
00:19:43.910 --> 00:19:47.390
you know, doing fine. They
have, you know, cushy jobs

396
00:19:47.390 --> 00:19:49.100
that they, you know, they're
allowed to be a jerk

397
00:19:49.100 --> 00:19:50.720
to their, you know, if
the people on their team,

398
00:19:50.720 --> 00:19:53.600
because, you know, they're the
programmer, you know, so there's

399
00:19:53.600 --> 00:19:56.210
a lot of resistance there,
but at the same time,

400
00:19:56.630 --> 00:19:58.490
the fact that we're even
talking about this, right. I

401
00:19:58.490 --> 00:20:00.290
mean, that's progress. And so
I think we see that

402
00:20:00.290 --> 00:20:02.330
on a global scale as
well, that, you know, we're

403
00:20:02.330 --> 00:20:04.760
talking about, you know, when
you look at, to like

404
00:20:04.760 --> 00:20:07.820
the resources, just everything that's,
you know, being depleted, like

405
00:20:07.880 --> 00:20:10.280
we have to change how
we live, because what we're

406
00:20:10.280 --> 00:20:13.010
doing is just not sustainable.
<inaudible> I think that we're

407
00:20:13.010 --> 00:20:15.080
all kind of hoping for
the star Trek, the next

408
00:20:15.080 --> 00:20:19.820
generation world that we're promised,
you know, food for all.

409
00:20:19.820 --> 00:20:21.440
And, and we would all
just kind of be, you

410
00:20:21.440 --> 00:20:23.780
know, everyone in star Trek,
no one has a, I

411
00:20:23.780 --> 00:20:25.670
don't really think about how
John Luke Picard gets a

412
00:20:25.670 --> 00:20:28.580
W2. Like, and he's not
really getting a paycheck. He

413
00:20:28.580 --> 00:20:31.160
just like there's food at
the wall. And he does

414
00:20:31.190 --> 00:20:32.990
art when he has spare
time. Really all they get

415
00:20:32.990 --> 00:20:36.200
to worry about is their
time and their, their intellectual

416
00:20:36.200 --> 00:20:39.110
pursuits. That would be the
ideal. Yeah, for sure. And

417
00:20:39.110 --> 00:20:40.370
I mean, I think we
can get there, you know,

418
00:20:40.370 --> 00:20:41.690
I mean, I think some
people will think, Oh, that's,

419
00:20:41.690 --> 00:20:44.180
you know, unrealistic, utopia, whatever,
but I don't think so.

420
00:20:44.180 --> 00:20:45.830
Cause I think, you know,
we can all make small

421
00:20:45.830 --> 00:20:48.850
changes every day to work
towards this, you know, brighter

422
00:20:48.850 --> 00:20:51.730
vision of the future. Just,
you know, we're more collaborative

423
00:20:51.730 --> 00:20:54.490
and we're cooperative and just
really taking care of each

424
00:20:54.490 --> 00:20:57.820
other more. And even if
you do it, even if,

425
00:20:57.820 --> 00:20:59.590
like I said, even if,
just when you're in traffic,

426
00:20:59.590 --> 00:21:01.690
if you think about it,
I mean, you're still helping

427
00:21:01.690 --> 00:21:04.480
raise the global kindness level.
You know? So I think

428
00:21:04.600 --> 00:21:08.130
whatever small steps we can
take are definitely worthwhile. I

429
00:21:08.130 --> 00:21:11.280
want to dig a little
bit more into the idea

430
00:21:11.280 --> 00:21:16.020
that you have is that
minimizing suffering, because compassionate is

431
00:21:16.020 --> 00:21:18.580
a, is a word that
has certain connotations. And in,

432
00:21:18.580 --> 00:21:21.090
in, in trying different words
out in my mouth and

433
00:21:21.090 --> 00:21:22.650
in my brain to try
to get an understanding of

434
00:21:22.650 --> 00:21:26.250
your perspective, the idea of
minimizing suffering a suffering is

435
00:21:26.250 --> 00:21:29.010
like a really bad word.
Like that's bad. We definitely

436
00:21:29.010 --> 00:21:32.610
don't want more of that.
You're saying compassion is the

437
00:21:32.610 --> 00:21:35.850
key to minimizing suffering for
yourself, for your users, for

438
00:21:35.850 --> 00:21:41.760
your team, for the Deb's
for everybody. Yeah, definitely. So

439
00:21:41.880 --> 00:21:45.090
there's, you know, there's different
definitions of compassion, but there's

440
00:21:45.090 --> 00:21:47.460
one that I read from
the greater good science center

441
00:21:47.460 --> 00:21:50.940
at UC Berkeley. And they
had this idea that compassion

442
00:21:50.940 --> 00:21:52.860
is kind of a more
active version of empathy. Cause

443
00:21:52.860 --> 00:21:54.510
with empathy, you know, you
can feel what the other

444
00:21:54.510 --> 00:21:57.120
person's feeling and there's different
types of empathy, but that's,

445
00:21:57.120 --> 00:21:59.940
you know, what it essentially
boils down to, whereas compassion,

446
00:22:00.060 --> 00:22:02.550
you not only can feel
how other people are feeling,

447
00:22:02.550 --> 00:22:07.260
but you actively desire to
make them feel better, right?

448
00:22:07.260 --> 00:22:09.750
So you want to reduce
their suffering. And so that's

449
00:22:09.750 --> 00:22:12.780
why I choose compassion. When
I talk about this, as

450
00:22:12.780 --> 00:22:14.910
often as I can, of
course, I also use empathy

451
00:22:14.910 --> 00:22:17.400
because, you know, that's, that's
become the buzz word that

452
00:22:17.400 --> 00:22:20.850
people understand a bit more,
but really I think the

453
00:22:20.850 --> 00:22:23.940
next level is really compassion
because it's just more active

454
00:22:23.940 --> 00:22:27.480
you and it's also potentially
more rational because you don't

455
00:22:27.480 --> 00:22:30.120
necessarily even have to be
very emotional at the time

456
00:22:30.120 --> 00:22:33.150
to, to make a compassionate
decision because you really can

457
00:22:33.150 --> 00:22:35.280
look at it logically and
think, okay, given all the

458
00:22:35.280 --> 00:22:40.080
parties involved, which step forward
will involve the least suffering.

459
00:22:40.110 --> 00:22:42.720
Right. Or, or the most
pleasure. But let's be honest.

460
00:22:42.720 --> 00:22:44.580
Like a lot of times
it's just about minimizing suffering.

461
00:22:45.510 --> 00:22:47.280
So yeah. That's why I
prefer that kind of way

462
00:22:47.280 --> 00:22:49.710
of looking at it. Yeah.
Yeah. Talk to me a

463
00:22:49.710 --> 00:22:52.350
little bit more about that
empathy versus compassion. So empathy

464
00:22:52.350 --> 00:22:54.690
is feeling what the other,
you know, the other people

465
00:22:54.690 --> 00:22:56.880
are thinking, but compassion is
kind of on the way

466
00:22:56.880 --> 00:23:00.870
true to altruism, but not
quite there. Yeah. Compassion is

467
00:23:00.870 --> 00:23:03.290
more, it's just more active.
So it's like you, you

468
00:23:03.300 --> 00:23:05.130
could, instead of you, you
can cause you could be

469
00:23:05.130 --> 00:23:07.650
empathetic. You could feel what
someone's feeling. It just not

470
00:23:07.650 --> 00:23:09.330
care. Right. So you can
feel what they're feeling and

471
00:23:09.330 --> 00:23:11.790
just move on with your
life. Compassion is more, you

472
00:23:11.790 --> 00:23:14.310
feel what they're feeling and
you're like, Oh, they're suffering.

473
00:23:14.310 --> 00:23:16.500
I want to make them
feel better. So yeah, it

474
00:23:16.500 --> 00:23:19.350
takes that active step. So
you actually commit to wanting

475
00:23:19.350 --> 00:23:22.500
to do better. And that's
what makes compassion different. And

476
00:23:22.500 --> 00:23:25.740
yet altruism is another they
and closely related of course.

477
00:23:26.250 --> 00:23:29.670
And you know, it can
be differentiated in different ways,

478
00:23:29.670 --> 00:23:32.190
but you know, there, I
think the difference is pretty

479
00:23:32.190 --> 00:23:35.820
subtle in terms of altruism,
but they're, they're all, they're

480
00:23:35.820 --> 00:23:38.010
all different, but related concepts
is how I look at

481
00:23:38.010 --> 00:23:40.290
it. It's a really interesting
spectrum. I mean, like we

482
00:23:40.290 --> 00:23:42.840
said, naming things is hard,
but once you get the,

483
00:23:43.200 --> 00:23:46.850
once you get the taxonomy
down, it's pretty powerful. There's

484
00:23:46.850 --> 00:23:49.430
a really cool infographic that
I'll put in the show

485
00:23:49.430 --> 00:23:53.090
notes by Robert Shelton, who's a
psychologist, it talks about pity.

486
00:23:53.630 --> 00:23:57.230
Hey, I acknowledge that you're
suffering sympathy. I care about

487
00:23:57.230 --> 00:24:00.440
your suffering empathy. Like you
said, I feel your suffering

488
00:24:00.440 --> 00:24:02.930
for myself and compassion. You
know, I want to believe

489
00:24:02.930 --> 00:24:05.190
it let's work together to
relieve your suffering. It's a

490
00:24:05.300 --> 00:24:09.430
spectrum as you, as you
head in one direction. Yeah,

491
00:24:09.460 --> 00:24:11.680
for sure. And, and again,
it's like, you know, many

492
00:24:11.680 --> 00:24:14.860
concepts where different different schools
of thought have, you know,

493
00:24:14.860 --> 00:24:17.050
different lines that they draw
and whatnot. So I think

494
00:24:17.260 --> 00:24:19.930
it's important just that whenever
people talk about it, so

495
00:24:19.930 --> 00:24:22.270
yeah, just get on the
same page, you know, it's,

496
00:24:22.330 --> 00:24:24.760
you know, there's this tendency
I think in tech, especially

497
00:24:24.970 --> 00:24:27.040
for there to be like
a right or wrong answer,

498
00:24:27.040 --> 00:24:28.660
like, you know, it's either
this way or that way.

499
00:24:28.960 --> 00:24:32.110
And I think in general,
even when it comes to

500
00:24:32.110 --> 00:24:34.600
like technical terms, it's more
just how you define it

501
00:24:34.600 --> 00:24:36.460
and how we agree that
we're going to define it

502
00:24:36.460 --> 00:24:38.950
and move forward in the
conversation versus like, this is

503
00:24:38.950 --> 00:24:41.140
the absolute right definition. You
know what I mean? Hmm.

504
00:24:41.140 --> 00:24:44.080
That's a really good point.
So ultimately it just matters

505
00:24:44.080 --> 00:24:46.810
if my, my team and
we all agree on, on

506
00:24:46.810 --> 00:24:48.490
how we're going to treat
each other and the right

507
00:24:48.490 --> 00:24:49.960
thing that we're going to
do and what compassion or

508
00:24:49.960 --> 00:24:52.240
empathy means to us. It
doesn't matter that the universe

509
00:24:52.270 --> 00:24:55.780
agree on the exact definitions.
Exactly. And is that the

510
00:24:55.780 --> 00:24:57.310
kind of thing that one
would learn in a work

511
00:24:57.340 --> 00:24:59.200
in a work group or
a workshop rather, or if

512
00:24:59.200 --> 00:25:01.300
we watched some of your
videos online on the topic?

513
00:25:02.200 --> 00:25:04.780
Yeah. That's, that's one of
the things they have. So

514
00:25:04.780 --> 00:25:09.010
really the workshops, they break
down emotional intelligence in terms

515
00:25:09.010 --> 00:25:14.170
of the self and then
social interactions. And then I

516
00:25:14.170 --> 00:25:16.360
always talk about continuous improvement.
So it was always have

517
00:25:16.360 --> 00:25:19.000
an action plan at the
end to kind of how

518
00:25:19.000 --> 00:25:22.150
you're going to keep improving.
And so self awareness and

519
00:25:22.150 --> 00:25:24.190
self management is really about
being aware of your own

520
00:25:24.190 --> 00:25:26.650
emotions and deciding, you know,
how to, how to manage

521
00:25:26.650 --> 00:25:29.650
them, how to understand what
your emotions really mean. And

522
00:25:29.650 --> 00:25:32.230
then the social interaction component
we talk about, you know,

523
00:25:32.230 --> 00:25:35.650
how to communicate with empathy,
how to imagine what other

524
00:25:35.650 --> 00:25:38.590
people are thinking and feeling
and, and asking them and

525
00:25:38.590 --> 00:25:43.090
discussing it directly, caring about
that and then effective communication

526
00:25:43.090 --> 00:25:45.850
styles, things like that. And
then continuous improvement is really

527
00:25:45.850 --> 00:25:48.040
where I take the idea
of the agile retrospective from

528
00:25:48.040 --> 00:25:50.650
agile software development. And I
apply it at a personal

529
00:25:50.650 --> 00:25:53.650
level and it's something that's
really helped me a lot

530
00:25:53.770 --> 00:25:56.380
to make improvement in my
own life using this personal

531
00:25:56.380 --> 00:25:59.140
retrospective. And so I always
share that in the workshops

532
00:25:59.140 --> 00:26:01.540
too, because you know, this
is an ongoing process. It's

533
00:26:01.540 --> 00:26:03.430
not just like you do
a workshop or you read

534
00:26:03.430 --> 00:26:05.830
a book and then, okay,
I'm emotionally intelligent. Now it's

535
00:26:05.830 --> 00:26:09.040
more like it's a continuous
process. Yeah. Somehow I was

536
00:26:09.040 --> 00:26:10.720
hoping I would go through
this process and I'd be

537
00:26:10.720 --> 00:26:12.160
able to put this on
my LinkedIn. I'd be like,

538
00:26:13.630 --> 00:26:17.020
yeah, exactly. April says you're
compassionate. So it's okay now.

539
00:26:17.500 --> 00:26:19.840
Yeah, exactly. It really is
a process. You never, you're

540
00:26:19.840 --> 00:26:21.490
never all the way there
and you're certainly gonna fall

541
00:26:21.490 --> 00:26:22.930
backwards. You're going to say
the wrong thing in a

542
00:26:22.930 --> 00:26:26.860
meeting, you're going to insult
someone accidentally or upset someone.

543
00:26:26.860 --> 00:26:28.900
And then you just say,
Hey, I, I hear you.

544
00:26:29.110 --> 00:26:32.080
I hear what you said.
Yeah. And that's where self-compassion

545
00:26:32.080 --> 00:26:34.960
comes in handy because, you
know, for sure, like the

546
00:26:34.960 --> 00:26:38.620
other day, like I tweeted
something sarcastic and I realized

547
00:26:38.620 --> 00:26:40.270
that that's not really how
I want to be in

548
00:26:40.270 --> 00:26:42.430
the world and it's not
consistent with my values, but

549
00:26:42.430 --> 00:26:44.130
instead of beating myself up
like, Oh my gosh, I'm

550
00:26:44.130 --> 00:26:47.370
a horrible person again. I
thought, well, you know, I

551
00:26:47.370 --> 00:26:49.860
was feeling this and that,
whatever it doesn't excuse what

552
00:26:49.860 --> 00:26:51.450
I did, but it's more
like, I understand why I

553
00:26:51.450 --> 00:26:53.370
did that. And so, you
know, I have compassion for

554
00:26:53.370 --> 00:26:55.080
myself and I'm going to
do better in the future.

555
00:26:55.080 --> 00:26:58.610
And that's, that's really all
it takes. There was another

556
00:26:58.610 --> 00:27:00.170
interesting thing I want to
kind of end on, on,

557
00:27:00.410 --> 00:27:03.980
on Twitter, which isn't really
a compassionate place, but I

558
00:27:03.980 --> 00:27:05.720
see you a lot there
and I'm there a lot.

559
00:27:05.750 --> 00:27:08.870
So somehow we are trying
to find the balance between

560
00:27:08.870 --> 00:27:11.420
being empathetic and compassionate people
and going to kind of

561
00:27:11.420 --> 00:27:15.470
the least compassionate place online.
But there was a discussion

562
00:27:15.470 --> 00:27:19.550
around an individuals hosted what
they thought was an innocuous

563
00:27:19.550 --> 00:27:22.220
tweet saying, Hey, you know,
if you switch jobs every

564
00:27:22.220 --> 00:27:25.130
year, we're probably not the
right fit for you. And

565
00:27:25.160 --> 00:27:29.540
they didn't think that that
was particularly controversial and Twitter

566
00:27:29.540 --> 00:27:33.710
disagreed indicating that not everybody
has the privilege to stick

567
00:27:33.710 --> 00:27:38.210
around at a crappy poisonous
job for, for a, for

568
00:27:38.210 --> 00:27:39.710
a year. And you were
kind of involved in that.

569
00:27:39.740 --> 00:27:42.050
And I noticed that at
the end of it, the

570
00:27:42.050 --> 00:27:44.870
individual wrote a whole post
about it. That was very

571
00:27:45.380 --> 00:27:48.770
empathetic. How, how did, were
you involved at all directly

572
00:27:48.770 --> 00:27:51.650
to get someone to go
through? Like the, I believe

573
00:27:51.650 --> 00:27:53.630
something know, I really, really
believe it. I'm going to

574
00:27:53.630 --> 00:27:56.660
double down unbelieving this to
the well you changed my

575
00:27:56.660 --> 00:28:00.620
perspective. Yeah, no, I, I
definitely can't take credit for

576
00:28:00.620 --> 00:28:03.560
that because I think that
that individual got a lot

577
00:28:03.560 --> 00:28:08.780
of tweets about that, about
his tweet. But I was

578
00:28:08.780 --> 00:28:12.020
really touched by the fact
that he took a step

579
00:28:12.020 --> 00:28:14.120
back cause people were, people
were kind of jumping on

580
00:28:14.120 --> 00:28:16.430
him too for not replying
right away. And I was

581
00:28:16.430 --> 00:28:18.740
thinking, and I actually did
tweet this. And I included

582
00:28:18.740 --> 00:28:20.180
him in that, which is
that I hope he's just

583
00:28:20.180 --> 00:28:22.460
taking a step back and
thinking about this because you

584
00:28:22.460 --> 00:28:24.620
know, I think that there's
this idea that, Oh, you

585
00:28:24.620 --> 00:28:26.600
have to respond immediately. But
the problem is when people

586
00:28:26.600 --> 00:28:29.630
respond immediately, it's usually coming
from that, you know, MIG

587
00:28:29.630 --> 00:28:32.090
Dilla and the brains coming
from that fight or flight

588
00:28:32.090 --> 00:28:35.240
defense that really defensive kind
of viewpoint. And so you're

589
00:28:35.240 --> 00:28:37.520
going to be, you're going
to defend yourself. You're gonna

590
00:28:37.530 --> 00:28:39.740
attack other people just because
you know, that part of

591
00:28:39.740 --> 00:28:43.340
that lizard brain that's still
with us is, you know,

592
00:28:43.340 --> 00:28:45.380
it's going to come out
and that's like, what most

593
00:28:45.380 --> 00:28:46.970
of Twitter is. Right. So
I actually thought it was

594
00:28:46.970 --> 00:28:48.440
great that he took a
step back. He got a

595
00:28:48.440 --> 00:28:50.870
lot of different perspectives to
gather a lot of perspectives.

596
00:28:51.230 --> 00:28:53.300
And then, yeah, like, like
you said, his, his blog

597
00:28:53.330 --> 00:28:56.600
post was really empathetic, really
thoughtful. And I think that

598
00:28:56.600 --> 00:28:58.700
that's the value of these
like, you know, longer form

599
00:28:58.700 --> 00:29:00.300
posts. I mean, you know,
I do tweet a lot,

600
00:29:00.420 --> 00:29:02.480
you know, I know you
tweet a lot too. And

601
00:29:02.480 --> 00:29:04.700
I try to be thoughtful
in my tweets, but you

602
00:29:04.700 --> 00:29:06.560
know, people can misconstrue them
and then it leads to

603
00:29:06.560 --> 00:29:10.550
misunderstandings and all that sort
of stuff. And blog posts

604
00:29:10.550 --> 00:29:12.320
are a little bit safer
in that regards. Cause you

605
00:29:12.320 --> 00:29:16.220
can explore things in more
depth, but yeah. I mean,

606
00:29:16.220 --> 00:29:18.560
I still, I don't know,
Twitter is an interesting place.

607
00:29:19.310 --> 00:29:22.880
Sometimes empathy happens there sometimes
not. Yeah. I think it's

608
00:29:23.090 --> 00:29:25.280
Twitter is actually a lot
like work. Sometimes I find

609
00:29:25.610 --> 00:29:28.010
joy and it feeds my
spirit and I love going

610
00:29:28.010 --> 00:29:30.020
to Twitter and other days
and just like, I really

611
00:29:30.020 --> 00:29:33.860
can't do this today. That
sounds about right. So are

612
00:29:33.860 --> 00:29:36.020
there particular video that you
would recommend that someone would

613
00:29:36.020 --> 00:29:37.280
explore if they were going
to go and take a

614
00:29:37.280 --> 00:29:39.710
look at your site and
like what's your, what's the

615
00:29:39.710 --> 00:29:44.830
best introduction to compassionate coding?
Yeah. So on compassionate coding.com.

616
00:29:44.860 --> 00:29:47.680
I have a site like
a media page and I

617
00:29:47.680 --> 00:29:50.440
think the one that gives
the best kind of overview

618
00:29:50.860 --> 00:29:53.830
is emotional intelligence for engineers.
The talk that I gave

619
00:29:53.830 --> 00:29:57.640
it in G Atlanta and
yeah. So I think that,

620
00:29:57.640 --> 00:29:59.820
that, one's probably the best
one to start with. Okay.

621
00:29:59.820 --> 00:30:01.920
Very cool. Yeah. We were
actually at the same at

622
00:30:01.920 --> 00:30:03.570
the same conference at the
same time I was at

623
00:30:03.570 --> 00:30:07.170
NJ Atlanta as well. Yes,
I believe that's true. Yeah.

624
00:30:07.380 --> 00:30:09.340
And that was a great
talk that you gave and

625
00:30:09.340 --> 00:30:11.400
then the video online is
really great. It's about 20

626
00:30:11.400 --> 00:30:13.230
minutes long, so it's not
going to be like, you

627
00:30:13.230 --> 00:30:15.290
know, 90 minutes of your
time and it's definitely worth

628
00:30:15.300 --> 00:30:18.750
taking the, taking the moment.
It's interesting. Of course that

629
00:30:18.750 --> 00:30:22.020
it's on YouTube. Hopefully the
comments will become the compliment.

630
00:30:24.120 --> 00:30:27.060
Yeah. There were some interesting
comments there, but I think

631
00:30:27.060 --> 00:30:28.350
they, I don't know if
they have, I don't, I

632
00:30:28.350 --> 00:30:30.690
don't have control over the
comments there, but I think

633
00:30:30.690 --> 00:30:32.970
the, the organizers there have
been pretty good about at

634
00:30:32.970 --> 00:30:35.790
least hiding the, the word
I've always felt like the

635
00:30:35.790 --> 00:30:37.920
thing that's wrong, that YouTube
is the down arrow, like

636
00:30:37.920 --> 00:30:39.780
who needs to down arrow
or something. And he just

637
00:30:39.790 --> 00:30:42.570
needs to have an up
arrow or no arrow. Like,

638
00:30:42.600 --> 00:30:44.580
you know, like I don't
want to thumbs down anything.

639
00:30:44.580 --> 00:30:46.890
I would just be nice
or say nothing, you know?

640
00:30:47.160 --> 00:30:49.440
Yeah. Thankfully it hasn't, it's
what's funny is it hasn't

641
00:30:49.440 --> 00:30:51.960
gotten too many downers, but
somebody posted on fortune of

642
00:30:51.960 --> 00:30:56.100
all places and that attracted
some interesting comments. So if

643
00:30:56.100 --> 00:30:58.140
you look at the hidden
comments you'll eat, I don't

644
00:30:58.140 --> 00:31:00.270
recommend looking at hidden comments.
How about that to everybody?

645
00:31:00.630 --> 00:31:02.010
I'm going to go ahead
and not do that because

646
00:31:02.010 --> 00:31:05.070
I decided to become a
compassionate coder. Yeah. There you

647
00:31:05.070 --> 00:31:07.980
go. Perfect. Thanks so much
for chatting with me today.

648
00:31:08.610 --> 00:31:10.980
Yeah. Thanks for having me.
This is super fun. This

649
00:31:10.980 --> 00:31:13.440
has been another episode of
Hanselminutes and we'll see you

650
00:31:13.440 --> 00:31:14.280
again next week.

