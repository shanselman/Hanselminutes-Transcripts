WEBVTT FILE

1
00:00:00.100 --> 00:00:03.500
Today's episode is sponsored by
data dog a cloud scale

2
00:00:03.500 --> 00:00:08.000
monitoring platform that unifies metrics
traces logs and more in

3
00:00:08.000 --> 00:00:12.400
a single pane of glass
automatically alert on performance anomalies

4
00:00:12.600 --> 00:00:16.800
easily build custom dashboards and
collaborate to resolve issues and

5
00:00:16.800 --> 00:00:21.000
real-time Tri data dog for
yourself with a free 14-day.

6
00:00:21.000 --> 00:00:24.900
Trial Hansel minutes listeners will
also receive a free t-shirt.

7
00:00:25.300 --> 00:00:33.600
Visit bitly.com /data dog shirt.
That's SBI tly.com data dog

8
00:00:33.600 --> 00:00:50.000
shirt to get started. Hi,
this is Scott Hanselman. This

9
00:00:50.000 --> 00:00:52.300
is another episode of cancel
minutes today. I'm talking with

10
00:00:52.300 --> 00:00:55.600
Sarah Beck. She's a machine
learning solution principal at Sloan

11
00:00:55.600 --> 00:00:58.800
build. How are you? I'm
doing well Scott. How are

12
00:00:58.800 --> 00:01:03.100
you? Very well. I am
out here teaching myself as

13
00:01:03.100 --> 00:01:05.600
much as I can about
data science, but it's it's

14
00:01:05.600 --> 00:01:10.400
a little confusing. There's machine
learning. There's deep learning. There's

15
00:01:10.400 --> 00:01:13.400
the data. There's the algorithm
maybe you can start by

16
00:01:13.400 --> 00:01:16.900
helping me understand the difference
between machine learning and deep

17
00:01:16.900 --> 00:01:19.300
learning because sometimes I feel
like the journalists and the

18
00:01:19.300 --> 00:01:23.200
tech writers just squish it
all together. I would love

19
00:01:23.200 --> 00:01:30.000
to machine learning is a
general term for algorithms. In

20
00:01:30.000 --> 00:01:36.100
data science that are optimizing
something under a constraint. I

21
00:01:36.100 --> 00:01:40.300
know that those sounds like
some fairly technical mathy terms

22
00:01:40.300 --> 00:01:43.200
and I'd be happy to
break that down more in

23
00:01:43.200 --> 00:01:48.600
a minute deep learning refers
to a specific area of

24
00:01:48.600 --> 00:01:52.500
machine learning that tends to
deal with what we refer

25
00:01:52.500 --> 00:01:58.400
to as neural networks and
neural network type models are

26
00:01:58.400 --> 00:02:03.600
very important. or computer vision
and I have made great

27
00:02:03.600 --> 00:02:14.700
strides and image processing and
Auto Audio detection. Mmm General

28
00:02:14.700 --> 00:02:18.800
machine learning algorithms are they
can work on Money More

29
00:02:18.800 --> 00:02:23.500
Problems things that like for
example, if you wanted to

30
00:02:23.500 --> 00:02:27.800
identify all the cats and
a picture you might apply

31
00:02:27.800 --> 00:02:31.800
deep learning algorithm to find
all the cats. Where's the

32
00:02:31.800 --> 00:02:35.700
machine learning algorithm others might
be able to tell you

33
00:02:36.200 --> 00:02:43.700
oh this Area of the
picture has fur and we

34
00:02:43.700 --> 00:02:45.800
think that the color of
what we think is for

35
00:02:45.800 --> 00:02:51.400
looks like a cat. So
anyway, there's this little distinction

36
00:02:51.400 --> 00:02:57.000
between deep learning and neural
network models and other algorithms

37
00:02:57.400 --> 00:03:03.900
where you can use the
information to draw other conclusions

38
00:03:03.900 --> 00:03:06.400
or find out more about
the problem. You're trying to

39
00:03:06.400 --> 00:03:10.200
model. Does that make sense?
A little bit but I

40
00:03:10.200 --> 00:03:11.900
want to get more I
want to get even clearer.

41
00:03:11.900 --> 00:03:14.000
If you don't mind. It
feels to me like deep

42
00:03:14.000 --> 00:03:18.200
learning architectures apply very often
or more up more often

43
00:03:18.200 --> 00:03:22.400
apply to Fields. Like Vision
speech natural language things that

44
00:03:22.400 --> 00:03:26.900
the brain does is that
a fair statement. I would

45
00:03:26.900 --> 00:03:31.400
say that it's very fair
and and recent years deep

46
00:03:31.400 --> 00:03:36.300
learning has led to huge
advances and those areas the

47
00:03:36.300 --> 00:03:42.000
reason deep learning techniques aren't
Is presently used all over

48
00:03:42.000 --> 00:03:47.300
the place has to do
with the computational power required

49
00:03:47.500 --> 00:03:52.300
to train models on all
of that data and that

50
00:03:52.300 --> 00:03:56.500
deep learning models are also
considered more of a black

51
00:03:56.500 --> 00:04:00.600
box model. There are a
lot of data science problems

52
00:04:00.600 --> 00:04:03.800
where it's very important to
be able to give an

53
00:04:03.800 --> 00:04:08.900
answer why a model is
suggesting a certain output with

54
00:04:08.900 --> 00:04:13.500
things. Things like image processing
it's less important to know

55
00:04:14.400 --> 00:04:19.100
why an algorithm in the
example before with cats decided

56
00:04:19.100 --> 00:04:24.200
there were many cats in
the picture versus another algorithm.

57
00:04:24.200 --> 00:04:28.700
We're knowing wise important if
it's a black box if

58
00:04:28.700 --> 00:04:32.000
we're effectively let's put this
in a simplistic terms because

59
00:04:32.000 --> 00:04:33.300
I think a lot of
the folks that are listening

60
00:04:33.300 --> 00:04:36.700
have written programs before and
they've written programs with lots

61
00:04:36.700 --> 00:04:39.900
of functions, and if I
were to then A function

62
00:04:39.900 --> 00:04:43.300
that is detect cat and
I write it programmatically and

63
00:04:43.300 --> 00:04:45.700
I figure out some way
in code with for loops

64
00:04:45.700 --> 00:04:48.600
and if statements to write
a function that's detected cats

65
00:04:48.800 --> 00:04:51.700
and then I turn that
over to a deep Learning

66
00:04:51.700 --> 00:04:55.300
System something that's doing deep
structured learning or hierarchical learning

67
00:04:55.600 --> 00:04:59.800
and I train a model
and now detect cat is

68
00:05:00.100 --> 00:05:03.800
Magic. It's literally like that
Far Side cartoon. We saw

69
00:05:03.800 --> 00:05:06.400
many years ago where someone
is describing the complexity of

70
00:05:06.400 --> 00:05:08.400
a system and then at
some point they get into

71
00:05:08.400 --> 00:05:11.500
the algorithm ago. Go and
then a miracle happens and

72
00:05:11.500 --> 00:05:13.500
then you continue on with
the rest of the algorithm.

73
00:05:13.800 --> 00:05:17.100
We're really inserting an unknown
into our thing and just

74
00:05:17.100 --> 00:05:21.600
trusting it in the sense.
We are it would say

75
00:05:21.600 --> 00:05:25.500
that neural network models are
great at doing what they

76
00:05:25.500 --> 00:05:29.900
do because they are designed
exactly the way our brains

77
00:05:29.900 --> 00:05:32.600
work in terms of when
we look at a picture

78
00:05:32.900 --> 00:05:36.800
and can identify a cat
are the nerves and our

79
00:05:36.800 --> 00:05:40.100
eyes get these light signals
from the and then it

80
00:05:40.100 --> 00:05:45.700
is sent to many layers
of our neurons, but For

81
00:05:45.700 --> 00:05:49.400
the most part I may
be an optometrist or a

82
00:05:49.400 --> 00:05:54.100
neurologist this process. I don't
know. Why when I'm looking

83
00:05:54.100 --> 00:05:56.300
at this picture, I think
it would look like a

84
00:05:56.300 --> 00:06:01.600
cat just it's some yes,
there's a lot of unknown

85
00:06:02.400 --> 00:06:05.100
but there are still ways
to test and validate that

86
00:06:05.100 --> 00:06:10.100
these models are being very
reliable. Mmm. We've had folks

87
00:06:10.100 --> 00:06:12.400
on the show talk about
like bias and machine learning

88
00:06:12.400 --> 00:06:15.100
and deep learning and how
if your data in is

89
00:06:15.100 --> 00:06:17.600
bad Then your function out
might be bad and it

90
00:06:17.600 --> 00:06:20.400
could identify a person as
a cat or a bush

91
00:06:20.400 --> 00:06:23.800
or anything like that is
the goal to be perfect

92
00:06:23.800 --> 00:06:25.500
or is the goal to
be as good as humans.

93
00:06:25.500 --> 00:06:29.000
What is the goal when
designing something like this? Well,

94
00:06:29.000 --> 00:06:32.000
I would say that that
really depends on the specific

95
00:06:32.000 --> 00:06:37.300
problem. Someone is trying to
solve in terms of maybe

96
00:06:37.300 --> 00:06:41.300
a Biometrics case for someone
is using deep learning to

97
00:06:41.300 --> 00:06:47.200
identify a fingerprint or an
iris for 30 reasons in

98
00:06:47.200 --> 00:06:50.400
those circumstances you really do
need to be perfect due

99
00:06:50.400 --> 00:06:53.600
to what might be at
stake in terms of security

100
00:06:54.200 --> 00:06:58.000
if the goal is just
to tell if there's a

101
00:06:58.000 --> 00:07:01.400
living creature or person so
that may be a self-driving

102
00:07:01.400 --> 00:07:06.900
car doesn't hit it or
something else. Then it really

103
00:07:06.900 --> 00:07:12.600
doesn't matter whether we're 100%
perfect and knowing this pixel

104
00:07:12.600 --> 00:07:17.900
is A dog or a
cat or person right if

105
00:07:17.900 --> 00:07:21.700
I use something like Google
photos, which is consistently asking

106
00:07:21.700 --> 00:07:24.800
me questions about new questions
saying, oh is this a

107
00:07:24.800 --> 00:07:26.400
picture of your wife is
that a picture of your

108
00:07:26.400 --> 00:07:29.000
wife and every couple of
times? I'll open it. It'll

109
00:07:29.000 --> 00:07:31.900
ask me things and I
presume it is then training.

110
00:07:31.900 --> 00:07:34.700
I'm teaching it more about
my family's would figures out

111
00:07:34.900 --> 00:07:37.500
who's who if it gets
someone wrong. It's you know

112
00:07:37.500 --> 00:07:40.400
it I guess the worst-case
scenario that could offend me,

113
00:07:40.700 --> 00:07:44.500
but it's not going to
be like life-threatening. But false

114
00:07:44.500 --> 00:07:47.000
positives if it was looking
at a an image and

115
00:07:47.000 --> 00:07:50.400
saying that someone has cancer
that could be hugely problematic.

116
00:07:51.000 --> 00:07:56.200
So it could and those
problems also exist even with

117
00:07:56.200 --> 00:08:02.900
like blood testing or been
looking at other biological tests

118
00:08:02.900 --> 00:08:08.900
that a doctor might suggest
for diagnosis good points. So

119
00:08:08.900 --> 00:08:11.800
that problem isn't very unique
to deep learning and I

120
00:08:11.800 --> 00:08:15.500
know that in all This
feels there's always the best

121
00:08:15.500 --> 00:08:23.700
effort to reduce false positives
without missing diagnosis because neither

122
00:08:23.700 --> 00:08:29.000
case is very good be
very stressful to find out

123
00:08:29.000 --> 00:08:31.800
you a doctor thought you
had cancer when you don't

124
00:08:31.800 --> 00:08:34.700
and not to mention expensive.
Hmm But you bring up

125
00:08:34.700 --> 00:08:36.600
a really good point actually
gets to the hype of

126
00:08:36.600 --> 00:08:40.000
machine learning and deep learning
is that we are we

127
00:08:40.000 --> 00:08:44.600
I think we're being sold.
We the A person the

128
00:08:44.600 --> 00:08:46.800
community is being sold an
idea that these are going

129
00:08:46.800 --> 00:08:50.100
to solve the really unsolvable
problems. But that doesn't mean

130
00:08:50.100 --> 00:08:52.300
that they're going to be
perfect. They really aren't like

131
00:08:52.300 --> 00:08:55.800
blood tests aren't perfect and
deep learning models are not

132
00:08:55.800 --> 00:09:00.500
perfect. That's exactly correct. And
I would say that depending

133
00:09:00.500 --> 00:09:03.400
on the nature of the
problem. Someone's trying to solve

134
00:09:03.500 --> 00:09:07.800
deep learning algorithms may not
really be the best choice

135
00:09:07.800 --> 00:09:11.100
just because with these neural
network models you do lose

136
00:09:11.100 --> 00:09:16.000
that visibility into what Slating
to the result or an

137
00:09:16.000 --> 00:09:19.500
inference or learning you could
glean from a different type

138
00:09:19.500 --> 00:09:24.300
of algorithm. Like for example,
if a bank had and

139
00:09:24.600 --> 00:09:30.000
machine learning model to assess
whether someone was credit worthy.

140
00:09:30.500 --> 00:09:32.500
It would be good to
be able to tell that

141
00:09:32.500 --> 00:09:36.100
like a loan applicant why
your application for credit was

142
00:09:36.100 --> 00:09:39.400
denied so that they might
be able to do something

143
00:09:39.400 --> 00:09:42.700
about it versus like a
deep learning model which is

144
00:09:42.700 --> 00:09:45.300
say yes or no. Oh
whether someone was credit worthy

145
00:09:45.300 --> 00:09:47.600
and it would be a
lot harder to prove there

146
00:09:47.600 --> 00:09:50.900
is no bias. Hmm. That's
a good point. Certainly if

147
00:09:50.900 --> 00:09:52.500
we had a deep learning
model that was telling me

148
00:09:52.500 --> 00:09:54.100
that I was guilty or
not or why I was

149
00:09:54.100 --> 00:09:56.800
going to jail or why
I was arrested. There'd be

150
00:09:56.800 --> 00:10:02.100
huge opportunities to either directly
or indirectly introduce bias and

151
00:10:02.100 --> 00:10:04.800
have no way to confirm.
It's just simply a thumbs

152
00:10:04.800 --> 00:10:08.200
up or thumbs down which
is a fairly. Well, I

153
00:10:08.200 --> 00:10:10.200
would be a plot for
a pretty scary science fiction

154
00:10:10.200 --> 00:10:14.000
movie. Definitely. Hey don't know
that I'd want to Watch

155
00:10:14.000 --> 00:10:18.100
that one. So when we
think about these models and

156
00:10:18.100 --> 00:10:19.800
we say that we describe
them of a black you

157
00:10:19.800 --> 00:10:22.300
describe them as a black
box. We don't always know

158
00:10:22.300 --> 00:10:26.600
why but if the use
case is appropriate then it's

159
00:10:26.600 --> 00:10:29.200
ok that it is a
black box you're saying yes,

160
00:10:29.200 --> 00:10:33.200
and I would say that
even outside of deep learning.

161
00:10:33.200 --> 00:10:37.100
It's really important to make
sure that whatever algorithm being

162
00:10:37.100 --> 00:10:41.100
selected really matches the use
case for like what that

163
00:10:41.100 --> 00:10:44.200
problem is trying to solve
and also, So the data

164
00:10:44.200 --> 00:10:47.600
available to try to solve
that problem is kind of

165
00:10:47.600 --> 00:10:51.900
this triangle of how those
three components work together the

166
00:10:51.900 --> 00:10:56.300
algorithm the data and the
nature of the problem. So

167
00:10:56.800 --> 00:11:01.200
that requires really significant thought
in the in the all

168
00:11:01.200 --> 00:11:04.600
three parts of that you
could see where someone might

169
00:11:04.600 --> 00:11:09.300
potentially take a shortcut maybe
their data isn't complete or

170
00:11:09.300 --> 00:11:13.200
thoughtful or enough or maybe
they just picked the default

171
00:11:13.200 --> 00:11:17.500
or Inappropriate algorithm any one
of those legs is wrong

172
00:11:17.500 --> 00:11:21.200
in the stool Falls over
correct? And that's actually a

173
00:11:21.200 --> 00:11:24.500
really big part that will
lead to what I'll call

174
00:11:24.500 --> 00:11:29.100
a statistical bias that is
a little different than what

175
00:11:29.200 --> 00:11:33.700
we might describe as bias
in terms of discrimination or

176
00:11:34.000 --> 00:11:39.300
unlawful bias often people. Don't
do their Dill due diligence

177
00:11:39.500 --> 00:11:43.400
exploring the data and understanding
how the data was collected.

178
00:11:43.500 --> 00:11:47.200
And these little nuances that
if you knew the answer

179
00:11:47.200 --> 00:11:50.100
to all those questions, maybe
you would choose a different

180
00:11:50.100 --> 00:11:53.600
algorithm or approach the problem
in a different way. This

181
00:11:53.600 --> 00:11:57.700
has really started in the
last you say the last

182
00:11:57.700 --> 00:12:00.700
10 15 years. This is
not deep learning isn't something

183
00:12:00.700 --> 00:12:03.000
going back into the 50s
as it mean like maybe

184
00:12:03.000 --> 00:12:05.700
the neural networks in the
understanding of how we might

185
00:12:05.700 --> 00:12:09.000
do these things have been
been around for a long

186
00:12:09.000 --> 00:12:11.900
time. But this is the
last 20-30 years wouldn't you

187
00:12:11.900 --> 00:12:14.500
say has been the big
ride? Of deep learning. Well,

188
00:12:14.500 --> 00:12:18.100
I would say that the
rise of deep learning is

189
00:12:18.100 --> 00:12:22.700
definitely more in the last
decade 20 years and coinciding

190
00:12:22.700 --> 00:12:27.300
with the increase of computational
power and the ability to

191
00:12:27.300 --> 00:12:31.600
collect more and more data
people were familiar with the

192
00:12:32.000 --> 00:12:36.200
I guess methodology behind deep
learning in terms of neural

193
00:12:36.200 --> 00:12:40.800
networks actually believe way earlier
than the 50s. It's just

194
00:12:41.000 --> 00:12:44.000
there wasn't a good way
to try to to build

195
00:12:44.000 --> 00:12:48.100
those types of models at
scale. Hmm, right when I

196
00:12:48.100 --> 00:12:50.700
was reading up on this.
I understand that people were

197
00:12:50.700 --> 00:12:53.700
thinking about kind of learning
algorithms in the 60s and

198
00:12:53.700 --> 00:12:57.100
70s and then the term
deep learning was introduced by

199
00:12:57.200 --> 00:13:01.000
a woman named Marina detector
in 1986, but in the

200
00:13:01.000 --> 00:13:05.700
early 2010's when when the
realization that a GPU would

201
00:13:05.700 --> 00:13:08.100
be a useful thing for
not just Graphics, but for

202
00:13:08.100 --> 00:13:11.200
expressing and solving these problems
that's where things really kind

203
00:13:11.200 --> 00:13:13.900
of kicked off because we
all have GPS Use and

204
00:13:13.900 --> 00:13:16.100
they've just been shooting bad
guys and suddenly I can

205
00:13:16.100 --> 00:13:19.300
use them for something else.
That's exactly right major on

206
00:13:19.300 --> 00:13:23.700
locks and image processing as
a result to this computational

207
00:13:23.700 --> 00:13:26.800
power at this the type
of video games kind of

208
00:13:26.800 --> 00:13:30.200
reminded me of that. Yeah
the idea that I have

209
00:13:30.200 --> 00:13:33.600
a deep learning capable GPU
that I've been just waiting

210
00:13:33.600 --> 00:13:37.500
wasting on on shooting bad
guys is not lost on

211
00:13:37.500 --> 00:13:40.300
me. There's something about gpus
that caused them to be

212
00:13:40.300 --> 00:13:44.100
just so much more appropriate
for four deep. Learning systems

213
00:13:44.100 --> 00:13:49.800
than general-purpose CPUs. Hey friends
backlog is an all-in-one project

214
00:13:49.800 --> 00:13:52.700
and code management tool development
teams have been waiting for

215
00:13:53.200 --> 00:13:57.000
with project management bug tracking
Wiki's and get rolled into

216
00:13:57.000 --> 00:14:01.000
one easy to use platform
backlog provides. The powerful features

217
00:14:01.000 --> 00:14:04.500
at development teams need under
a clean UI that anyone

218
00:14:04.500 --> 00:14:08.100
can use you can easily
on board your whole team

219
00:14:08.400 --> 00:14:12.300
and start working on tasks
in minutes additional features like

220
00:14:12.300 --> 00:14:15.700
Gant and burdened. Charts make
it easy to manage projects

221
00:14:16.400 --> 00:14:19.700
mobile apps keep your team
connected and simple pricing scales

222
00:14:19.700 --> 00:14:22.000
with you. So you can
stop worrying about per user

223
00:14:22.000 --> 00:14:27.100
charges build your next project
with backlog get a 30-day

224
00:14:27.200 --> 00:14:34.000
obligation free trial at backlog.com
Hansel minutes. That's ba CK

225
00:14:34.000 --> 00:14:39.300
L OG.com Hansel minutes. So
I've been looking into building

226
00:14:39.300 --> 00:14:42.700
these myself and as I
learn about this, I don't

227
00:14:42.700 --> 00:14:47.600
understand. What regression models are
as I'm trying to pick

228
00:14:47.600 --> 00:14:49.900
the right data and feed
this into a model? What

229
00:14:49.900 --> 00:14:54.000
does a regression model well
regression models are not what

230
00:14:54.000 --> 00:14:57.700
I would describe as a
deep learning algorithm. Did you

231
00:14:57.700 --> 00:15:01.600
ever take a physics class
in high school or sure

232
00:15:01.600 --> 00:15:05.100
you might like try to
design or use your data

233
00:15:05.100 --> 00:15:08.200
to find the best fit
line. Yep. Absolutely. Yeah, you

234
00:15:08.200 --> 00:15:10.900
have a bunch of like
dots on a graph and

235
00:15:10.900 --> 00:15:13.500
you kind of like pick
a line that goes through.

236
00:15:13.500 --> 00:15:16.600
Through them all for the
most part regression models follow

237
00:15:16.600 --> 00:15:21.300
that class of format where
you're trying to find the

238
00:15:21.600 --> 00:15:24.500
best line or it could
be a shape instead of

239
00:15:24.500 --> 00:15:28.100
a straight line that describes
the data and the case

240
00:15:28.100 --> 00:15:31.300
for you like have a
line where you might have

241
00:15:31.300 --> 00:15:34.500
why being and I may
be at the money in

242
00:15:34.500 --> 00:15:39.600
your bank account and X
ping time like 38 coefficients

243
00:15:39.900 --> 00:15:43.300
on the variables like time.
There are different ways to

244
00:15:43.400 --> 00:15:47.000
to build these models so
that you can like in

245
00:15:47.000 --> 00:15:50.900
this example of money over
time predict something that is

246
00:15:50.900 --> 00:15:55.700
linear and like a continuous
number. There are other ways

247
00:15:55.700 --> 00:16:01.000
to apply these same types
of regression models to questions

248
00:16:01.000 --> 00:16:06.600
that have a yes no
outcome or others where the

249
00:16:06.600 --> 00:16:10.700
model is actually very nonlinear
like something like perhaps the

250
00:16:10.700 --> 00:16:16.100
cost model or different. Models
regression models are a great

251
00:16:16.100 --> 00:16:21.000
way to get some inference
about like what's causing the

252
00:16:21.000 --> 00:16:27.000
result result of your problem,
assuming that all the assumptions.

253
00:16:27.200 --> 00:16:32.100
The regression models make make
sense for that situation. Okay.

254
00:16:32.100 --> 00:16:34.600
So let me see if
I can understand that and

255
00:16:34.600 --> 00:16:38.000
phrase it differently because I'm
I'm still trying to learn

256
00:16:38.000 --> 00:16:41.000
about this. So if I
want to understand the relationship

257
00:16:41.000 --> 00:16:45.500
between some variables and In
the simplest case. It's a

258
00:16:45.500 --> 00:16:47.800
bunch of dots heading in
a general direction and I'm

259
00:16:47.800 --> 00:16:49.500
drawing a line through it
like a vector and I

260
00:16:49.500 --> 00:16:51.500
could then if it's a
nice straight line I can

261
00:16:51.500 --> 00:16:56.200
predict what's coming next and
that can make answering questions

262
00:16:56.200 --> 00:16:59.900
about that data pretty easy.
But if it's something like

263
00:17:00.500 --> 00:17:04.400
the stock market, it looks
like it's random and I

264
00:17:04.400 --> 00:17:06.700
guess one could argue that
it's not random and there's

265
00:17:06.700 --> 00:17:09.300
just an infinite number of
variables that feed into it.

266
00:17:10.300 --> 00:17:13.100
I'm hearing you say that
if I feed the wrong

267
00:17:13.100 --> 00:17:17.600
variable. Then I might try
to draw correlation where there

268
00:17:17.600 --> 00:17:20.900
just simply isn't any that's
definitely part of it. If

269
00:17:20.900 --> 00:17:27.000
you have attributes that aren't
impactful for what you're trying

270
00:17:27.000 --> 00:17:31.800
to predict you have a
risk of making bad predictions

271
00:17:31.900 --> 00:17:36.400
leveraging those variables. Another part
I wanted to touch on

272
00:17:36.400 --> 00:17:41.500
was it's not just but
variables you use it's also

273
00:17:42.000 --> 00:17:46.600
how those variables are. Transformed
before they are even used

274
00:17:46.600 --> 00:17:52.400
in the regression. For example,
maybe one of your variables

275
00:17:52.400 --> 00:17:56.800
for a regression that has
to do with whether or

276
00:17:56.800 --> 00:18:00.500
not someone will be accepted
into a university. Maybe one

277
00:18:00.500 --> 00:18:04.200
of those variables is a
grade someone had in the

278
00:18:04.200 --> 00:18:09.000
class if it's really hard
class and only two people

279
00:18:09.400 --> 00:18:14.300
have is that's going to
lead to It Model results

280
00:18:14.300 --> 00:18:18.300
then if maybe you use
the score of what someone

281
00:18:18.300 --> 00:18:23.400
had in the class like
98 compared to 73 or

282
00:18:23.400 --> 00:18:27.400
C or a right? So
you're saying that there's context

283
00:18:27.400 --> 00:18:29.700
in any story and you're
basically trying to data that

284
00:18:29.700 --> 00:18:32.500
you feed into this model
is a story that you're

285
00:18:32.500 --> 00:18:34.700
trying to tell and as
you go and you say

286
00:18:34.700 --> 00:18:36.500
hey, I got a 98
in this class. I'm amazing

287
00:18:36.500 --> 00:18:39.700
that that's a story but
it doesn't necessarily tell the

288
00:18:39.700 --> 00:18:42.900
full story if it turns
out that the next person

289
00:18:42.900 --> 00:18:45.200
in the Glass got a
16 or one else flunked

290
00:18:45.400 --> 00:18:48.400
and you know you cheated
suddenly the Troy has a

291
00:18:48.400 --> 00:18:53.500
lot more context. Yes and
models will handle things like

292
00:18:53.500 --> 00:18:57.100
we just described differently depending
on the nature of the

293
00:18:57.100 --> 00:19:02.000
algorithm how many people or
how prevalent the different situations

294
00:19:02.000 --> 00:19:05.600
are that are being modeled
a lot of what I

295
00:19:05.700 --> 00:19:11.000
consider data science myself is
building an empathy for the

296
00:19:11.000 --> 00:19:15.400
situation. You're trying to model
and The data so that

297
00:19:15.900 --> 00:19:19.700
you can really apply the
best algorithm. Hmm. Another thing

298
00:19:19.700 --> 00:19:23.600
that's interesting is that these
are situations that change right

299
00:19:23.600 --> 00:19:27.100
like using the classroom example
classes and things that we

300
00:19:27.100 --> 00:19:29.700
take in college are different
now and they'll be different

301
00:19:29.700 --> 00:19:31.300
in 10 years and they're
different than they were 20

302
00:19:31.300 --> 00:19:34.100
years ago. So you could
describe a model that's amazing.

303
00:19:34.100 --> 00:19:35.700
But in six months it
turns out that that's not

304
00:19:35.700 --> 00:19:38.900
a good model. Are you
going to be constantly revising

305
00:19:38.900 --> 00:19:42.700
models? The constant revision of
models does happen quite a

306
00:19:42.700 --> 00:19:46.200
lot in practice. Practice, I
would say that there are

307
00:19:46.200 --> 00:19:51.700
maybe two major camps to
model updating one is perhaps

308
00:19:51.700 --> 00:19:54.800
you're building a beautiful model
that works. Well for right

309
00:19:54.800 --> 00:19:58.600
now and you implement that
same model over and over

310
00:19:58.600 --> 00:20:03.400
again until time passes and
your model has become stale

311
00:20:03.400 --> 00:20:06.800
because you're noticing maybe higher
error than you were before

312
00:20:07.000 --> 00:20:12.200
another campus machine learning models
would be called online learning.

313
00:20:12.500 --> 00:20:16.100
Do you use any Internet
of Things devices like a

314
00:20:16.100 --> 00:20:20.400
Smartwatch, for example. Oh my
goodness. I you asked the

315
00:20:20.400 --> 00:20:24.000
wrong person because I've been
about 40 different internet of

316
00:20:24.000 --> 00:20:26.400
things I my house is
the internet of crap. I

317
00:20:26.400 --> 00:20:29.700
have all the devices. So
yes, the the quit the

318
00:20:29.700 --> 00:20:34.900
answer that question is absolutely
myself. I'm an avid triathlete

319
00:20:34.900 --> 00:20:41.900
and I wear a smartwatch
my train. Often the estimation

320
00:20:41.900 --> 00:20:44.600
for how fast I'm going
when I'm running the type

321
00:20:44.600 --> 00:20:48.100
of model the to use
to make those estimates would

322
00:20:48.100 --> 00:20:52.200
be in the class called
online learning where it's learning

323
00:20:52.200 --> 00:20:55.600
from the information you have
right now based off of

324
00:20:55.600 --> 00:21:01.700
a few different measurements. So
that's being updated continuously as

325
00:21:01.700 --> 00:21:04.800
time goes on like a
smartwatch doesn't need a new

326
00:21:04.800 --> 00:21:08.300
model. It just needs new
information to make sure I'm

327
00:21:08.300 --> 00:21:12.000
getting it. Great up-to-date running
speed right? So then using

328
00:21:12.000 --> 00:21:14.300
the example I had before
of the Google photos. The

329
00:21:14.300 --> 00:21:18.100
model is solid but they
just need more context about

330
00:21:18.100 --> 00:21:20.100
new photographs that are being
taken and they go I

331
00:21:20.100 --> 00:21:23.100
think these are the same
person give me a little

332
00:21:23.100 --> 00:21:26.100
bit more information and then
the model improves. Yes. I

333
00:21:26.100 --> 00:21:29.700
don't want to speak for
Google because I don't work

334
00:21:29.700 --> 00:21:32.600
for them. Of course. I'm
weary. I'm thinking in the

335
00:21:32.600 --> 00:21:35.400
general sense in the case
of deep learning models that

336
00:21:35.400 --> 00:21:40.200
are computer vision based my
bestest. Fission is that they

337
00:21:40.200 --> 00:21:44.700
have maybe a kind of
a scheduled model retraining hmm.

338
00:21:44.800 --> 00:21:48.300
We're like, oh now that
they have enough information. It's

339
00:21:48.300 --> 00:21:52.700
a few months later. Let's
return and deploy whereas deep

340
00:21:52.700 --> 00:21:57.300
learning image processing models can
be very difficult to train

341
00:21:57.300 --> 00:22:00.700
retrain very quickly. So you're
more likely to see something

342
00:22:00.700 --> 00:22:05.100
scheduled or have periodic updates
on more of a rigid

343
00:22:05.100 --> 00:22:09.300
Cadence. Okay, so if I'm
if I'm Having to do

344
00:22:09.300 --> 00:22:11.500
that like how would I
know that my model needs

345
00:22:11.500 --> 00:22:13.500
to be retrained regularly and
do what I then kind

346
00:22:13.500 --> 00:22:16.900
of introduce for lack of
a better term continuous integration

347
00:22:16.900 --> 00:22:19.700
and continuous deployment into my
model so that I've got

348
00:22:19.700 --> 00:22:21.200
a model I use it
my company and then we

349
00:22:21.200 --> 00:22:25.200
decide every quarter will go
and retrain and redeploy in

350
00:22:25.200 --> 00:22:29.200
terms of using something like
continuous integration for machine learning

351
00:22:29.200 --> 00:22:35.600
that is very problem specific.
I often see models retrains

352
00:22:35.600 --> 00:22:41.100
in practice and have to
bigger. Situations one is when

353
00:22:41.600 --> 00:22:46.100
there's now like an undesirable
outcome in terms of error

354
00:22:46.100 --> 00:22:50.400
or something and a different
process has happened where maybe

355
00:22:50.400 --> 00:22:54.000
it takes the model longer
than it should or an

356
00:22:54.000 --> 00:23:00.500
increase in maybe false positives.
The other situation is when

357
00:23:00.800 --> 00:23:04.700
maybe the model needs to
support a new feature or

358
00:23:04.700 --> 00:23:09.100
the nature of the data
is going to change. So

359
00:23:09.100 --> 00:23:12.200
much in the future that
the old model will not

360
00:23:12.200 --> 00:23:17.100
be relevant. So a common
strategy. I've seen a lot

361
00:23:17.100 --> 00:23:20.400
of companies use in terms
of when to promote a

362
00:23:20.400 --> 00:23:25.700
new model to production is
to like actually use a/b

363
00:23:25.700 --> 00:23:30.400
testing and see how a
model is performing relative to

364
00:23:30.600 --> 00:23:34.200
the old trusty production model.
Hmm. That's a very good

365
00:23:34.200 --> 00:23:37.100
point. If one is going
to introduce revisions to their

366
00:23:37.100 --> 00:23:40.500
model one should not just
blindly deploy the newer one

367
00:23:40.500 --> 00:23:43.400
because it's newer it may
just well be worse and

368
00:23:43.400 --> 00:23:45.400
unless you test you won't
ever know which is a

369
00:23:45.400 --> 00:23:48.200
good reminder that whether it's
machine learning or deep learning

370
00:23:48.200 --> 00:23:52.000
or just straight procedural programming,
you know measure twice cut.

371
00:23:52.000 --> 00:23:54.800
Once that is a really
good point trying to figure

372
00:23:54.800 --> 00:23:57.400
out the integration point for
these things how the data

373
00:23:57.400 --> 00:24:02.300
scientist interacts with the development
team and where the line

374
00:24:02.600 --> 00:24:05.400
kind of blurs is kind
of a little bit fascinating

375
00:24:05.400 --> 00:24:08.000
to me. I know that
you work on systems with

376
00:24:08.000 --> 00:24:11.800
large. Another engineers and usually
I assume that this does

377
00:24:11.800 --> 00:24:14.800
this data scientists it with
the developers and you all

378
00:24:14.800 --> 00:24:18.100
work hand-in-hand you're a developer
as well. I would say

379
00:24:18.100 --> 00:24:22.600
that during the earlier stages
of a project. I'm more

380
00:24:22.600 --> 00:24:26.400
likely to work with it
data engineer or other data

381
00:24:26.400 --> 00:24:32.900
scientists and towards final stages
of project. I'm more likely

382
00:24:32.900 --> 00:24:37.200
to work with a developer
or software engineer to you

383
00:24:37.200 --> 00:24:40.700
know build the API. Make
sure that everything is being

384
00:24:40.700 --> 00:24:44.500
well integrated and then pours
the end. It is one

385
00:24:44.500 --> 00:24:46.600
runs on its own and
you're in your needed less

386
00:24:46.600 --> 00:24:49.100
less. I want it's more
of a keep it keep

387
00:24:49.100 --> 00:24:52.000
the lights on situation. I
would say so I guess

388
00:24:52.000 --> 00:24:55.200
after you said that one
example of a past experience

389
00:24:55.200 --> 00:24:59.400
relate pop to mind. I've
built some digital fraud models

390
00:24:59.400 --> 00:25:04.500
in the past and to
detect fraudulent transactions and fraudsters

391
00:25:04.500 --> 00:25:09.900
never sleep. So in that
sense data Just like need

392
00:25:09.900 --> 00:25:13.200
to continually work to make
sure that their models are

393
00:25:13.200 --> 00:25:17.700
preventing as much fraud as
possible. In other circumstances. My

394
00:25:17.700 --> 00:25:21.600
job would be done and
maybe someone would call me

395
00:25:21.600 --> 00:25:25.800
for future support or something
looks strange, but I know

396
00:25:25.800 --> 00:25:27.900
I've been saying this a
lot. It really depends on

397
00:25:27.900 --> 00:25:31.300
the situation fraud would be
a huge example of the

398
00:25:31.300 --> 00:25:34.600
work is never finished. Well,
I think it's important that

399
00:25:34.600 --> 00:25:36.500
you are saying that a
lot and I don't think

400
00:25:36.500 --> 00:25:38.600
that that's something to apologize
for because what you're really

401
00:25:38.600 --> 00:25:43.200
doing. Scribing is if ever
there were an industry where

402
00:25:43.400 --> 00:25:48.000
quote-unquote it depends. It is
machine learning and deep learning

403
00:25:48.000 --> 00:25:51.200
and when you're training these
models I'm hearing you say

404
00:25:51.200 --> 00:25:54.800
that really a true understanding
of the problem space is

405
00:25:54.800 --> 00:25:59.800
as important as anything. It
is important in that three-legged

406
00:25:59.800 --> 00:26:03.300
stool that you were describing
about the data the algorithm

407
00:26:03.800 --> 00:26:07.400
and the problem space and
to not think about that

408
00:26:07.400 --> 00:26:10.100
or not respect that. At
that understanding the problem space

409
00:26:10.100 --> 00:26:13.700
is important is is to
Doom yourself to fail. Oh,

410
00:26:13.700 --> 00:26:16.000
I could not agree more
with what you just said.

411
00:26:16.000 --> 00:26:19.000
It's hearing someone else say
it is like music for

412
00:26:19.000 --> 00:26:22.600
my ears, huh? That makes
me feel like I know

413
00:26:22.600 --> 00:26:24.600
I understand that. You know,
that's why we do podcast

414
00:26:24.600 --> 00:26:26.900
right? Because we try to
come to an understanding so

415
00:26:26.900 --> 00:26:29.900
that myself and The Listener
and the audience all go.

416
00:26:30.100 --> 00:26:33.000
Ah, okay. Now we do
understand it because I can

417
00:26:33.000 --> 00:26:35.700
paraphrase it back to you
which makes me happy great

418
00:26:35.700 --> 00:26:39.600
and it had just because
data Sciences I mean, I

419
00:26:39.600 --> 00:26:44.300
would say business and technology
think it's really a creative

420
00:26:44.300 --> 00:26:48.700
job that requires a lot
of empathy trying to make

421
00:26:48.700 --> 00:26:53.200
something feasible for both the
business user or the customer

422
00:26:53.200 --> 00:26:59.600
and the like technology engineering
side without asking questions and

423
00:26:59.700 --> 00:27:04.300
really diving deep into those
problems. Just you're not going

424
00:27:04.300 --> 00:27:07.100
to be able to do
a good job pleasing both

425
00:27:07.100 --> 00:27:09.800
groups of people. Yeah. Absolutely,
you've got a lot of

426
00:27:09.800 --> 00:27:14.200
constraints to work within and
this is where the the

427
00:27:14.200 --> 00:27:16.600
model is such a for
lack of a better word

428
00:27:16.600 --> 00:27:19.100
a squishy thing, you know,
you can't really put your

429
00:27:19.400 --> 00:27:22.300
finger on it and you
can't really understand it. So

430
00:27:22.300 --> 00:27:25.500
you make sure that the
things that you do understand

431
00:27:25.500 --> 00:27:28.300
or as clear as possible
so that the inputs the

432
00:27:28.300 --> 00:27:31.500
three inputs that we've been
describing are put in and

433
00:27:31.500 --> 00:27:33.600
if you come to new
understanding and you understand it

434
00:27:33.600 --> 00:27:36.900
better then you make sure
that that understanding is fed

435
00:27:36.900 --> 00:27:40.200
back into the model, of
course. And it's a very

436
00:27:40.200 --> 00:27:44.200
iterative process where like maybe
we explore the data. We

437
00:27:44.200 --> 00:27:48.300
build a model and back
when we gain more understanding

438
00:27:48.500 --> 00:27:52.200
it just kind of Cycles
on an iteration until enough

439
00:27:52.200 --> 00:27:55.700
Cycles have happened where it's
ready to be deployed or

440
00:27:55.700 --> 00:27:59.100
all the wisdom that can
be cleaned. Is there not

441
00:27:59.100 --> 00:28:00.600
to put you on the
spot, but do you have

442
00:28:00.600 --> 00:28:03.800
a favorite book or a
favorite starting point for people

443
00:28:03.800 --> 00:28:05.500
who are listening and they
want to go and they

444
00:28:05.500 --> 00:28:08.200
want to do a I
like I send time sometimes

445
00:28:08.200 --> 00:28:11.700
people lie. I say to
go to AI school which

446
00:28:11.700 --> 00:28:14.400
is a thing that Microsoft
offers there's lots of different

447
00:28:14.400 --> 00:28:17.300
online tutorials and books. I'm
curious what your favorite is

448
00:28:17.300 --> 00:28:20.000
or favorite author in the
space. This is probably going

449
00:28:20.000 --> 00:28:23.000
to reveal I'm a certain
type of data scientists more

450
00:28:23.000 --> 00:28:29.000
than others. I enjoyed the
textbook Bayesian data analysis. I

451
00:28:29.500 --> 00:28:33.600
really like this book because
the Bayesian school of thought

452
00:28:33.600 --> 00:28:38.600
helps people apply other wisdom
from areas that they do.

453
00:28:38.800 --> 00:28:43.100
Know something about and helps
people integrate that prior knowledge

454
00:28:43.100 --> 00:28:48.200
into other models that then
can update those opinions and

455
00:28:48.200 --> 00:28:53.500
prior knowledge. So philosophically I
enjoyed that book for someone

456
00:28:53.500 --> 00:28:56.900
trying to learn but I
don't actually spend a lot

457
00:28:56.900 --> 00:29:00.800
of time reading data science
or machine learning books. I'm

458
00:29:00.800 --> 00:29:04.600
more likely to read a
tech article or perhaps a

459
00:29:04.600 --> 00:29:07.500
medium post and I want
to go ahead and get

460
00:29:07.500 --> 00:29:11.000
links from you or suggested
reading there's all kinds of

461
00:29:11.000 --> 00:29:14.500
great great places online where
people can learn like edx

462
00:29:14.500 --> 00:29:17.900
dot edu has a great
data science articles Coursera and

463
00:29:17.900 --> 00:29:20.800
others. So we'll have a
link to that book and

464
00:29:20.800 --> 00:29:23.000
others in the show notes
for this show. That would

465
00:29:23.000 --> 00:29:25.900
be great. I had look
forward to sending those along

466
00:29:25.900 --> 00:29:28.900
fantastic. Thanks so much for
chatting with me Sarah Beck.

467
00:29:29.000 --> 00:29:32.600
She's a machine learning solution
principal at slalom build and

468
00:29:32.600 --> 00:29:34.300
be sure to check out
the show notes for all

469
00:29:34.300 --> 00:29:37.100
the different things that we
talked about. This has been

470
00:29:37.100 --> 00:29:39.500
another episode of Hansel minutes.
It's and we'll see you

471
00:29:39.500 --> 00:29:40.700
again next week.

