WEBVTT FILE

1
00:00:00.840 --> 00:00:03.960
Hi, this is Scott. I
really appreciate our sponsors because

2
00:00:03.960 --> 00:00:07.320
they make the show possible.
Today's show is sponsored by

3
00:00:07.350 --> 00:00:11.580
Tellerik create compelling app experiences
across any screen with the

4
00:00:11.580 --> 00:00:16.440
Tellerik platform Telerx end to
end platform. Uniquely combines industry

5
00:00:16.440 --> 00:00:20.130
leading UI tools with cloud
services to simplify the entire

6
00:00:20.130 --> 00:00:24.600
app development cycle Tellerik offers
everything.net developers need to build

7
00:00:24.600 --> 00:00:34.020
quality apps faster. Try it
free at tellerik.com/platform that's tellerik.com/platform

8
00:00:47.420 --> 00:00:53.360
From hanselminutes.com Hansel minutes, a
weekly discussion with web developer

9
00:00:53.540 --> 00:00:58.010
technologist Scott Hanselman. This is
Lawrence Ryan and Madison show number

10
00:00:58.010 --> 00:01:02.270
four 67. Okay. This episode,
Scott talks with doctor. How

11
00:01:02.810 --> 00:01:09.290
about making robots smarter? Hi,
this is Scott Hanselman. This

12
00:01:09.290 --> 00:01:11.450
is another episode of Hansel
minutes. And today I am

13
00:01:11.450 --> 00:01:15.230
talking with dr. Ayana Howard.
She is from zero Biotics

14
00:01:15.230 --> 00:01:18.260
and she has her BS
in electrical engineering. Her master's

15
00:01:18.260 --> 00:01:20.870
from the university of Southern
California and a PhD in

16
00:01:20.870 --> 00:01:23.840
electrical engineering. So I am
quite intimidated and thrilled, thrilled,

17
00:01:23.840 --> 00:01:26.570
thrilled to be talking to
you. Thank you. Thank you.

18
00:01:26.570 --> 00:01:28.930
I'm excited to be on
the show. Thank you. Yeah.

19
00:01:28.930 --> 00:01:32.180
And you were at JPL
also, weren't you? I was,

20
00:01:32.180 --> 00:01:35.840
I was there for, Oh,
over 10 years, almost 15

21
00:01:35.840 --> 00:01:39.110
years working on Mars stuff.
Oh, wow. That's pretty exciting.

22
00:01:39.410 --> 00:01:41.930
I had the pleasure of
talking with, I think the

23
00:01:41.930 --> 00:01:47.750
CTO of JPL recently in
where were we somewhere in

24
00:01:47.750 --> 00:01:52.160
Scandinavia. Oh, what did he
say? We talked about kind

25
00:01:52.160 --> 00:01:54.890
of the, the future of
technology, a in a general

26
00:01:54.890 --> 00:01:58.100
sense and where he thinks
that if we invest more

27
00:01:58.100 --> 00:02:01.550
into space and into Mars,
that it will only help

28
00:02:01.550 --> 00:02:03.530
us here on, on the
ground. This was a Tom

29
00:02:03.530 --> 00:02:06.740
Sodestrom, but it's true. If
you look at a lot

30
00:02:06.740 --> 00:02:08.660
of the research, that's come
out of the NASA world.

31
00:02:08.930 --> 00:02:10.670
A lot of it we
use today. I mean, even

32
00:02:10.670 --> 00:02:16.490
something as simple as tango
are as simple as eye

33
00:02:16.490 --> 00:02:20.090
glasses for playing our golf
balls. A lot of things

34
00:02:20.090 --> 00:02:23.000
we just assume it's like,
Oh, this is commercial actually

35
00:02:23.000 --> 00:02:26.540
came out of NASA research.
Have you read the, this

36
00:02:26.540 --> 00:02:28.250
is random, but have you
read the Martian? Have you

37
00:02:28.250 --> 00:02:31.580
seen this book? The Martian?
No. Who is this? Who's

38
00:02:31.580 --> 00:02:33.470
the author. Oh my goodness.
Let me get the author

39
00:02:33.470 --> 00:02:35.600
for you. So this is
the best book that I

40
00:02:35.600 --> 00:02:40.010
have read all, all year
long. And the author is

41
00:02:40.640 --> 00:02:45.590
Andy ware, w E I
R. And what the Martian

42
00:02:45.590 --> 00:02:50.210
is all about is a
theoretical human Mars landing, where

43
00:02:50.360 --> 00:02:55.100
one of the individuals get
stranded and he has, you

44
00:02:55.100 --> 00:02:57.050
know, they just basically, you
know, there was a storm

45
00:02:57.050 --> 00:02:58.460
and they had to take
off and they left in

46
00:02:58.460 --> 00:03:01.510
there accidentally, and now he
has to survive for six

47
00:03:01.510 --> 00:03:06.910
or 700 days. And it's
completely plausible because it's really

48
00:03:06.910 --> 00:03:09.280
focused on the science, like
how he fixes the water

49
00:03:09.280 --> 00:03:12.520
reclaimer and how he gets
potatoes growing in a little

50
00:03:12.520 --> 00:03:15.820
greenhouse. And you know, all
the little things involved and

51
00:03:16.150 --> 00:03:18.850
all that tech is based
on, you know, if they

52
00:03:18.850 --> 00:03:20.740
had left anyone else who
didn't have a really strong

53
00:03:20.740 --> 00:03:22.680
sense of science, he would
have been dead. He would

54
00:03:22.680 --> 00:03:26.310
have been, he would have
lasted up until the night.

55
00:03:27.240 --> 00:03:29.760
No, that's actually great. So
that means it was assumed

56
00:03:29.760 --> 00:03:32.490
there was some infrastructures, so
he could grab some of

57
00:03:32.490 --> 00:03:35.460
the technology. They've landed, they've
set up their tent. They've

58
00:03:35.460 --> 00:03:38.400
got the couple of rovers.
He also has, interestingly, the

59
00:03:38.400 --> 00:03:40.500
other things that they left
on Mars over the last

60
00:03:40.500 --> 00:03:44.280
20 years or 15 years.
So you got solar panels,

61
00:03:44.940 --> 00:03:49.080
he's got some old microprocessors
and he can program up

62
00:03:49.080 --> 00:03:52.290
some, a heater in fact,
cause the rovers have internal

63
00:03:52.290 --> 00:03:56.340
heaters. Okay. Okay. Right. So,
you know, I'm trying to

64
00:03:56.340 --> 00:03:59.670
get my nontechnical friends to
read this, but as soon

65
00:03:59.670 --> 00:04:01.380
as they see that it's
Oh, well he did this

66
00:04:01.380 --> 00:04:04.020
many amps through this and
then, Oh, that's too technical.

67
00:04:04.770 --> 00:04:09.550
Yeah, absolutely fascinating and wonderful
example of real technology on

68
00:04:09.550 --> 00:04:11.790
a, on a foreign, Oh,
I'll have to take a

69
00:04:11.790 --> 00:04:15.000
look at it. So tell
me about XY robotics, because

70
00:04:15.000 --> 00:04:17.670
it seems like you guys
do many, many things And

71
00:04:17.670 --> 00:04:20.580
we do so is our
robotics is actually a Georgia

72
00:04:20.580 --> 00:04:25.380
tech startup. So the technology
that our robotics focus on

73
00:04:25.410 --> 00:04:30.600
is on technology that can
impact humankind, primarily individuals with

74
00:04:30.600 --> 00:04:34.470
disabilities. And we focus right
now on children with disabilities.

75
00:04:35.280 --> 00:04:37.080
And so the goal is,
is to take what we

76
00:04:37.080 --> 00:04:39.300
take for granted. So if
you think about the wording

77
00:04:39.300 --> 00:04:42.540
of pervasive technologies, which are
tablets and cell phones and

78
00:04:42.540 --> 00:04:46.590
computers and things that we
just naturally assume everyone uses,

79
00:04:47.400 --> 00:04:50.130
if you have issues, if
you have physical issues in

80
00:04:50.130 --> 00:04:53.760
terms of impairments, it's difficult
for you to say pension

81
00:04:53.760 --> 00:04:57.420
swipe, or if you have
any cognitive impairments, so that

82
00:04:57.720 --> 00:05:01.950
it's difficult for you to
focus our sensory impairments, example,

83
00:05:01.950 --> 00:05:06.270
children with autism, this technology,
the pervasive technology we take

84
00:05:06.270 --> 00:05:09.600
advantage of becomes difficult to
use. So we're trying to

85
00:05:09.600 --> 00:05:12.600
kind of bridge that gap
between what's already out there

86
00:05:12.870 --> 00:05:17.160
and these children with disabilities
And are these custom custom

87
00:05:17.160 --> 00:05:19.770
machines, or are you making
modifications to machines that already

88
00:05:19.770 --> 00:05:23.040
exist? So we designed, you
can think of it as

89
00:05:23.040 --> 00:05:26.460
interfaces. So in terms of
the hardware, their interfaces to

90
00:05:26.460 --> 00:05:31.530
say tablet, computers, touchscreen interfaces,
and what technology the child

91
00:05:31.530 --> 00:05:33.990
already uses. So as an
example, if, if a child

92
00:05:33.990 --> 00:05:36.930
is using a wheelchair, they
typically control it with a

93
00:05:36.930 --> 00:05:39.450
joystick. So is there a
way I can just take

94
00:05:39.450 --> 00:05:42.570
my joystick, which I'm very
used to using and hook

95
00:05:42.570 --> 00:05:44.610
it up to something. So
I can use those same

96
00:05:44.610 --> 00:05:48.450
motions to now interact with
tablet apps. So we're providing

97
00:05:48.450 --> 00:05:51.690
the interface between what's already
being used by the child

98
00:05:51.930 --> 00:05:55.590
and what's already out there,
this pervasive technology. Interesting. When

99
00:05:55.590 --> 00:05:57.440
I read about the things
that they've done to Steve

100
00:05:57.440 --> 00:06:02.150
for Stephen Hawking's because he's,
he's singular, he gets customer

101
00:06:02.330 --> 00:06:05.180
behavior, he gets custom, everything,
you know, he has his

102
00:06:05.180 --> 00:06:07.880
own software and his own
equipment. Can you do that

103
00:06:07.880 --> 00:06:09.680
for every child or do
you, can you make it

104
00:06:09.680 --> 00:06:12.370
more generic? Well, we have
to make it more generic

105
00:06:12.400 --> 00:06:16.980
because custom also means very
costly. And so these children

106
00:06:17.060 --> 00:06:20.320
already have in terms of
medical costs and having to

107
00:06:20.320 --> 00:06:23.500
go through therapy, the price
that their parents are paying

108
00:06:23.500 --> 00:06:27.850
are ready financially is unbelievable.
And so, you know, a

109
00:06:27.860 --> 00:06:30.580
child could have a custom
solution, but it's not realistic.

110
00:06:30.880 --> 00:06:33.190
And so what we're trying
to do is create solutions

111
00:06:33.190 --> 00:06:36.070
that actually adapt to whatever
the ability of the child

112
00:06:36.070 --> 00:06:39.010
is. So that's adaptive both
in a software sense. And

113
00:06:39.010 --> 00:06:42.340
then in whatever hardware that
you create, that interfaces, Yes.

114
00:06:42.410 --> 00:06:45.370
Or the way we adapt,
we focus on the software.

115
00:06:45.910 --> 00:06:49.630
So we take the hardware.
It's generic. It provides, you

116
00:06:49.630 --> 00:06:52.570
could think of it as
a hardware, providing all inputs.

117
00:06:52.570 --> 00:06:55.540
So all the possible inputs.
And then the software is

118
00:06:55.540 --> 00:06:58.630
where you filter it out.
You feel out good data

119
00:06:58.630 --> 00:07:03.010
from that data. So we
use the software to drive

120
00:07:03.010 --> 00:07:07.030
the magic of interaction. You
know, I'm, I'm just now

121
00:07:07.060 --> 00:07:10.300
getting my kids involved in,
you know, little robots, basic

122
00:07:10.300 --> 00:07:13.810
stuff, you know, a couple
of motors. And I think

123
00:07:13.810 --> 00:07:15.730
we're taking a big track.
You know, the, remember the

124
00:07:15.730 --> 00:07:18.580
big track from the eighties,
the big, see, it was

125
00:07:18.580 --> 00:07:20.400
almost like a Mars Lander,
and we're going to play

126
00:07:20.410 --> 00:07:22.780
that. We're always going to
put in a plug, a

127
00:07:22.780 --> 00:07:26.320
raspberry PI into that, but
I'm realizing that from again,

128
00:07:26.350 --> 00:07:28.390
I speaking from a place
of ignorance, so forgive me,

129
00:07:28.720 --> 00:07:31.420
but it doesn't feel like
a lot has really happened.

130
00:07:31.870 --> 00:07:34.390
There's, you know, voltage to
a motor and the motor

131
00:07:34.390 --> 00:07:37.090
spins, the motor maybe has
a clutch, or it doesn't

132
00:07:37.090 --> 00:07:41.080
have a clutch, but we're
finding ourselves really the motor

133
00:07:41.080 --> 00:07:43.270
doesn't know when to stop
or the robot crashes into

134
00:07:43.270 --> 00:07:46.630
the wall. There's so many
layers of abstractions that we're

135
00:07:46.630 --> 00:07:50.560
missing. Why is that? And
why are the robots so

136
00:07:50.560 --> 00:07:54.550
dumb Because the robots. So
if you think about the

137
00:07:54.550 --> 00:07:57.730
hardware, the hardware, the motors
and time, if you think

138
00:07:57.730 --> 00:08:00.130
about the research is getting
it to be lower cost,

139
00:08:00.130 --> 00:08:05.890
smaller, more power, more torque,
those, those issues aren't helping,

140
00:08:05.890 --> 00:08:10.090
the robot becomes smarter. What's
really making these robots smarter

141
00:08:10.090 --> 00:08:13.270
is intelligence is the software
programming. So if you notice

142
00:08:13.270 --> 00:08:16.300
there's been a lot of
issues, I would say drama

143
00:08:16.330 --> 00:08:20.230
about artificial intelligence lately in
the news about how we

144
00:08:20.230 --> 00:08:23.740
need to be careful about
AI and robotics, because you

145
00:08:23.740 --> 00:08:28.600
know, we're doing to create
our own destiny of being

146
00:08:28.810 --> 00:08:32.830
eliminated. And why is that?
Because if you have robots,

147
00:08:32.830 --> 00:08:36.370
which are, if you think
about physically they're functional, they

148
00:08:36.370 --> 00:08:39.190
can do things just like
us. And if you add

149
00:08:39.190 --> 00:08:42.580
that, the ability for them
to be intelligent and think,

150
00:08:42.580 --> 00:08:46.270
and not run into walls
and navigate intelligently and figure

151
00:08:46.270 --> 00:08:50.560
out how to do things
new. Well, then what happens,

152
00:08:50.560 --> 00:08:53.770
what happens to these robots
that are as functional as

153
00:08:53.770 --> 00:08:57.750
us, and maybe can think
better than maybe half the

154
00:08:57.750 --> 00:09:01.020
population at some point, or
learn faster than us. You

155
00:09:01.020 --> 00:09:05.190
know, there's this fear that
we're, we're doomed to a

156
00:09:05.190 --> 00:09:09.230
destiny where there will be
no more humankind. Ah, okay.

157
00:09:09.230 --> 00:09:12.170
So tying that back to
what you were saying about

158
00:09:12.170 --> 00:09:15.860
using the existing pieces of
hardware that these children have,

159
00:09:16.130 --> 00:09:18.260
it really is the magic
of the software. That's allowing

160
00:09:18.260 --> 00:09:20.750
you to take a basic
joystick. And as you said,

161
00:09:20.750 --> 00:09:22.580
filtering, maybe you can talk
a little bit about that

162
00:09:22.580 --> 00:09:25.310
filtering the input because not
everyone can simply push forward

163
00:09:25.310 --> 00:09:29.090
on a joystick and have
it be intentionally forward. Right?

164
00:09:29.090 --> 00:09:32.120
So as an example, if
you think about a joystick

165
00:09:32.270 --> 00:09:37.010
movements, you call that are
uncontrollable are unintentional. So it

166
00:09:37.010 --> 00:09:40.280
might be that I have
tremors are, it might be

167
00:09:40.280 --> 00:09:43.550
that I want to go
forward, but I can only

168
00:09:43.550 --> 00:09:45.890
make a little bit of
a movement. So just, just

169
00:09:45.890 --> 00:09:47.660
a little bit of a,
if you look at the

170
00:09:47.660 --> 00:09:50.660
angles, you know, maybe a
five degree angle versus someone

171
00:09:50.660 --> 00:09:52.820
who is fully capable can
do a a hundred degree

172
00:09:52.820 --> 00:09:55.940
angle. So how do I
take that same joystick that's

173
00:09:55.940 --> 00:09:59.990
used in different ways and
have it say control an

174
00:09:59.990 --> 00:10:04.370
application where you can navigate
a YouTube channel. So in

175
00:10:04.370 --> 00:10:06.740
that one is, you know,
if you think about a

176
00:10:06.740 --> 00:10:09.890
YouTube channel, you go in
and the functions you need

177
00:10:09.890 --> 00:10:13.790
are pretty much some type
of search and play and

178
00:10:13.790 --> 00:10:18.980
stop. And so how do
you have a joystick that

179
00:10:19.610 --> 00:10:23.360
goes from five degrees to
90 degrees give you the

180
00:10:23.360 --> 00:10:26.090
same function as you know,
I want to hit the

181
00:10:26.090 --> 00:10:30.530
play button. Wow. Okay. So
that, that is now kind

182
00:10:30.530 --> 00:10:32.600
of changing my, the way
that I'm thinking about the

183
00:10:32.600 --> 00:10:36.050
relationship with software to hardware,
that the hardware really is

184
00:10:36.290 --> 00:10:39.260
the smaller part of the
problem. It's just how you

185
00:10:39.290 --> 00:10:41.900
process those signals, that input
from the hardware. That is

186
00:10:41.900 --> 00:10:43.970
where all the magic is.
Yeah. The magic is in

187
00:10:43.970 --> 00:10:47.150
the software. I mean, hardware
getting it to be robust

188
00:10:47.150 --> 00:10:51.050
and reliable is difficult, but
making it so that it

189
00:10:51.050 --> 00:10:55.880
works intelligently, it's a software,
it's definitely the software. So

190
00:10:55.880 --> 00:10:58.550
that would explain why my
robot's always smashed into the

191
00:10:58.550 --> 00:11:01.250
walls is that I'm simply
telling them really basic stuff

192
00:11:01.280 --> 00:11:04.460
like go forward. And I'm
not thinking at all about

193
00:11:04.790 --> 00:11:08.000
getting feedback back from those
modes. Right. We have a

194
00:11:08.000 --> 00:11:11.660
saying garbage in garbage out.
So if you put programming

195
00:11:11.660 --> 00:11:16.040
garbage in, you're going to
get programmed behavior out. Absolutely.

196
00:11:16.040 --> 00:11:17.570
I will be sure not
to tell my children that,

197
00:11:17.570 --> 00:11:19.940
but I think, Oh no,
you have to be very

198
00:11:19.940 --> 00:11:25.730
kind with that. So you've
got applications for tablets and

199
00:11:25.730 --> 00:11:27.890
I've, I've seen, I've seen
one of the applications you

200
00:11:27.890 --> 00:11:31.400
have coming soon is called
the Zoomo learning system. Yes.

201
00:11:32.120 --> 00:11:36.200
So the Zoomer learning system
is a unique approach to

202
00:11:36.200 --> 00:11:40.310
this assessability. So what we
found was that younger kids,

203
00:11:40.310 --> 00:11:43.820
primarily, if you think about
a child who's first been

204
00:11:43.820 --> 00:11:49.400
diagnosed with a disability, they
haven't quite gotten to learn

205
00:11:49.400 --> 00:11:53.740
how to say controller joystick
with intent, how do start

206
00:11:53.770 --> 00:11:57.370
teaching them or training them,
having them to do therapy

207
00:11:58.090 --> 00:12:02.500
in a way that feels
natural. And so kids actually

208
00:12:02.500 --> 00:12:05.140
enjoy toys. It doesn't matter
whether you have a disability

209
00:12:05.140 --> 00:12:08.440
or not. There's something about
having the ability to play

210
00:12:08.770 --> 00:12:12.550
and to interact with your
world. And so Zoomo is

211
00:12:12.580 --> 00:12:15.850
a device and inside of
it as the same brain

212
00:12:15.910 --> 00:12:19.840
as the system that allows
you to connect the joystick

213
00:12:19.840 --> 00:12:22.450
to the tablet, same brain.
But what we do is

214
00:12:22.450 --> 00:12:25.600
we put it into a,
it's basically a stuffed animal

215
00:12:25.870 --> 00:12:30.400
where we can isolate movements
using sensors so that you

216
00:12:30.400 --> 00:12:35.590
can interact with your toy
companion, for example, but still

217
00:12:35.590 --> 00:12:38.320
interact with your tablet. And
so the design of it

218
00:12:38.320 --> 00:12:41.500
is that it, it actually
has a joystick design. So

219
00:12:41.500 --> 00:12:44.260
as you learn how to
move your hand in the

220
00:12:44.710 --> 00:12:49.240
forward and backward left and
right direction, you're actually also

221
00:12:49.300 --> 00:12:53.320
training yourself how to have
intent with, say, for example,

222
00:12:53.320 --> 00:12:56.970
a joystick. Interesting. And then
what other sensors that you're

223
00:12:56.970 --> 00:12:59.940
using in the, in the
controllers inside there? Is there

224
00:12:59.940 --> 00:13:01.710
a brain in there or
are you running that through

225
00:13:01.710 --> 00:13:03.930
the tablet? No, no, no.
There's, there's a brain in

226
00:13:03.930 --> 00:13:06.930
there and we use something
you can think of it

227
00:13:06.930 --> 00:13:10.980
as a force sensor force,
resistive sensors. So equivalent to

228
00:13:10.980 --> 00:13:15.570
that, basically we're looking at
aspects of pressure, which has

229
00:13:15.570 --> 00:13:18.300
also filtered out. So if
a child has very little,

230
00:13:18.300 --> 00:13:22.740
basically they have very little,
have very little ability to

231
00:13:22.740 --> 00:13:26.190
apply a pressure versus a
child who can bang away.

232
00:13:26.670 --> 00:13:29.310
How do you take that?
And also convert it into

233
00:13:29.310 --> 00:13:32.100
these, you know, swipe and
pinch gestures on the tablet.

234
00:13:32.640 --> 00:13:35.670
Are you making generic algorithms
or are you making algorithms

235
00:13:35.670 --> 00:13:38.100
that can tune themselves or
that a parent can tune?

236
00:13:39.150 --> 00:13:45.540
So right now the algorithms
are self adaptive. So they

237
00:13:45.540 --> 00:13:48.630
are taking and we have,
in some of our apps,

238
00:13:48.630 --> 00:13:52.650
we actually have the ability
to learn basically a learning

239
00:13:52.650 --> 00:13:56.220
behavior. So they self adapt.
Now they don't self adapt.

240
00:13:56.250 --> 00:13:58.980
I E as I'm, you
have to tell it, I

241
00:13:58.980 --> 00:14:02.280
need you to self adapt
right now, at least. And

242
00:14:02.280 --> 00:14:05.370
because we don't want the,
the behavior to change. So

243
00:14:05.370 --> 00:14:07.560
as soon as you have
it working, you don't want

244
00:14:07.560 --> 00:14:11.370
the behavior to start changing
because it's trying to improve

245
00:14:11.370 --> 00:14:14.340
it unless the parent or
the therapist or the special

246
00:14:14.340 --> 00:14:16.920
ed teacher wants that. And
so you basically have to,

247
00:14:16.920 --> 00:14:20.190
so there's where the, the
unintelligent part is, is you

248
00:14:20.190 --> 00:14:22.380
actually have to tell the
software. It is basically just

249
00:14:22.380 --> 00:14:24.450
a little button. Okay. I
want you to learn. I

250
00:14:24.450 --> 00:14:27.780
want you to adapt versus
not. I see. So you,

251
00:14:28.140 --> 00:14:30.150
you improve it and then
you reach a plateau and

252
00:14:30.150 --> 00:14:32.460
you stay at that plateau.
So it is reliable and

253
00:14:32.460 --> 00:14:34.140
it can be counted on
by the child. And then

254
00:14:34.140 --> 00:14:37.290
if the child wants to
improve again, or if maybe

255
00:14:37.290 --> 00:14:39.840
they have regressed in some
way physically, then, then you

256
00:14:39.840 --> 00:14:43.170
can relearn and find a
new model. Correct? Correct. Exactly.

257
00:14:44.160 --> 00:14:50.190
Okay. In looking through your
extensive scholarly works and it's,

258
00:14:50.220 --> 00:14:52.400
it's really amazing worked on
it. It's such a wide

259
00:14:52.400 --> 00:14:54.470
variety of things. One thing
stood out to me that

260
00:14:54.470 --> 00:14:57.920
I wanted to understand this,
this term, soft computing and

261
00:14:57.920 --> 00:15:01.070
soft computing strategies and soft
computing approaches. Can you explain

262
00:15:01.070 --> 00:15:04.630
that to me in layman's
terms? So soft computing is

263
00:15:04.630 --> 00:15:09.250
a, I would say a
field of knowledge that is

264
00:15:10.120 --> 00:15:15.280
designed to imitate how people
think. So as an example,

265
00:15:15.280 --> 00:15:19.270
what belongs in the soft
computing world is methodology such

266
00:15:19.270 --> 00:15:22.330
as fuzzy logic. And so
fuzzy logic is if you

267
00:15:22.330 --> 00:15:25.630
think about how we drive,
so we drive a car,

268
00:15:25.900 --> 00:15:27.550
and if you ever had
a teenager, you had to

269
00:15:27.550 --> 00:15:30.070
teach how to drive. You
know, you would say, okay,

270
00:15:30.070 --> 00:15:32.440
when you get to this
corner, you do a hard

271
00:15:32.440 --> 00:15:35.980
left, or, you know, you
gently step on the gas

272
00:15:35.980 --> 00:15:38.680
while you're turning. He use
terms like that. You don't

273
00:15:38.680 --> 00:15:42.460
say, okay, you need to
turn your wheel 20 degrees

274
00:15:42.490 --> 00:15:46.780
and oppress your foot down
with 20 Newtons. You don't

275
00:15:46.780 --> 00:15:49.570
use that kind of language.
You use this very linguistic

276
00:15:49.990 --> 00:15:52.810
type of information, and yet
we know how to take

277
00:15:52.810 --> 00:15:55.300
that and control whatever it
is that we need to

278
00:15:55.930 --> 00:15:58.900
example the car. But we
know that based on so

279
00:15:58.900 --> 00:16:01.450
much context, like context that
we're not even thinking about,

280
00:16:01.450 --> 00:16:05.320
right. You know, years of
context, Right? So fuzzy logic,

281
00:16:05.320 --> 00:16:08.530
what it attempts to do
is it attempts to encapsulate

282
00:16:08.830 --> 00:16:11.920
some of this information as
a set of linguistic rules

283
00:16:12.700 --> 00:16:15.160
and the engine behind it
to take all this knowledge

284
00:16:15.160 --> 00:16:19.930
and come up with a
valid control methodology, because at

285
00:16:19.930 --> 00:16:22.060
the end of the day,
you still have hardware. So

286
00:16:22.120 --> 00:16:24.760
as an example, if you
have a car that needs

287
00:16:24.760 --> 00:16:27.190
to drive based on fuzzy
logic, at the end of

288
00:16:27.190 --> 00:16:28.930
the day, you still have
to provide it a certain

289
00:16:28.930 --> 00:16:31.510
amount of volts. You have
certain amount of torque, a

290
00:16:31.510 --> 00:16:35.740
certain amount of information. So
you still have this linguistic

291
00:16:35.740 --> 00:16:39.730
information, but it also still
has to be converted. Ah,

292
00:16:39.760 --> 00:16:43.840
okay. And in, in, in
the context of my world,

293
00:16:43.840 --> 00:16:46.060
which is the web development
world, sometimes I feel like

294
00:16:46.060 --> 00:16:49.510
there's only maybe three or
four layers of abstraction between

295
00:16:49.510 --> 00:16:52.060
me and, you know, kind
of machine code. Maybe that's

296
00:16:52.060 --> 00:16:54.610
overstating, let's say maybe six
to 10. You know what

297
00:16:54.610 --> 00:16:58.270
I mean? I can get
down pretty quickly and see

298
00:16:58.270 --> 00:17:00.850
where the assembler is and
where the registers are. You

299
00:17:00.850 --> 00:17:03.070
know, I have a garbage
collector, but for the most

300
00:17:03.070 --> 00:17:05.740
part, there's only a few
layers when I'm building large

301
00:17:05.740 --> 00:17:08.170
systems like this, this is
systems that have soft computing

302
00:17:08.170 --> 00:17:11.230
understanding. Are you quite a
bit deeper in the, in

303
00:17:11.230 --> 00:17:12.850
the call stack? Is there
quite a few more layers

304
00:17:12.850 --> 00:17:16.120
of abstraction? Yeah. You're actually
higher up in the levels

305
00:17:16.120 --> 00:17:20.200
of abstraction. And the, the
concept of one of the,

306
00:17:20.200 --> 00:17:22.990
I would say critiques about
it is that some people

307
00:17:22.990 --> 00:17:26.170
look at it as, as
a black box. So I

308
00:17:26.170 --> 00:17:29.890
can define inputs. I can
define outputs and the system

309
00:17:29.890 --> 00:17:39.790
itself figures out what's inside,
which yeah, because you know

310
00:17:39.790 --> 00:17:43.300
what goes in and you
assume that what comes out

311
00:17:43.300 --> 00:17:47.050
is what you want. But
how that is organized is

312
00:17:47.050 --> 00:17:50.130
uncertain, which is also the
fear, because it might be

313
00:17:50.190 --> 00:17:52.050
organized such that if you
get a different set of

314
00:17:52.050 --> 00:17:54.630
inputs, it does something you
don't want it to do.

315
00:17:57.020 --> 00:17:58.550
And I'm going back a
little bit in time here,

316
00:17:58.550 --> 00:18:01.760
but when you worked on
a soft computing approach for

317
00:18:01.790 --> 00:18:05.330
navigating planetary rovers, this is
almost 15 years ago. Did

318
00:18:05.330 --> 00:18:08.750
you ever find yourself when
testing robots on other planets

319
00:18:08.990 --> 00:18:11.450
that, you know, I, I
told it to go over

320
00:18:11.450 --> 00:18:13.310
this rock and I don't
know why it did that.

321
00:18:13.310 --> 00:18:15.080
And then you spend a
lot of time debugging. Like

322
00:18:15.080 --> 00:18:17.390
we have no clue why
the Rover thought that was

323
00:18:17.390 --> 00:18:21.080
a good idea, but It
did. Yeah. And we, we

324
00:18:21.620 --> 00:18:25.580
ultimately figure out why it
did what it did. Cause

325
00:18:25.580 --> 00:18:29.270
you own the algorithm. Exactly.
I own the algorithm. I

326
00:18:29.270 --> 00:18:32.330
mean, it's usually because the
input was noisy a lot

327
00:18:32.330 --> 00:18:34.940
of times it's because the
input was noisy. So what

328
00:18:34.970 --> 00:18:38.120
I observed it was doing
the right thing. But what

329
00:18:38.120 --> 00:18:40.610
I observed did not, it
was not compatible with what

330
00:18:40.640 --> 00:18:43.280
input it was actually getting.
There's usually what, what the

331
00:18:43.280 --> 00:18:47.360
reason was, or it, I
gave you some inputs that

332
00:18:47.360 --> 00:18:49.220
it had no idea what
to do with it. So

333
00:18:49.220 --> 00:18:52.910
it just classified it as
something totally different. And so

334
00:18:52.910 --> 00:18:54.650
I'm looking, I'm like, why
is it doing that is

335
00:18:54.650 --> 00:18:57.470
because I gave it garbage
as far as it was

336
00:18:57.470 --> 00:18:59.300
concerned. And he said, well,
I need to do something

337
00:18:59.300 --> 00:19:01.610
with this. So I'm going
to do what is my

338
00:19:01.610 --> 00:19:05.480
closest approximation. So yeah, you
could look under the hood

339
00:19:05.810 --> 00:19:07.970
and figure it out. But
as of course, it's cause

340
00:19:08.390 --> 00:19:11.450
you know, I'm designing the
algorithm. So I know exactly

341
00:19:11.450 --> 00:19:16.160
where to look, Hey, this
is Scott. You know, I

342
00:19:16.160 --> 00:19:19.100
like a good community edition.
The best things in life

343
00:19:19.130 --> 00:19:21.980
are free. And the community
license from our sponsors sync

344
00:19:21.980 --> 00:19:25.610
fusion will give individual and
small business developers access to

345
00:19:25.610 --> 00:19:29.660
more than 650 components across
12 platforms. And that includes

346
00:19:29.660 --> 00:19:34.430
WPF and windows forms, JavaScript,
Xamarin, Lightswitch. You can check

347
00:19:34.430 --> 00:19:39.320
it all out at sync,
fusion.com/hanselman to claim your free

348
00:19:39.320 --> 00:19:44.210
license. One of my, one
of my favorite programmer jokes

349
00:19:44.210 --> 00:19:48.410
is that, you know, a
nontechnical nontechnical spouse says, you

350
00:19:48.410 --> 00:19:49.700
know, would you go to
the grocery store and buy

351
00:19:49.700 --> 00:19:53.330
some bread? And if they
have eggs by six, so

352
00:19:53.330 --> 00:19:56.420
then the, you know, the
engineer goes and says, okay,

353
00:19:56.420 --> 00:19:59.030
and comes back later with
six loaves of bread and

354
00:19:59.030 --> 00:20:01.340
the nontechnical spouses, what did
you do? Why don't you

355
00:20:01.340 --> 00:20:03.650
buy six loaves of bread?
And the engineer says, well,

356
00:20:03.650 --> 00:20:10.070
they had a By six
because you prefaced it with

357
00:20:10.070 --> 00:20:13.790
the bread. If that makes
total sense, Total sense, you

358
00:20:13.790 --> 00:20:16.490
know, but again, garbage in
garbage out, like why did

359
00:20:16.490 --> 00:20:20.180
you not, if they have
X by six, you know,

360
00:20:20.240 --> 00:20:22.940
for bread, I just run
the program. It seems like,

361
00:20:23.540 --> 00:20:28.190
it seems like accessible fuzzy
logic for me, for the

362
00:20:28.190 --> 00:20:30.980
non PhD for the person
who's doing a small project,

363
00:20:30.980 --> 00:20:34.040
is this, is this possible?
Or there is fuzzy logic.

364
00:20:34.040 --> 00:20:36.170
Something that I can apply
to my robotics projects with.

365
00:20:37.070 --> 00:20:40.520
Oh yes, definitely. Definitely. And
in fact, I find that

366
00:20:41.750 --> 00:20:45.770
as long as you are
comfortable with them, not having

367
00:20:45.770 --> 00:20:50.830
to look inside the box,
you can totally explain to

368
00:20:50.830 --> 00:20:54.850
them how fuzzy logic is
used in, in the, in

369
00:20:54.850 --> 00:20:57.670
the much more abstract sense.
And so you can create

370
00:20:57.670 --> 00:21:00.610
a entire fuzzy logic Sysmex,
for example, with the robot

371
00:21:00.610 --> 00:21:04.530
hitting the wall, a good
example is if close to

372
00:21:04.550 --> 00:21:09.640
wall, turn left, I can
understand that. Right. And I

373
00:21:09.640 --> 00:21:11.470
can, and I have a
system where I can say

374
00:21:11.500 --> 00:21:13.420
if close, and so then
it's like, okay, what is

375
00:21:13.420 --> 00:21:16.210
close? Well, let's see what
sensors do we have. Okay.

376
00:21:16.210 --> 00:21:19.180
We're going to say close
is, you know, five centimeters

377
00:21:19.180 --> 00:21:21.340
or 20 centimeters. And so
that's where you put your

378
00:21:21.340 --> 00:21:25.330
human knowledge or expertise in.
And then what's left is,

379
00:21:25.330 --> 00:21:30.080
you know, anywhere from 45
degrees to 120 degrees, we

380
00:21:30.080 --> 00:21:32.440
don't care. We're just going
to classify left as, as

381
00:21:32.440 --> 00:21:36.610
general range, a person who
isn't techie can understand how

382
00:21:36.610 --> 00:21:39.210
to do that. And they
can understand that this is

383
00:21:39.210 --> 00:21:41.670
the, this is the inherent
behavior. The intrinsic behavior of

384
00:21:41.670 --> 00:21:44.010
this robot is it doesn't
like being up against that

385
00:21:44.010 --> 00:21:46.500
wall. It's going to turn
left and then I'll work

386
00:21:46.500 --> 00:21:48.780
around it. And they'll they'll.
And they'll begin to understand

387
00:21:48.780 --> 00:21:52.680
it and build that into
their design. Correct? Correct. You

388
00:21:52.680 --> 00:21:56.760
have described your research as
being centered around humanized intelligence.

389
00:21:56.760 --> 00:22:00.300
What does that mean humans?
So my goal with my

390
00:22:00.300 --> 00:22:04.410
research and in all of
my projects is to design

391
00:22:04.410 --> 00:22:07.650
robots that can function in
our world with us or

392
00:22:07.650 --> 00:22:10.620
to help us. And in
order to do that, we

393
00:22:10.620 --> 00:22:13.920
can't assume that the human
is going to change. I

394
00:22:13.920 --> 00:22:16.560
mean, they'll adapt a little
bit, cause like touching them

395
00:22:16.560 --> 00:22:19.080
swiping every time you go,
even to a bank teller,

396
00:22:19.340 --> 00:22:21.900
like, okay, wait, there's a
keyboard. What does this mean?

397
00:22:22.710 --> 00:22:26.040
So we have changed our
behavior quite drastically. So I'm

398
00:22:26.040 --> 00:22:28.050
always so disappointed. I can't
pinch and zoom on the

399
00:22:28.050 --> 00:22:35.340
GPS in my car. I'm
like, So we, we do

400
00:22:35.340 --> 00:22:39.510
adapt to technology. And so
it's not that we don't

401
00:22:39.510 --> 00:22:43.470
want humans or people to
adapt to robotics, but we

402
00:22:43.470 --> 00:22:47.550
don't want to force that.
And so therefore the best

403
00:22:47.550 --> 00:22:51.810
way to design robotic technology
is to design robots that

404
00:22:52.590 --> 00:22:56.940
function like people are function
so that when they do

405
00:22:56.940 --> 00:23:00.930
things it's intuitive to a
person. And so that's this

406
00:23:00.930 --> 00:23:04.950
whole concept of humanized intelligence.
What makes us as intelligent

407
00:23:04.950 --> 00:23:09.450
creatures? What makes us intelligent
through our behaviors, through our

408
00:23:09.450 --> 00:23:12.900
movement patterns, through what we
do, not necessarily how we

409
00:23:12.900 --> 00:23:16.980
think, but the outcome of
how we think. Okay. So

410
00:23:17.160 --> 00:23:19.410
these, these systems, these robots,
as you call them these

411
00:23:19.410 --> 00:23:22.050
autonomous systems, they have a
path that they're going to

412
00:23:22.050 --> 00:23:24.510
go through to get their
job done. And you're injecting

413
00:23:24.510 --> 00:23:27.930
the human intelligence into that
process that the human is

414
00:23:27.930 --> 00:23:30.360
almost a partner to the,
to the, to the robot.

415
00:23:30.630 --> 00:23:33.750
Human is definitely a partner,
definitely partner. So as an

416
00:23:33.750 --> 00:23:37.980
example, if I'm designing a
robot wheelchair as an example,

417
00:23:38.700 --> 00:23:42.030
the best way to do
that would be have the

418
00:23:42.030 --> 00:23:46.760
user operate their wheelchair with
whatever device they have and

419
00:23:46.760 --> 00:23:49.340
start recording that data, see
what their preference are, is

420
00:23:49.340 --> 00:23:51.830
maybe they're, you know, a
hot rodder and they like

421
00:23:51.830 --> 00:23:55.700
to go really fast or
maybe they're very slow and

422
00:23:55.700 --> 00:23:59.030
cautious and they don't go
out too often. So I

423
00:23:59.030 --> 00:24:02.300
can take that information and
take the inputs in terms

424
00:24:02.300 --> 00:24:04.850
of the sensors and their
behaviors, and then create a

425
00:24:04.850 --> 00:24:08.960
system that mimics that for
that one person. So that

426
00:24:08.960 --> 00:24:11.480
would be an example of
taking how a human operates

427
00:24:11.480 --> 00:24:15.620
in the world and creating
a control system, a logic

428
00:24:15.620 --> 00:24:19.430
system that functions as they
do so that when they

429
00:24:19.430 --> 00:24:23.570
are in this wheelchair and
they're navigating, if something happens,

430
00:24:23.840 --> 00:24:25.940
the robot can take over
and it's like, Oh wait,

431
00:24:25.940 --> 00:24:27.530
hold on. This is a
little bit different than what

432
00:24:27.530 --> 00:24:31.700
they're normally doing. There must
be something that's wrong or,

433
00:24:31.730 --> 00:24:35.530
you know, maybe they're tired
and things like that. So,

434
00:24:35.530 --> 00:24:37.420
but when you're saying that
you could make a system

435
00:24:37.420 --> 00:24:39.670
for this individual kind of
calling back to the very

436
00:24:39.670 --> 00:24:43.060
beginning, you can't necessarily do
that for an individual, but

437
00:24:43.060 --> 00:24:45.550
maybe you could build a
fuzzy logic control system that

438
00:24:45.550 --> 00:24:48.800
can take recorded input and
Right. And, and it, and

439
00:24:48.800 --> 00:24:51.100
it learns for that person.
So if you think about

440
00:24:51.100 --> 00:24:55.870
the software, but the underlying
program is the same. What's

441
00:24:55.870 --> 00:24:59.260
different is that when you
put it on a personalized

442
00:24:59.260 --> 00:25:03.400
system, it learns a different
behavior. And so my robot

443
00:25:03.460 --> 00:25:05.590
is going to be different
than your robot because my

444
00:25:05.590 --> 00:25:10.210
behaviors are different, but the
software is the same. Interesting.

445
00:25:10.210 --> 00:25:12.730
And that is that kind
of idea that, that, that

446
00:25:12.790 --> 00:25:15.730
Holy grail of the intelligent
agent, where you've had a

447
00:25:15.730 --> 00:25:18.610
computer for some number of
years, and you have, you

448
00:25:18.610 --> 00:25:20.680
know, maybe, maybe not a
relationship with it, but a

449
00:25:20.680 --> 00:25:23.830
definite enough affinity for it.
You have an affinity, right?

450
00:25:24.250 --> 00:25:27.340
You have a relationship though.
I mean, they've done studies

451
00:25:27.340 --> 00:25:29.590
to show that. I mean,
just like you have your

452
00:25:29.590 --> 00:25:32.740
car and a lot of
people are just so upset

453
00:25:32.740 --> 00:25:34.420
when they have to get
rid of their very first

454
00:25:34.420 --> 00:25:36.730
car that they ever had.
But even if it was

455
00:25:36.730 --> 00:25:40.660
a junky car, you know,
you do establish relationships with

456
00:25:40.660 --> 00:25:43.690
technology and having a robot
is going to be no

457
00:25:43.690 --> 00:25:46.630
different. So you just kind
of blew my mind there

458
00:25:46.630 --> 00:25:49.330
for a second because you're
absolutely right. If you're, you've

459
00:25:49.330 --> 00:25:52.690
got your manual very first,
my $300 first manual shift

460
00:25:52.690 --> 00:25:54.970
car, it had so many
problems, you know, but I

461
00:25:54.970 --> 00:25:57.400
knew what I needed to
do. So that was my

462
00:25:57.400 --> 00:25:59.470
robot. I had a partnership
with that robot. I knew

463
00:25:59.470 --> 00:26:01.420
how to shift it and
when to downshift and how

464
00:26:01.420 --> 00:26:02.620
to move it, to get
it, to do what I

465
00:26:02.620 --> 00:26:05.910
wanted it to do. Exactly.
Exactly. It was, it was,

466
00:26:06.050 --> 00:26:10.720
it was beautiful in its,
in, in its imperfection, But

467
00:26:10.720 --> 00:26:15.760
you accepted its imperfections. I
think that's the trick. People

468
00:26:16.060 --> 00:26:19.690
have such high expectations of
technology now where if something

469
00:26:19.690 --> 00:26:21.940
doesn't work or hiccups, they
just kind of declare it,

470
00:26:21.970 --> 00:26:25.330
Oh, this sucks. Now that
pension zoom wasn't recognized, you

471
00:26:25.330 --> 00:26:27.430
know, so there's some things
we will not tolerate, like

472
00:26:27.700 --> 00:26:31.270
lack of reaction to input
is unacceptable from a user

473
00:26:31.270 --> 00:26:34.540
interface perspective. Correct. So there
are certain, so there are

474
00:26:34.540 --> 00:26:37.420
some things we will accept
and there's certain things we

475
00:26:37.420 --> 00:26:40.720
won't accept as an example.
I would love it if,

476
00:26:41.110 --> 00:26:44.880
when I go into my
computer and get that little,

477
00:26:45.870 --> 00:26:49.650
little icon that says I'm
working, could you just tell

478
00:26:49.650 --> 00:26:52.440
me why it could just
say, you know, my, my

479
00:26:52.830 --> 00:26:56.880
Ram is a little bit
imploded or, you know, Oh,

480
00:26:56.910 --> 00:26:59.460
guess what you're also doing,
or we're doing a backup

481
00:26:59.460 --> 00:27:01.620
in the background that you
didn't know about. And that's

482
00:27:01.620 --> 00:27:04.200
why I would love it.
I would not be as

483
00:27:04.200 --> 00:27:08.310
frustrated if you actually told
me the reason why I'm

484
00:27:08.340 --> 00:27:10.890
having to wait here. And
so I think what's unique

485
00:27:10.890 --> 00:27:14.120
about robotic systems is that
they can do that. They

486
00:27:14.130 --> 00:27:17.250
can say, look, I wasn't
trained to do this, you

487
00:27:17.250 --> 00:27:19.710
know? Or, you know, I'm
sorry, I'm about to make

488
00:27:19.710 --> 00:27:23.220
a mistake because you know,
I'm assuming a, B, C,

489
00:27:23.230 --> 00:27:26.100
or D, or even just
sitting, I'm sorry. I think

490
00:27:26.550 --> 00:27:29.910
those are the things that
will make robots more acceptable

491
00:27:29.910 --> 00:27:33.390
than say, you know, my,
my system that just freezes

492
00:27:33.390 --> 00:27:37.750
for absolutely no reason, at
least I can see, It's

493
00:27:37.820 --> 00:27:41.180
funny. I feel like we're
always on the cusp. Like

494
00:27:41.210 --> 00:27:43.340
I can, I can see
this robot that I don't

495
00:27:43.340 --> 00:27:46.340
have yet. This one that
doesn't exist because I can,

496
00:27:46.370 --> 00:27:49.580
I can hear him or
her in Siri, occasionally, like

497
00:27:49.580 --> 00:27:52.070
he's in there, but he
has no body. And then

498
00:27:52.070 --> 00:27:53.750
Siri will do something stupid.
I'm like, Oh, I don't

499
00:27:53.750 --> 00:27:56.630
want a stupid robot. And
I can, I can see

500
00:27:56.630 --> 00:27:58.640
him as I, you know,
move my, my kids are

501
00:27:58.640 --> 00:28:00.650
doing all robots around, but
you know, but they're not

502
00:28:00.650 --> 00:28:04.970
attached to Siri. Have you
seen the trailer for the

503
00:28:04.970 --> 00:28:10.010
movie chappy yet? Yes I
have. Yes. Yes. The return

504
00:28:10.010 --> 00:28:14.600
of Johnny five from a
short circuit. Yeah. Johnny five

505
00:28:14.600 --> 00:28:21.140
was an interesting, yes. I
think that, that's what we

506
00:28:21.140 --> 00:28:23.570
want though. We want that,
that, that, that friend, that,

507
00:28:23.630 --> 00:28:26.990
that companion for the kids,
the babysitter, the Butler That

508
00:28:26.990 --> 00:28:34.160
learns that is a partner,
social partner. That's exciting. That's

509
00:28:34.160 --> 00:28:37.880
engaging that caters to your
every, or at least tries

510
00:28:37.880 --> 00:28:42.530
to your ever every need.
I think we do want

511
00:28:42.530 --> 00:28:45.020
that. And I truly believe
we will get to that

512
00:28:45.020 --> 00:28:48.110
point, but it will be
very specialized. It's one of

513
00:28:48.110 --> 00:28:50.990
those things we say is
that every time we get

514
00:28:50.990 --> 00:28:54.500
closer, the definition changes as
an example. If you think

515
00:28:54.500 --> 00:28:57.290
about your phone and Siri,
you know, if I gave

516
00:28:57.290 --> 00:29:00.710
that to you say, 50
years ago, and I said,

517
00:29:01.280 --> 00:29:03.590
this is what the future
looks like. People would be

518
00:29:03.590 --> 00:29:05.540
like, Oh my gosh, I
can talk to it. And

519
00:29:05.540 --> 00:29:09.740
it does this and that,
all this is amazing. This

520
00:29:09.740 --> 00:29:13.580
is intelligent. And now we're
like, Oh, it's a program.

521
00:29:14.060 --> 00:29:17.960
So our definition of, you
know, what's grand and what's

522
00:29:17.960 --> 00:29:21.770
artificial intelligence and what's robotics
changes as we get closer

523
00:29:21.770 --> 00:29:25.160
to the goal. And so
in one aspect, it's this

524
00:29:25.430 --> 00:29:27.590
until we get a choppy,
but even when we get

525
00:29:27.590 --> 00:29:30.280
a chappy, I think it's
going to be a, well,

526
00:29:30.290 --> 00:29:33.080
it's a little stupid. It
has to learn why can't

527
00:29:33.080 --> 00:29:36.680
I just press the button?
And it just knows. I

528
00:29:36.680 --> 00:29:39.920
think we're just always gonna
redefine what it is that

529
00:29:39.920 --> 00:29:43.180
we want. It's like Louis
C K says, Everything's amazing.

530
00:29:43.180 --> 00:29:47.910
And nobody's happy. Yes, exactly.
Now I know that you

531
00:29:47.910 --> 00:29:50.880
have a passion for, for
STEM and for getting kids

532
00:29:50.880 --> 00:29:55.110
involved in, in technology. I've
got little boys, a seven

533
00:29:55.110 --> 00:29:56.700
year old and a nine
year old. And when I

534
00:29:56.700 --> 00:29:59.940
introduced robots and, and by
robots, I mean, you know,

535
00:29:59.940 --> 00:30:03.960
Arduinos and motors, the first
thing that they said is,

536
00:30:03.990 --> 00:30:05.970
Oh, we're going to make
a robot that will go

537
00:30:05.970 --> 00:30:09.300
and get mommy, a soda
pop and bring it to

538
00:30:09.300 --> 00:30:11.430
her. And I just, I
we've been working on this

539
00:30:11.430 --> 00:30:12.780
for a couple of years
now, and I just don't

540
00:30:12.780 --> 00:30:15.270
have the, the heart to
explain to them that there's

541
00:30:15.270 --> 00:30:20.520
so much involved With getting
to that point. Just, just

542
00:30:20.580 --> 00:30:22.500
go over to the fridge.
What's the fridge, you know,

543
00:30:22.500 --> 00:30:26.760
like this, we've got it,
basically. So if it starts

544
00:30:26.760 --> 00:30:29.910
at a specific place, it
can make it there. But

545
00:30:29.910 --> 00:30:32.880
if anything goes wrong on
the way, it doesn't make

546
00:30:32.880 --> 00:30:36.900
it. How do you, how
do you get kids who

547
00:30:36.900 --> 00:30:41.370
are excited about STEM, excited
about working towards this thing

548
00:30:41.370 --> 00:30:43.680
that may never happen. We
may ask them tonically approach

549
00:30:44.280 --> 00:30:48.720
and never reached chappy without
discouraging them. We'll see. I

550
00:30:48.720 --> 00:30:52.710
think there's a difference between
what you can do at

551
00:30:52.710 --> 00:30:55.920
a certain age and then
what's out there. So as

552
00:30:55.920 --> 00:30:58.950
an example, there are robots
now that I can start

553
00:30:58.950 --> 00:31:03.300
at at any arbitrary location
and say, go get, and

554
00:31:03.300 --> 00:31:07.020
it comes back. Now the
cost price is not something

555
00:31:07.020 --> 00:31:11.490
that I can buy, but
it does exist. There's robots

556
00:31:11.490 --> 00:31:14.280
out there that, I mean,
even not just in the

557
00:31:14.280 --> 00:31:19.650
U S but in Japan
and overseas, that can grab

558
00:31:19.680 --> 00:31:24.120
things and open them and
pop a bottle. And you

559
00:31:24.120 --> 00:31:26.880
can have it fee people.
I mean, these things do

560
00:31:26.880 --> 00:31:31.290
exist again. The price point
is quite, quite a lot.

561
00:31:32.220 --> 00:31:35.070
I mean, even if you
think of something as people

562
00:31:35.070 --> 00:31:38.490
might think of, as, as
simple as Roomba the vacuum

563
00:31:38.490 --> 00:31:41.100
cleaner and, and think about
it, I have a vacuum

564
00:31:41.100 --> 00:31:43.230
cleaner. I can put it
in the middle of the

565
00:31:43.230 --> 00:31:47.100
floor are not, or I
put in the corner and

566
00:31:47.100 --> 00:31:49.410
guess what? It does one
thing. And it doesn't very

567
00:31:49.410 --> 00:31:52.230
well, but it's a vacuum
cleaner, but do you know

568
00:31:52.230 --> 00:31:54.570
what goes into that to
make it so that it's

569
00:31:54.570 --> 00:31:59.370
robust and it actually vacuums,
and it doesn't break down

570
00:31:59.370 --> 00:32:00.810
in the middle of the
floor and say, I don't

571
00:32:00.810 --> 00:32:02.640
know what this cat is.
I'm just going to do

572
00:32:02.640 --> 00:32:05.730
nothing. Just sit here. I
mean, there's a lot that

573
00:32:05.730 --> 00:32:09.720
went into that. And so
I think with kids, it's

574
00:32:10.110 --> 00:32:14.210
giving them the skills, whether
it's the raspberry PI or

575
00:32:14.220 --> 00:32:17.940
their Draino to build, but
then also showing them what's

576
00:32:17.940 --> 00:32:20.130
nice about the internet is
that all of this stuff

577
00:32:20.130 --> 00:32:23.250
is online. It's on videos.
People are giving talks about

578
00:32:23.250 --> 00:32:26.130
it, but show them as
like, look, you keep with

579
00:32:26.160 --> 00:32:29.610
this and you can create,
you know, the pepper, which

580
00:32:29.610 --> 00:32:32.760
is the new platform that's
coming out. Social robotics are,

581
00:32:32.760 --> 00:32:36.030
you can then create, you
know, the next PR too,

582
00:32:36.030 --> 00:32:40.190
which is a robot that
they were using for individuals

583
00:32:40.190 --> 00:32:42.560
are, you can create. And
so I think is showing

584
00:32:42.560 --> 00:32:45.440
them both worlds. It's saying,
you know, this is what

585
00:32:45.440 --> 00:32:49.460
you can do so that
you are prepared to build

586
00:32:49.460 --> 00:32:54.160
these great, wonderful things in
the future. The I'll put

587
00:32:54.160 --> 00:32:56.170
links to all of the
stuff that we've mentioned in

588
00:32:56.170 --> 00:32:59.080
the show notes here. I'm
looking at at pepper here,

589
00:32:59.410 --> 00:33:03.250
pepper is a little small
person, little, maybe three and

590
00:33:03.250 --> 00:33:06.760
a half foot tall, but
it's not just a, it's

591
00:33:06.760 --> 00:33:09.790
not just a Dalek. She
seems to have a waist

592
00:33:09.790 --> 00:33:12.790
and she can turn and
move in very fluid and

593
00:33:12.790 --> 00:33:17.290
very natural ways without necessarily
going into the uncanny Valley.

594
00:33:17.680 --> 00:33:19.630
Right. And if you look
at the price point, which

595
00:33:19.630 --> 00:33:22.630
is a good link, it's
not what I would consider

596
00:33:22.630 --> 00:33:25.930
that expensive. How much is
pepper? I'm, I'm, I've, I'm

597
00:33:25.930 --> 00:33:28.090
looking at pictures. I haven't
got to the buy now.

598
00:33:28.120 --> 00:33:30.280
Cause sometimes I find things
like this and I'm like,

599
00:33:30.280 --> 00:33:33.370
where's the shut up and
take my money. You see,

600
00:33:33.380 --> 00:33:36.790
unfortunately, they are not selling
it in the U S

601
00:33:36.820 --> 00:33:40.750
but if you go overseas,
you're good to go. I

602
00:33:40.750 --> 00:33:44.350
think I'd last point I
saw was about 1900. I

603
00:33:44.350 --> 00:33:48.850
don't know if that was
European dollars or U S

604
00:33:49.260 --> 00:33:52.330
It's not the $20,000. No,
that's what I'm saying. That's

605
00:33:52.690 --> 00:33:57.730
that's reasonable. That's, you know,
times four I-phones I mean,

606
00:33:58.180 --> 00:34:01.570
of in the reasonable domain.
I think one of the

607
00:34:01.570 --> 00:34:04.000
things that I think is
interesting about our conversation here

608
00:34:04.000 --> 00:34:08.290
is that you, you keep
bringing it back to kind

609
00:34:08.290 --> 00:34:10.900
of reality, but I keep
going to movies. I feel

610
00:34:10.900 --> 00:34:13.750
like I've already recommended movies
and books to you. I

611
00:34:13.750 --> 00:34:15.550
think it just shows the
different worlds that we live

612
00:34:15.550 --> 00:34:18.520
in. You know, someone with
actual robotics experience versus someone

613
00:34:18.520 --> 00:34:22.000
whose entire perspective of robotics
is filtered through pop culture.

614
00:34:24.490 --> 00:34:26.740
Well, but you know what?
I actually watched all of

615
00:34:26.740 --> 00:34:30.640
these movies as well. Oh
no. In fact, I love

616
00:34:30.640 --> 00:34:35.530
them because honestly, when you
watch, I don't read as

617
00:34:35.530 --> 00:34:37.870
much as I should, but
definitely with the TV shows

618
00:34:37.870 --> 00:34:41.080
and the movies, there's always
some truth in it. There's

619
00:34:41.080 --> 00:34:43.360
always some shoes. In fact,
you look at it. You

620
00:34:43.360 --> 00:34:47.260
know, one of my favorite
is surrogate with, with Bruce Willis.

621
00:34:48.310 --> 00:34:50.410
I look at that, I'm
like, Oh my gosh, if

622
00:34:50.410 --> 00:34:54.490
you only knew how close
we are to that, it's

623
00:34:54.490 --> 00:34:57.580
kind of scary, but it's,
it's you see it. And

624
00:34:57.580 --> 00:35:00.700
it's like, I know someone
who's working on that part

625
00:35:00.700 --> 00:35:03.820
and that part and that
part. And when they get

626
00:35:03.820 --> 00:35:05.950
it to the point where
it can be integrated together,

627
00:35:06.280 --> 00:35:09.280
where there, and so I
look at these movies because

628
00:35:09.310 --> 00:35:11.410
I look at it, I'm
like, Oh yeah, that exists.

629
00:35:11.600 --> 00:35:13.900
It's those working on that?
Oh yeah. That exists. So

630
00:35:13.900 --> 00:35:16.420
it's working on that. That's
amazing. And you actually know

631
00:35:16.420 --> 00:35:18.110
them. You're like, you'll be
watching a movie. Just say,

632
00:35:18.120 --> 00:35:19.690
you know, I'm going to
call Jeff and tell him

633
00:35:21.730 --> 00:35:25.810
Yes, That's cool. Yeah. The,
the surrogate to there's a

634
00:35:25.810 --> 00:35:29.410
great book by John Scalzi
called lock-in, which extends that

635
00:35:29.410 --> 00:35:32.650
concept of surrogate even even
further. And the movie that

636
00:35:32.650 --> 00:35:35.710
I was just thinking about
is robot and Frank. Oh,

637
00:35:35.710 --> 00:35:38.310
that's a beautiful movie. Isn't
it just a wonderful, and

638
00:35:38.310 --> 00:35:40.470
they actually had a ballerina
in the robot to get

639
00:35:40.470 --> 00:35:42.300
it, to move the way
they wanted it to move.

640
00:35:42.300 --> 00:35:45.720
A little tiny ballerina was
in there that I did

641
00:35:45.720 --> 00:35:49.110
not know she was about
four foot six or something

642
00:35:49.110 --> 00:35:52.710
like that. They had two
different dancers in there because

643
00:35:52.710 --> 00:35:56.250
they wanted the robot to
move human, but robotically human.

644
00:35:56.750 --> 00:36:00.020
Interesting. I did not know
this. That's a wonderful, wonderful

645
00:36:00.020 --> 00:36:01.970
movie. I think we'll, we'll
end on that. And I

646
00:36:01.970 --> 00:36:04.700
want to encourage people to
check out what you're doing

647
00:36:04.700 --> 00:36:07.670
is I robotics and they
can certainly go on Google

648
00:36:07.670 --> 00:36:09.950
and search for dr. Anna Howard
and read all about some

649
00:36:09.950 --> 00:36:12.440
of the amazing stuff that
you've done and the products

650
00:36:12.440 --> 00:36:14.390
that you guys have at
Sarah robotics. Is there anything

651
00:36:14.390 --> 00:36:16.670
else I should be sure
to point people towards? No.

652
00:36:16.670 --> 00:36:20.000
And you know, definitely if
you have kids encourage them

653
00:36:20.000 --> 00:36:23.270
and STEM, we still need
it is really important, whether

654
00:36:23.270 --> 00:36:27.170
it was computer science or
engineering or math, I think

655
00:36:27.170 --> 00:36:29.510
is one of the things
that we will be in

656
00:36:29.510 --> 00:36:33.200
trouble if we don't produce
more engineers and scientists. So

657
00:36:33.200 --> 00:36:35.060
there's still room at the
top. We should encourage the

658
00:36:35.060 --> 00:36:38.630
kids to go there. Yes,
definitely. Fantastic. Thank you so

659
00:36:38.630 --> 00:36:42.140
much, dr. Anna Howard, this has
been another episode of Hanselminutes

660
00:36:42.200 --> 00:36:43.550
and we'll see you again
next week.

