WEBVTT FILE

1
00:00:00.180 --> 00:00:03.240
This episode is brought to
you by Datadog a cloud

2
00:00:03.240 --> 00:00:08.910
scale monitoring platform built by
engineers for engineers, Datadog, unifies

3
00:00:08.910 --> 00:00:12.840
metrics, logs, traces, and APM
features in a single intuitive

4
00:00:12.840 --> 00:00:17.460
interface. Allowing your engineers to
quickly troubleshoot issues and collaborate

5
00:00:17.460 --> 00:00:21.990
across departments and time zones.
Plus machine learning based alerts

6
00:00:22.110 --> 00:00:27.540
auto-detect application performance, anomalies, and
forecast future performance. To begin

7
00:00:27.540 --> 00:00:30.780
monitoring your environment. Start a
free trial of Datadog by

8
00:00:30.810 --> 00:00:38.550
visiting bitly.com/data dog shirt, and
receive a complimentary t-shirt that's

9
00:00:38.550 --> 00:00:58.640
bitly.com/datadog shirt. Hi, this is
Scott Hanselman, Hansel minutes. And

10
00:00:58.640 --> 00:01:01.670
today I'm talking with dr.
Anna Howard. I spoke to you

11
00:01:01.670 --> 00:01:07.790
in episode 467, and we're
getting towards 700 now that

12
00:01:07.790 --> 00:01:11.810
was like forever ago. It
was forever goes 2015, so

13
00:01:11.810 --> 00:01:14.780
four years ago. And we
talked a little bit about

14
00:01:15.050 --> 00:01:17.780
some of the work you'd
done at NASA and JPL

15
00:01:17.780 --> 00:01:19.370
and the Mars rovers. And
I think you were just

16
00:01:19.370 --> 00:01:22.160
getting started with XY robotics
at the time. Is that

17
00:01:22.160 --> 00:01:26.720
correct? Yeah. So let's see.
2015, the company had been

18
00:01:26.720 --> 00:01:30.380
in existence for two years,
but we had just started

19
00:01:30.380 --> 00:01:34.370
to release our first set
of products in 2015. So

20
00:01:34.370 --> 00:01:37.130
that was still a who's
the market who's going to

21
00:01:37.130 --> 00:01:40.730
actually purchase these things. First
customer kind of things. And

22
00:01:41.120 --> 00:01:43.820
you know, at first customer
is also the first customer.

23
00:01:43.820 --> 00:01:46.610
Blue's trying to figure out
the service and things like

24
00:01:46.610 --> 00:01:49.880
that. And if you have
to pivot or exactly, do

25
00:01:49.880 --> 00:01:52.610
you have to pivot, is
it really addressing the need?

26
00:01:52.940 --> 00:01:57.020
What's the pricing and all
of those good things, and

27
00:01:57.020 --> 00:02:00.710
you are the CTO at
XY robotics. But then when

28
00:02:00.710 --> 00:02:02.300
I, when I say, well,
wait a second, are you

29
00:02:02.300 --> 00:02:05.390
still teaching? How many jobs
do you have? I actually

30
00:02:05.390 --> 00:02:08.960
have three jobs. So I'm
a CTO. Ads are robotics.

31
00:02:08.960 --> 00:02:12.860
I'm also a professor at
Georgia tech. So I teach,

32
00:02:12.860 --> 00:02:16.250
I have students and also
I run a school of

33
00:02:16.250 --> 00:02:18.350
interactive computing. So I'm a
chair. So I have an

34
00:02:18.400 --> 00:02:21.950
administrative hat as well with,
I don't know, I'm thinking

35
00:02:21.950 --> 00:02:27.200
with staff and faculty over
50 employees. So yeah. Do

36
00:02:27.200 --> 00:02:29.690
you have some hours in
the day that I do

37
00:02:29.690 --> 00:02:33.170
not have? I don't know.
Cause I still exercise. I

38
00:02:33.170 --> 00:02:37.010
still teach Zumba. I have
kids that I go to

39
00:02:37.010 --> 00:02:46.820
as well showing off. Yeah,
that's fantastic. Yeah. And you've

40
00:02:46.820 --> 00:02:48.500
been, and then here's the
thing though. You've been doing

41
00:02:48.500 --> 00:02:51.710
this for, with all due
respect for a long time.

42
00:02:51.710 --> 00:02:55.310
You were at NASA starting
in 1993. You worked at

43
00:02:55.850 --> 00:02:58.880
a, I mean the jet
propulsion laboratory. This has been

44
00:02:58.880 --> 00:03:02.680
a amazing career where you've
been pivoting and doing multiple

45
00:03:02.680 --> 00:03:06.960
things at once. Yeah. And
I think it's because I

46
00:03:07.410 --> 00:03:12.060
go to things that I
find exciting and interesting, and

47
00:03:12.060 --> 00:03:14.760
then it allows me to
really be passionate about it,

48
00:03:14.940 --> 00:03:16.740
which means that I can
do, like, even though I'm

49
00:03:16.740 --> 00:03:19.290
saying I have three, three
and a half jobs, like

50
00:03:19.290 --> 00:03:22.350
I don't actually feel like
it because I enjoy doing

51
00:03:22.350 --> 00:03:23.940
it. And it's just, I
make it part of my

52
00:03:23.940 --> 00:03:27.240
life and it's not something
that's separate like, Oh, I

53
00:03:27.240 --> 00:03:29.220
gotta go to work. Oh,
I gotta do this. It's

54
00:03:29.220 --> 00:03:32.160
like a, Oh, let's think
of new ideas what's going

55
00:03:32.160 --> 00:03:35.970
on. And when I get
bored, I do switch. Hmm.

56
00:03:36.000 --> 00:03:37.860
That's a really good point.
I mean, ultimately if it

57
00:03:37.860 --> 00:03:39.900
was three jobs where it's
like, Oh, I got to

58
00:03:39.900 --> 00:03:43.090
wake up this morning and
do this whole thing. Right.

59
00:03:43.120 --> 00:03:45.720
I'm going to go to
make robots and make kids

60
00:03:45.720 --> 00:03:51.960
happy and successful. Yeah. No,
it's, it's actually enjoyable. And

61
00:03:51.960 --> 00:03:56.220
then you get that internal
motivation too, when you see

62
00:03:56.460 --> 00:03:59.910
like with kids like them
learning and their, their eyes

63
00:03:59.910 --> 00:04:02.490
lighting up and you're like,
okay, yeah. This is like

64
00:04:02.490 --> 00:04:06.960
really satisfied. Now how's everything
been robots for you? Cause

65
00:04:06.960 --> 00:04:09.180
I know that you're the
chair of the multidisciplinary robotics

66
00:04:09.210 --> 00:04:12.390
PhD program at Georgia tech.
And a lot of things

67
00:04:12.390 --> 00:04:14.820
keep coming back to robots,
but you, you do have

68
00:04:14.820 --> 00:04:17.610
experience in other areas. You
do, although I'm no longer

69
00:04:17.610 --> 00:04:21.060
chair of the PhD program.
Cause that would have been

70
00:04:21.060 --> 00:04:25.860
like another five or six
jobs. So I actually, I,

71
00:04:26.220 --> 00:04:29.760
yeah, it's been a lot.
So in four years I

72
00:04:29.760 --> 00:04:34.200
was the PhD for the
robotics. And then I transitioned

73
00:04:34.200 --> 00:04:37.320
to be the associate director
for research for robotics Institute.

74
00:04:37.860 --> 00:04:41.910
And then I transitioned to
associate chair for faculty development

75
00:04:41.910 --> 00:04:46.500
and now chair for school
of interactive computing, but kind

76
00:04:46.500 --> 00:04:49.920
of the theme is been
engineering, computer science. My research

77
00:04:49.920 --> 00:04:55.260
has been robotics. It's now
strongly in the AI space,

78
00:04:55.410 --> 00:05:00.390
but still the embodiment with
robotics embodied AI. Are you

79
00:05:00.390 --> 00:05:03.090
trying to find the intersection
of where AI begins and

80
00:05:03.090 --> 00:05:04.710
like, are you trying to
build them bodies so that

81
00:05:04.710 --> 00:05:07.350
they can come and defeat
us all? No, not defeat

82
00:05:07.350 --> 00:05:09.990
us all, help us all
improve our quality of life.

83
00:05:11.490 --> 00:05:14.460
So one of the things
about like robotics in the

84
00:05:14.460 --> 00:05:18.000
past was that the AI
wasn't advanced enough. I mean,

85
00:05:18.030 --> 00:05:20.730
you know, AI was search
and like search and robotics.

86
00:05:20.730 --> 00:05:24.720
It just didn't meld. But
now like the things that

87
00:05:24.720 --> 00:05:27.240
are going on in the
AI world made sense for

88
00:05:27.240 --> 00:05:30.810
robotics. And so that's why,
you know, interacting with people

89
00:05:30.810 --> 00:05:34.500
and adapting to them and
learning about their preferences. Like

90
00:05:34.530 --> 00:05:38.160
that's part of AI emotion
recognition. That's part of AI

91
00:05:38.190 --> 00:05:41.180
now it's part of robotics.
Yeah. Interesting. So when, when

92
00:05:41.180 --> 00:05:43.740
I think about AI, I
think about, you know, human

93
00:05:43.740 --> 00:05:45.960
computer interactions and I've had
a number of people on

94
00:05:45.960 --> 00:05:49.980
the show talking about, you
know, suggestions in an action

95
00:05:49.980 --> 00:05:53.250
that a robot can take.
Do robots also use AI

96
00:05:53.280 --> 00:05:55.730
and how they decide to
move? Or are their movements

97
00:05:55.730 --> 00:06:01.220
still procedurally written? Well, no.
So their, their movements themselves

98
00:06:01.250 --> 00:06:06.230
might be procedural algorithms, but
which movement should they select?

99
00:06:06.620 --> 00:06:11.840
Is this intelligence, right? So
for example, if I recognize

100
00:06:11.840 --> 00:06:16.790
that I'm a child that
I'm interacting with is happy,

101
00:06:17.240 --> 00:06:21.740
right? So what is my
corresponding responsible is to be,

102
00:06:21.740 --> 00:06:24.140
it's probably supposed to be
happy or encouraging cause they

103
00:06:24.140 --> 00:06:27.500
did something good. Right? And
so that learning process is

104
00:06:27.500 --> 00:06:32.810
becomes, okay, what's the happy
behavior versus a sad behavior

105
00:06:32.840 --> 00:06:36.680
versus a frustrated behavior. And
so when I get at

106
00:06:36.680 --> 00:06:39.290
a happy, it may be
like I have five ways

107
00:06:39.290 --> 00:06:41.630
to express happiness. And so
I have to think about,

108
00:06:41.630 --> 00:06:46.060
well, what's the most optimal
way to express happiness. Ah,

109
00:06:46.090 --> 00:06:50.950
okay. Is there any concern
about applying AI to movement?

110
00:06:50.950 --> 00:06:54.370
For example, I could say
have an AI decide whether

111
00:06:54.370 --> 00:06:56.410
or not to go over
to the child, which would

112
00:06:56.410 --> 00:06:59.470
be the decision point. Well
then the movement would be

113
00:06:59.470 --> 00:07:02.800
procedural or I could say
to the robot, you have

114
00:07:03.310 --> 00:07:07.180
these ways to move, figure
it out. And the robot

115
00:07:07.180 --> 00:07:09.520
could like, we see robots
that have learning to walk

116
00:07:09.520 --> 00:07:11.380
where we didn't actually teach
the robot how to move

117
00:07:11.380 --> 00:07:14.590
its legs. It just trial
and error until it finally

118
00:07:14.590 --> 00:07:17.050
figured it out. Is there
any concerns about applying AI

119
00:07:17.050 --> 00:07:20.290
to the physicality of a
robot? No. So some of

120
00:07:20.290 --> 00:07:24.190
the concerns also feed into
the AI space. So this

121
00:07:24.190 --> 00:07:27.460
aspect of safety or user
safety, right? We worry about

122
00:07:27.460 --> 00:07:31.150
that with, with AI systems
that are running things. We

123
00:07:31.150 --> 00:07:34.660
worry about that with robotic
systems, like how do you

124
00:07:34.720 --> 00:07:39.700
have a robot safely figure
out something? And the problem

125
00:07:39.700 --> 00:07:42.880
is, is that there's now
an actuation, right? So if

126
00:07:42.880 --> 00:07:45.070
I teach a robot, I
want the robot to learn

127
00:07:45.070 --> 00:07:47.950
how to drive, right? The
robot may learn how to

128
00:07:47.950 --> 00:07:51.520
drive on sidewalks. Like that's
something we probably don't want

129
00:07:51.520 --> 00:07:55.570
to do. And so thinking
about the rules that you

130
00:07:55.570 --> 00:07:57.760
have to put into the
system so that it can

131
00:07:57.760 --> 00:08:02.740
learn freely, but it also
has some bounded constraints. I

132
00:08:02.740 --> 00:08:06.040
see. Interesting. Cause I, I'm
not sure if we talked

133
00:08:06.040 --> 00:08:09.220
about this four years ago,
but I'm currently on an

134
00:08:09.250 --> 00:08:13.570
open source artificial pancreas system.
So I have a DIY

135
00:08:13.600 --> 00:08:18.910
closed loop, insulin pump and
sensor system. So this is

136
00:08:18.910 --> 00:08:22.390
a system by which my
blood sugar is read off

137
00:08:22.390 --> 00:08:25.840
of a sensor. It's interpreted
the decision is made for

138
00:08:25.840 --> 00:08:29.050
me much like a self
driving car except for diabetics.

139
00:08:29.110 --> 00:08:31.330
And then the actuation in
this case would be the

140
00:08:31.330 --> 00:08:34.930
delivery of insulin through the
insulin pump and that's ongoing

141
00:08:34.930 --> 00:08:38.260
and, and, and, and happening.
But they're also pretty significant

142
00:08:38.260 --> 00:08:41.290
constraints. Even if the system
were to fly wildly out

143
00:08:41.290 --> 00:08:44.170
of control, I have to
think about what's the worst

144
00:08:44.200 --> 00:08:46.500
possible thing that could happen.
That would affect my, my

145
00:08:46.700 --> 00:08:48.970
health in an, in a
negative way. Right. And so

146
00:08:48.970 --> 00:08:52.480
you think about thresholds for
example. Yeah. That's exactly what

147
00:08:52.480 --> 00:08:54.790
we do. Yeah. Alright. That's
good to know that we're

148
00:08:54.790 --> 00:08:57.780
thinking the right things, because
this is a community of

149
00:08:57.810 --> 00:09:00.780
DIY kind of enthusiasts that
have, that are putting systems

150
00:09:00.780 --> 00:09:03.140
like this together in the
open source space. Yeah. So

151
00:09:03.140 --> 00:09:06.200
I think that's really the
thing is making sure that

152
00:09:06.200 --> 00:09:10.610
there are still constraints. And
are you thinking about this

153
00:09:10.880 --> 00:09:14.090
more so than now that
you are developing a technology

154
00:09:14.090 --> 00:09:16.790
for kids and little kids?
So robotics is doing a

155
00:09:16.790 --> 00:09:19.940
lot of work in the
K you know, kindergarten one

156
00:09:20.000 --> 00:09:23.570
to second grade type. Yeah.
So three to seven, three

157
00:09:23.570 --> 00:09:28.440
to eight. I do think
about it quite often. And

158
00:09:28.490 --> 00:09:32.630
mainly it's also because when
you're working with kids, they

159
00:09:32.630 --> 00:09:35.390
don't necessarily have a baseline
of like what's right. Or

160
00:09:35.390 --> 00:09:37.850
what's wrong yet. Like as
an adults, when we see

161
00:09:37.850 --> 00:09:41.360
an AI system, you know,
doing something wrong, we kind

162
00:09:41.360 --> 00:09:44.570
of get it, but it's
doing something that's incorrect, but

163
00:09:44.570 --> 00:09:48.890
because kids don't necessarily have
a baseline, they may learn

164
00:09:49.340 --> 00:09:54.170
incorrect things. And so it
is very important at that

165
00:09:54.170 --> 00:09:56.630
space and in that environment
to ensure that there are

166
00:09:56.630 --> 00:09:59.240
constraints and that, you know,
the robot has to be,

167
00:09:59.240 --> 00:10:01.640
or the AI system has
to be, you know, pretty

168
00:10:01.640 --> 00:10:04.310
much a hundred percent correct.
And if not, you know,

169
00:10:04.490 --> 00:10:06.830
basically being able to send
a signal back to the

170
00:10:06.830 --> 00:10:10.010
teacher or the parent or
the clinician that, you know,

171
00:10:10.250 --> 00:10:12.470
well, Hey, this is what
the desk guests are. This

172
00:10:12.470 --> 00:10:15.290
is what the norm is,
but there's also some, you

173
00:10:15.290 --> 00:10:18.830
know, One of the things
that you've worked on since

174
00:10:18.830 --> 00:10:21.740
I saw you last was
this idea of Zoomo Z

175
00:10:21.740 --> 00:10:26.450
U M O, and you
can check it out@zumolearning.com. Is

176
00:10:26.450 --> 00:10:29.780
this a robot eye? It
looks like a big plush

177
00:10:29.780 --> 00:10:32.660
turtle. That's quite large. Yes.
It is a plush turtle.

178
00:10:32.690 --> 00:10:35.660
Although we have had people
call it a soft robot,

179
00:10:35.900 --> 00:10:39.170
which is quite interesting. And
so we go with it,

180
00:10:39.470 --> 00:10:42.800
but it's, it's actually, it's
not a, a robot in

181
00:10:42.800 --> 00:10:46.460
terms of the actuation, what
it is is it has

182
00:10:46.460 --> 00:10:49.460
the sensing in it and
it has the intelligence. And

183
00:10:49.460 --> 00:10:52.940
so what it does is
it takes a user's inputs,

184
00:10:53.150 --> 00:10:57.950
which are presses and, and
bangs on the surface of

185
00:10:58.310 --> 00:11:02.210
the push turtle. And it
converts it to signals with

186
00:11:02.210 --> 00:11:04.070
respect to an app for
control of an app. And

187
00:11:04.070 --> 00:11:07.370
so you can have kids
learn things like sequences, you

188
00:11:07.370 --> 00:11:09.800
know, one, two, three, four,
five in the correct order

189
00:11:09.980 --> 00:11:12.200
that matches to an app.
And so it'll take that

190
00:11:12.890 --> 00:11:16.640
and provided as a sensor
input to the different educational

191
00:11:16.640 --> 00:11:19.850
and therapy apps that we
have. It's fascinating to me,

192
00:11:19.850 --> 00:11:22.220
when I look at the
videos of the kids playing

193
00:11:22.220 --> 00:11:25.790
with, Zoomo, how they just
seem completely nonplussed at this,

194
00:11:25.790 --> 00:11:27.830
of course, I've got this
robot and it's, you know,

195
00:11:27.830 --> 00:11:29.990
I've got this plush turtle
and it's talking to the

196
00:11:30.020 --> 00:11:32.450
screen, it's all wireless. There's
all these things that we

197
00:11:32.450 --> 00:11:34.910
take for granted. I mean,
the most exciting thing that

198
00:11:34.910 --> 00:11:37.790
I did growing up in
the late seventies and the

199
00:11:37.790 --> 00:11:42.800
early eighties was a attach
a Sharpie to a turtle

200
00:11:42.800 --> 00:11:44.930
and draw, you know, on
a piece of butcher paper.

201
00:11:44.930 --> 00:11:47.540
And I thought that was
the greatest thing ever, that

202
00:11:47.540 --> 00:11:49.400
took an entire weekend to
figure out. And now we've

203
00:11:49.400 --> 00:11:53.500
got a wireless plus turtle
robot with AI know it's

204
00:11:53.530 --> 00:11:56.950
it's. And I think that's,
what's so magical about it

205
00:11:57.250 --> 00:12:00.700
is that it feels natural.
Like kids play with stuffed

206
00:12:00.700 --> 00:12:04.480
animals all the time. And
they're curious. And so even

207
00:12:04.480 --> 00:12:07.720
when we see how a
kid interacts at the very

208
00:12:07.720 --> 00:12:10.420
beginning with the push turtle,
they like press something and

209
00:12:10.420 --> 00:12:13.600
something happens on the screen.
They very quickly realize that

210
00:12:13.600 --> 00:12:18.220
it's a controller right. Very
quickly. And it is one

211
00:12:18.220 --> 00:12:21.190
thing that engages them because
like our apps, for example,

212
00:12:21.190 --> 00:12:24.970
our math apps now it's,
it's a game. It's not

213
00:12:24.970 --> 00:12:26.950
like, Oh, I'm learning math.
It's like, Oh no, I'm

214
00:12:26.950 --> 00:12:29.290
playing. And you know, we're
in the back of it.

215
00:12:29.290 --> 00:12:31.210
Like, yeah, but you're learning
that, but that's okay. You're

216
00:12:31.210 --> 00:12:33.910
learning coding, but that's okay
because you're having fun doing

217
00:12:33.910 --> 00:12:37.680
it. It's a stealth maneuver.
You snuck STEM into there,

218
00:12:37.800 --> 00:12:40.050
the plush, the plushy that
they were already carrying around

219
00:12:40.050 --> 00:12:43.140
with them. Exactly. And what's
nice is that it really

220
00:12:43.140 --> 00:12:46.320
is about assessability. And so
with the design of Zemo,

221
00:12:46.590 --> 00:12:49.400
as well as the apps,
it addresses children who have,

222
00:12:49.410 --> 00:12:52.470
might have motor disabilities. Cause
the plush is basically a

223
00:12:52.470 --> 00:12:55.620
switch. And so if you
just have a light touch,

224
00:12:55.890 --> 00:12:59.400
it will convert that it
also allows you to focus.

225
00:12:59.400 --> 00:13:02.760
So you can, a parent
can isolate it so that

226
00:13:03.210 --> 00:13:06.540
nothing works except for the
center button. Right. So that

227
00:13:06.540 --> 00:13:09.270
allows a student to actually
focus like, okay, we want

228
00:13:09.270 --> 00:13:12.390
you to start focusing. You
have too much going on.

229
00:13:12.390 --> 00:13:15.360
It's hard for you to
focus. Okay. This allows you

230
00:13:15.360 --> 00:13:18.600
to not focus. I see.
So it can be as

231
00:13:18.600 --> 00:13:20.520
simple or as complex. It
can grow with the child.

232
00:13:20.520 --> 00:13:22.380
Cause it looks like from
the back, I'm guessing it's

233
00:13:22.380 --> 00:13:24.900
got like, you know, up,
down left, right. And then

234
00:13:24.900 --> 00:13:26.940
the center button, it has
a cross on its back

235
00:13:27.360 --> 00:13:29.820
and that can be made
as complicated or as simple.

236
00:13:29.980 --> 00:13:32.610
I see some kids that
are very young, just simply

237
00:13:32.670 --> 00:13:35.820
squeezing it and that's enough
for their game, but someone

238
00:13:35.820 --> 00:13:38.520
else could use it as
a game controller. Correct. So

239
00:13:38.520 --> 00:13:42.030
the whole gamut. And so
it's adaptable as based on

240
00:13:42.030 --> 00:13:43.920
that. And we just say
based on the child's ability,

241
00:13:44.100 --> 00:13:48.480
whatever their ability is, the
system can be used. How

242
00:13:48.480 --> 00:13:50.190
far do you want to
take something like this? Do

243
00:13:50.190 --> 00:13:52.110
you want to go kind
of all Teddy Ruxpin and

244
00:13:52.110 --> 00:13:55.770
have it talking and thinking
and adding features. Yeah. We

245
00:13:55.770 --> 00:13:59.220
would love that. So one
would be having it doesn't

246
00:13:59.220 --> 00:14:04.230
have a sound module, for
example, can yeah. Version. Well,

247
00:14:04.230 --> 00:14:06.720
so it's version one, but
they have some version, two

248
00:14:06.720 --> 00:14:11.340
components. For example, there is
some bi-directional communication. We just

249
00:14:11.340 --> 00:14:14.870
don't take advantage of it,
right? Because of the, so

250
00:14:14.870 --> 00:14:18.150
the intelligence is there. We
just don't have the outputs,

251
00:14:18.150 --> 00:14:21.480
like a speaker in the
system, but it can talk

252
00:14:21.480 --> 00:14:25.980
back and forth. Debugging is
an essential skill use to

253
00:14:25.980 --> 00:14:28.620
maintain a high standard of
code yet. No one really

254
00:14:28.620 --> 00:14:31.410
teaches you how to effectively
do bug. So it ends

255
00:14:31.410 --> 00:14:34.110
up taking a lot of
time. And that's where Oz

256
00:14:34.110 --> 00:14:37.470
code comes in. Oscoda code
is a visual studio extension

257
00:14:37.470 --> 00:14:42.090
for C sharp developers that
gives you debugging superpowers with

258
00:14:42.090 --> 00:14:47.400
a powerful heads up display,
advanced search inside objects, link

259
00:14:47.430 --> 00:14:52.880
query to bugging side-by-side object
comparisons and Oz code leads

260
00:14:52.880 --> 00:14:55.730
you to the root cause
in half the time with

261
00:14:55.730 --> 00:15:00.170
low to no frustration, go
to Oz code that's <inaudible>

262
00:15:00.260 --> 00:15:06.290
dash code.com/magic to check out
their magical features. You can

263
00:15:06.290 --> 00:15:08.990
download the free 14 day
trial and try it out

264
00:15:08.990 --> 00:15:17.780
for yourself. That's <inaudible> hyphen
code.com/magic. What made you go

265
00:15:17.780 --> 00:15:22.190
from something as, as powerful,
as an impressive, as thinking

266
00:15:22.190 --> 00:15:26.330
about robots on Mars and
working with, with, with NASA

267
00:15:26.330 --> 00:15:29.750
and JPL to making a,
a plush turtle. I mean,

268
00:15:29.750 --> 00:15:33.560
I think that they are
both very important, but one

269
00:15:33.560 --> 00:15:35.840
could look at them as
being a pretty dramatic move.

270
00:15:35.840 --> 00:15:38.270
Did you feel like you
kind of like kicked butt

271
00:15:38.300 --> 00:15:41.000
out there in, in the
world and now is ready

272
00:15:41.000 --> 00:15:43.600
to bring that knowledge to
the show? No. So it

273
00:15:43.600 --> 00:15:48.160
was it's, it's, it's a
strange path. So when I

274
00:15:48.160 --> 00:15:50.560
was at NASA, just, you
know, thinking about how do

275
00:15:50.560 --> 00:15:53.080
you make future Rover missions
and how do you make

276
00:15:53.080 --> 00:15:56.080
them more intelligent? When I
came to Georgia tech, when

277
00:15:56.080 --> 00:15:58.570
I first came, I was
still doing science, different robotics.

278
00:15:58.570 --> 00:16:02.170
We were sending rovers robotics
to glaciers, right. Both on

279
00:16:02.200 --> 00:16:04.480
the glacier, on top, as
well as under the water.

280
00:16:05.560 --> 00:16:08.920
So I was still doing
those things. But then what

281
00:16:08.920 --> 00:16:11.530
happened was, you know, after
like one of the last

282
00:16:11.530 --> 00:16:15.940
studies with like the aquanauts
and deploying underwater, I lost

283
00:16:15.940 --> 00:16:19.810
that, like that passion, right.
It was like engineering and

284
00:16:19.810 --> 00:16:22.030
it was like, cool. Cause
people said it was cool,

285
00:16:22.330 --> 00:16:24.040
but it wasn't that like,
I wake up in the

286
00:16:24.040 --> 00:16:25.420
middle of the night, I'm
like, Oh my gosh, I

287
00:16:25.420 --> 00:16:28.900
figured it out. I wasn't
doing that anymore. And so

288
00:16:28.900 --> 00:16:31.660
I had to figure out
what I wanted to do.

289
00:16:31.690 --> 00:16:35.140
How do I reboot? Because
when things get a little

290
00:16:35.140 --> 00:16:40.720
boring, you start filling those
hours. And ever since, even

291
00:16:40.720 --> 00:16:44.680
with NASA days, I'd always
done outreach. I'd always done

292
00:16:44.680 --> 00:16:48.520
education. I'd always done camps,
STEM, camps, robotics, STEM, camps,

293
00:16:48.790 --> 00:16:51.790
like that had been kind
of something I've always done.

294
00:16:52.480 --> 00:16:56.320
And one year at Georgia
tech, I had a camp.

295
00:16:56.500 --> 00:16:58.900
So typical camp, I just
do this. And I had

296
00:16:58.900 --> 00:17:02.680
a young lady who had
a visual impairment and the

297
00:17:02.680 --> 00:17:06.280
stuff wasn't assessable. Like it
was clearly not assessable. And

298
00:17:06.280 --> 00:17:08.950
I thought she was bright
and inquisitive. And we basically,

299
00:17:09.010 --> 00:17:10.930
I had a student sit
next to her and just

300
00:17:10.930 --> 00:17:14.080
kind of go through things.
And I, as I sat

301
00:17:14.080 --> 00:17:15.760
there, I was thinking, you
know, this is a shame,

302
00:17:15.760 --> 00:17:19.210
like, how is it that
we, as scientists are basically

303
00:17:19.210 --> 00:17:23.950
cutting off the world for
some, an individual when they

304
00:17:23.980 --> 00:17:27.190
clearly could do it, but
we've made the world inaccessible.

305
00:17:28.540 --> 00:17:30.070
And so I started doing
that, but it was more

306
00:17:30.070 --> 00:17:32.140
of a hobby, right. It
was like, I had like

307
00:17:32.200 --> 00:17:35.710
undergrads working on this. It
still wasn't the main research.

308
00:17:36.250 --> 00:17:39.370
But what happened was I
started getting more involved in

309
00:17:39.370 --> 00:17:41.950
that and saying, well, Hey,
now we can bring in

310
00:17:41.950 --> 00:17:46.060
robotics, like hardcore robotics as
well as education. Oh. I'm

311
00:17:46.060 --> 00:17:48.820
like, I can do therapy
and work with clinicians. And

312
00:17:48.820 --> 00:17:53.430
so just started becoming much
more interesting. And so the

313
00:17:53.430 --> 00:17:55.980
switch that a lot of
people see, isn't like a

314
00:17:55.980 --> 00:17:58.740
drastic switch. It was that
if you think about the

315
00:17:58.740 --> 00:18:02.220
train tracks, like I was
doing both and at some

316
00:18:02.220 --> 00:18:06.260
point it just crossed. Hmm.
It's funny how that sneaks

317
00:18:06.260 --> 00:18:08.000
up on us. Sometimes then
you wake up one day

318
00:18:08.000 --> 00:18:10.070
and you go, wow, I'm
more interested in this than

319
00:18:10.070 --> 00:18:13.460
I was in that. Right.
If it's, it's amazing. Cause

320
00:18:13.460 --> 00:18:16.180
you still do both. You
just don't realize that once

321
00:18:16.790 --> 00:18:20.690
it becomes more of a
priority, it is fascinating. So

322
00:18:20.930 --> 00:18:23.810
robotics is making a bunch
of products now, not just

323
00:18:23.850 --> 00:18:27.040
Zuma learning, but there's also
STEM stories and STEM dash

324
00:18:27.050 --> 00:18:32.330
are making apps also apps
to teach kids. Yes. And

325
00:18:32.330 --> 00:18:34.880
I'm actually excited about that
in literacy apps as well.

326
00:18:35.990 --> 00:18:40.700
So the coding came from
thinking about assessable STEM and

327
00:18:40.700 --> 00:18:43.490
what are the components of
STEM? So of course there's

328
00:18:43.490 --> 00:18:47.330
the math, which was fairly
straight forward. And then there

329
00:18:47.330 --> 00:18:50.150
was the coding, which was
becoming more. And I see

330
00:18:50.150 --> 00:18:52.190
it even now as just
becoming so much more of

331
00:18:52.190 --> 00:18:56.240
a mainstay and States are
starting to require it for

332
00:18:56.240 --> 00:19:01.580
graduation and me interacting with
kids, especially children with diverse

333
00:19:01.580 --> 00:19:04.940
learning needs. I was like,
you know, these systems have

334
00:19:04.940 --> 00:19:07.550
no clue. You can't just
say, Oh, all of our

335
00:19:07.550 --> 00:19:09.920
12th graders are going to
suddenly have to learn how

336
00:19:09.920 --> 00:19:13.130
to code. What about those
individuals who don't have access?

337
00:19:13.130 --> 00:19:16.340
What about individuals who have
special needs? And so that's

338
00:19:16.340 --> 00:19:18.110
how coding came was like,
okay, I can teach a

339
00:19:18.110 --> 00:19:21.320
four year old now how
to code whatever their abilities.

340
00:19:22.100 --> 00:19:24.320
And, and when you think
about coding, it's like, you

341
00:19:24.320 --> 00:19:28.640
know, how do you think
through sequences and putting words

342
00:19:28.640 --> 00:19:31.580
together, a code together to
solve a problem and that

343
00:19:31.580 --> 00:19:36.800
concept. So we designed a
full curriculum. That again, starting

344
00:19:36.800 --> 00:19:40.160
from actually three, all the
way to eight, we have

345
00:19:40.160 --> 00:19:43.490
three coding apps that get
them more and more advanced

346
00:19:43.490 --> 00:19:45.290
through that, but as fun
and as engaging and it

347
00:19:45.290 --> 00:19:49.490
works with Zoomo. So it
was pretty, pretty cool. So

348
00:19:49.520 --> 00:19:53.000
rather than accepting that coding
should be done like this,

349
00:19:53.000 --> 00:19:55.700
and people should learn how
to code like this. You

350
00:19:55.700 --> 00:19:58.550
are like, you're, you're saying
a three year old can

351
00:19:58.550 --> 00:20:03.050
learn the procedural things needed
the systems thinking required to

352
00:20:03.050 --> 00:20:05.630
be a coder, even at
that age, even almost before

353
00:20:05.630 --> 00:20:08.780
they can read. Yes. You
know? And, and one of

354
00:20:08.780 --> 00:20:11.750
the things is I, kids
are natural scientists and engineers

355
00:20:11.750 --> 00:20:14.510
it's us as society that
kind of drums it out

356
00:20:14.510 --> 00:20:16.940
of them. But if you
ever see a child, I

357
00:20:16.940 --> 00:20:20.720
mean, they're curious, they're picking
up things, they're blinking styles.

358
00:20:20.750 --> 00:20:23.570
They're right. They're exploring their
world and looking at cause

359
00:20:23.570 --> 00:20:26.600
and effects and things like
that. And so when they're

360
00:20:26.600 --> 00:20:29.600
in that mode of how
does the world work because

361
00:20:29.600 --> 00:20:31.850
I'm growing and I'm in
it. I mean, that's the

362
00:20:31.850 --> 00:20:34.130
perfect time to say, Oh,
by the way, the world

363
00:20:34.130 --> 00:20:36.470
also works in terms of
coding. And this is how

364
00:20:36.470 --> 00:20:39.710
you think about putting things
together and sequencing. When you

365
00:20:39.710 --> 00:20:42.620
want to explore how to
break your mom's ex, well,

366
00:20:42.620 --> 00:20:46.730
these are the steps to
do it. You know, I

367
00:20:46.730 --> 00:20:48.970
was thinking this in the
context of what I learned,

368
00:20:48.970 --> 00:20:52.330
which was my, you know,
logo, the little, we thought

369
00:20:52.330 --> 00:20:53.860
it was a turtle. It
turns out it was actually

370
00:20:53.860 --> 00:20:58.150
just a triangle. I remember
it very distinctly as being

371
00:20:58.150 --> 00:21:00.490
this amazing turtle in this
high resolution screen, but it

372
00:21:00.490 --> 00:21:01.810
turns out it was just
black and white and it

373
00:21:01.810 --> 00:21:05.620
was this little triangle. But
the new, the new way

374
00:21:05.620 --> 00:21:08.170
of thinking is something like
you've got Tommy, the robot.

375
00:21:08.860 --> 00:21:11.470
This is like a little,
a little, little, I don't

376
00:21:11.470 --> 00:21:13.960
know how to describing. He's
a little cube with little

377
00:21:13.960 --> 00:21:17.920
cube feet. And he walks
around on a mat and,

378
00:21:17.980 --> 00:21:20.170
but you have to move
him with code. You have

379
00:21:20.170 --> 00:21:22.080
to teach him how to
write. You have to move

380
00:21:22.080 --> 00:21:24.750
him with code. And he
has friends. So the app,

381
00:21:24.810 --> 00:21:28.320
he has friends that you
have to help in terms

382
00:21:28.320 --> 00:21:30.510
of coding. And so, you
know, there's cat and dog

383
00:21:30.510 --> 00:21:34.110
and Al and you got
to get Tommy to go

384
00:21:34.110 --> 00:21:38.100
around the map and find
castle cats. Good. And then,

385
00:21:38.310 --> 00:21:41.250
you know, cat and turtle.
Well, where's the dog dog

386
00:21:41.280 --> 00:21:44.910
still wants to be a
friend too. And so, yeah.

387
00:21:45.240 --> 00:21:49.830
And kids love things that
move, right? These all have

388
00:21:49.830 --> 00:21:51.990
curriculum. So if you're a
teacher or, you know, a

389
00:21:51.990 --> 00:21:55.440
teacher, this isn't just a
thing that for, for someone

390
00:21:55.710 --> 00:21:57.780
who has some means to
go and spend money and

391
00:21:57.780 --> 00:21:59.430
buy a robot for their
one kid, this has been

392
00:21:59.430 --> 00:22:01.380
in a classroom setting, right.
It's meant to be in

393
00:22:01.380 --> 00:22:05.400
a classroom setting. All the
curriculum is online. It's quote

394
00:22:05.400 --> 00:22:07.860
unquote, open source. I use
like in files that you

395
00:22:07.860 --> 00:22:11.100
can easily download and change
based on, you know, if

396
00:22:11.100 --> 00:22:12.870
you want to make it
a one day or if

397
00:22:12.870 --> 00:22:16.290
you want to make it
a semester long curriculum. Hmm.

398
00:22:17.040 --> 00:22:19.770
So you're doing all of
this on both iOS and

399
00:22:19.770 --> 00:22:22.770
Android. So some of these
things are apps. You can

400
00:22:22.770 --> 00:22:26.520
download Tommy. The robot has
a robot that comes with

401
00:22:26.520 --> 00:22:30.630
him. The Zoomo learning system
with the, the plush turtle

402
00:22:30.630 --> 00:22:33.270
or the soft robot. Does
that include a tablet or

403
00:22:33.270 --> 00:22:35.700
do you use your own
tablet? You use your own

404
00:22:35.700 --> 00:22:39.420
tablet. So back in, when
we first started selling Zumo

405
00:22:39.690 --> 00:22:43.380
we included a tablet. And
what we found was that

406
00:22:43.650 --> 00:22:47.700
everyone just threw the tablet
away because they had like

407
00:22:47.760 --> 00:22:50.160
a lot of times, like
the school system had already

408
00:22:50.160 --> 00:22:52.800
got in the tablets, like
the brand new, you know,

409
00:22:52.980 --> 00:22:55.470
iPads. Right. I see. So
you had an Android tablet,

410
00:22:55.470 --> 00:22:58.470
you were including, and now
there's just no need. Cause

411
00:22:58.470 --> 00:23:01.770
like nobody used it. It
was like, okay, well let's

412
00:23:01.770 --> 00:23:03.810
like get rid of that
and lower the price. Like

413
00:23:03.840 --> 00:23:06.840
that makes perfect sense. Oh,
I see. That's wish I

414
00:23:06.840 --> 00:23:08.460
was. That's funny that you
mentioned that. Cause I noticed

415
00:23:08.460 --> 00:23:10.230
that the price, basically it
was cut in half and

416
00:23:10.230 --> 00:23:11.610
I was trying to figure
out what the story was

417
00:23:11.610 --> 00:23:13.830
there. The story was I'll
use the tablet that I've

418
00:23:13.830 --> 00:23:18.330
already got. Yes. Interesting. Now
you had talked about your

419
00:23:18.330 --> 00:23:21.630
interest in working with kids
that have different learning abilities

420
00:23:21.630 --> 00:23:25.770
and that might be a
mental limitation or a physical

421
00:23:25.770 --> 00:23:29.880
limitation. And there's also, you're
having an interest in accessible

422
00:23:30.060 --> 00:23:33.150
switches. People who have motor
limitations to still be able

423
00:23:33.150 --> 00:23:35.580
to play with a tablet,
even though they might not

424
00:23:35.580 --> 00:23:39.060
be able to touch the
screen. Correct. Because I think

425
00:23:39.360 --> 00:23:43.740
when you have the abilities
that we should, as scientists

426
00:23:43.740 --> 00:23:49.130
and engineers, we should basically
ensure that we provide access

427
00:23:49.130 --> 00:23:52.610
to what's out there. And
you know, the world is

428
00:23:52.610 --> 00:23:56.920
becoming this, this world. I
would say touch based world,

429
00:23:56.930 --> 00:24:00.290
right? Like even the OEMs
that used to have key.

430
00:24:00.470 --> 00:24:02.090
Like you have to go
and you have to touch

431
00:24:02.090 --> 00:24:08.090
the screen. And so I
worry that as we continue,

432
00:24:08.090 --> 00:24:11.120
if we don't think about
this consciously, that we're going

433
00:24:11.120 --> 00:24:14.420
to be in trouble, especially
since we now know that

434
00:24:14.420 --> 00:24:17.930
older adults, as they age
also have limitations in fine

435
00:24:17.930 --> 00:24:21.440
motor control. So we need
to design on both ends

436
00:24:21.440 --> 00:24:25.750
the ends of the spectrum.
Great point. And I've noticed

437
00:24:25.750 --> 00:24:30.550
that your, your apps don't
require really precise touches. You've

438
00:24:30.550 --> 00:24:33.820
put this, not just with
physical things like this button

439
00:24:33.820 --> 00:24:35.680
that you can add to
your tablet, but also the

440
00:24:35.890 --> 00:24:39.610
apps themselves don't require real
fine motor control because these

441
00:24:39.610 --> 00:24:44.320
are for kids. Correct? So
no, no tiny buttons, no

442
00:24:44.320 --> 00:24:50.470
swiping across no nothing that
requires you to physically like

443
00:24:50.470 --> 00:24:53.920
to make the movement also
a cognitive based movement. We're

444
00:24:53.920 --> 00:24:57.190
not doing that. You see
things like this going in

445
00:24:57.190 --> 00:25:02.350
the future right now. You
said that the turtle is,

446
00:25:02.740 --> 00:25:06.340
has the potential for, for
back and forth communication. Do

447
00:25:06.340 --> 00:25:09.850
you think that that plushes,
that are smart, have the

448
00:25:09.850 --> 00:25:11.980
potential to be as big
as something like an iPad?

449
00:25:13.510 --> 00:25:15.970
I think for a certain
target demographic. I know for

450
00:25:15.970 --> 00:25:19.300
one thing you can, if
you think about even the

451
00:25:19.360 --> 00:25:22.240
more gaming industry, right? Like
with all the things that

452
00:25:22.240 --> 00:25:26.410
are out there, Xbox, and
so what happens if you

453
00:25:26.410 --> 00:25:29.860
can convert that into an
educational tool, which really isn't

454
00:25:29.920 --> 00:25:33.430
right as a gaming tool,
but if you can convert

455
00:25:33.430 --> 00:25:38.740
that to an educational tool,
make your games educational and

456
00:25:38.740 --> 00:25:42.610
then add in things like
plushes and things that's exciting,

457
00:25:42.640 --> 00:25:44.710
just like, and people would
just be like, Oh, this

458
00:25:44.710 --> 00:25:47.650
is like kind of a
nice controller for this game

459
00:25:47.650 --> 00:25:53.560
machine. I think you can
start really making education a

460
00:25:53.560 --> 00:25:58.540
priority. Yeah. I was really
impressed with the, the accessible

461
00:25:59.350 --> 00:26:03.460
X-Box controller came out recently.
I mean, that's, that's got

462
00:26:03.570 --> 00:26:05.590
a lot of potential as
well. And I wonder if

463
00:26:05.590 --> 00:26:08.830
that could be expanded or
extended with something like your,

464
00:26:08.860 --> 00:26:11.830
your, your things. I mean,
it has an open API,

465
00:26:11.830 --> 00:26:14.410
basically anything with a one
eighth inch headphone Jack can

466
00:26:14.410 --> 00:26:19.480
be plugged into it. There's
a whole ecosystem around buttons

467
00:26:19.480 --> 00:26:22.420
and things that can plug
into Xbox controllers. I wonder

468
00:26:22.420 --> 00:26:23.950
if they could be used
on something that wasn't an

469
00:26:23.980 --> 00:26:27.370
Xbox, I don't know. Well,
I think it's pretty close.

470
00:26:27.370 --> 00:26:34.090
In fact, there, the construction
of the, the basically assessable

471
00:26:34.120 --> 00:26:39.100
Xbox controller that they released.
Yeah. Which is awesome. Functionally

472
00:26:39.100 --> 00:26:42.010
it's it's housing will works.
So if you open up

473
00:26:42.010 --> 00:26:47.010
Zumo, there's actually all of
these 3.5 millimeter jacks that

474
00:26:47.010 --> 00:26:51.390
go into our controller. Yes.
Great minds think alike. Yes,

475
00:26:52.910 --> 00:26:54.920
Of course. I would never
open my friend up or

476
00:26:55.230 --> 00:27:00.740
certainly not within the group.
Yeah. That would be kind

477
00:27:00.740 --> 00:27:03.020
of, you'd have to like,
yeah, that wouldn't be kind

478
00:27:03.020 --> 00:27:05.210
of nice. Not in front
of someone. Right? You have

479
00:27:05.210 --> 00:27:08.660
to be an a behind
closed door. No one peeking

480
00:27:08.660 --> 00:27:13.850
in. Absolutely. So Zumo has
apps on iTunes and Google

481
00:27:13.850 --> 00:27:17.660
plays. What works with both
Android and an iOS. Are

482
00:27:17.660 --> 00:27:20.060
you making new apps all
the time or what's next

483
00:27:20.060 --> 00:27:23.480
for Xero? We are. So
our current set of apps

484
00:27:23.480 --> 00:27:27.500
are an enhancement to our
STEM stories. So one of

485
00:27:27.500 --> 00:27:30.020
the things that we had
discovered, so our STEM stories

486
00:27:30.020 --> 00:27:34.010
combine literacy. So because again,
we're working with kids who

487
00:27:34.010 --> 00:27:37.490
are also learning how to
read early readers and our

488
00:27:37.490 --> 00:27:42.560
STEM stories. We're putting in
STEM concepts. So you can

489
00:27:42.560 --> 00:27:47.930
read about like Tommy, the
turtle, who's a turtle, walks

490
00:27:47.930 --> 00:27:52.790
you through coding through this
interactive storytelling mood. What we

491
00:27:52.790 --> 00:27:56.300
had was teachers were asking,
well, can we, can I

492
00:27:56.300 --> 00:27:59.030
get some feedback? And I
use it to also provide

493
00:27:59.030 --> 00:28:02.000
some input to the student.
So we have this whole

494
00:28:02.990 --> 00:28:07.250
call readable stories. That's based
on STEM stories that as

495
00:28:07.250 --> 00:28:10.400
it reads to you, you
be to it, it provides

496
00:28:10.400 --> 00:28:13.940
you information on how to
correct your reading. So it's

497
00:28:13.940 --> 00:28:18.590
literally a reading tutor in
the home. I see. So

498
00:28:18.590 --> 00:28:21.980
you're reading the story on
your iPad. I can see

499
00:28:21.980 --> 00:28:23.900
that the, you can download
STEM some of the STEM

500
00:28:23.900 --> 00:28:27.020
stories for free, and then
on the screen, you can

501
00:28:27.080 --> 00:28:30.950
decide what Zumo does and
how he interacts basically key

502
00:28:30.950 --> 00:28:34.010
bindings for the, for the
techies amongst us. And you

503
00:28:34.010 --> 00:28:37.460
adjust Zumos key bindings and
interact with him as you

504
00:28:37.460 --> 00:28:40.280
read the, I read the
books, but it also verbally

505
00:28:40.280 --> 00:28:45.260
gives you input. The tablet
talks, the tablet talks. Okay.

506
00:28:45.290 --> 00:28:48.920
So there's a narrator. There
was a narrator in avatar.

507
00:28:49.220 --> 00:28:53.000
Yes. Oh, okay. And you
said that they are switch

508
00:28:53.060 --> 00:28:56.720
accessible eBooks where that switch
might be Zoomo the turtle,

509
00:28:56.720 --> 00:28:58.940
or it might be one
of your other switches. It

510
00:28:58.940 --> 00:29:01.730
can be not just our
switches. So we really wanted

511
00:29:01.730 --> 00:29:05.150
to ensure, like, when I
think about accessibility and access,

512
00:29:05.360 --> 00:29:09.260
it also means this community.
And so any of the

513
00:29:09.260 --> 00:29:13.490
switches that are out there
and available that currently work

514
00:29:13.490 --> 00:29:19.070
with iPad or Android will
work with our apps. Okay.

515
00:29:19.130 --> 00:29:22.010
It's pretty slick. And it's
also worth pointing out that

516
00:29:22.010 --> 00:29:25.700
the app has no in
app purchases is no advertising.

517
00:29:25.700 --> 00:29:28.370
I know that we download
a lot of iPad apps

518
00:29:28.370 --> 00:29:30.440
that are filled with ads
and things that we don't

519
00:29:30.440 --> 00:29:34.850
necessarily want blasted into our
kids' brains. There's a ton

520
00:29:34.850 --> 00:29:36.860
of apps that you've got,
I'm looking at, what is

521
00:29:36.860 --> 00:29:39.680
it like 20 plus apps?
That's our robotics has in

522
00:29:39.680 --> 00:29:42.250
the, in at least in
the iTunes full of STEM

523
00:29:42.250 --> 00:29:48.220
Stories, correct? Without advertising with
none, no advertising. So there's

524
00:29:48.220 --> 00:29:52.000
no fear of like something
coming in through the back

525
00:29:52.000 --> 00:29:55.390
door. And it also means
that, so one of the

526
00:29:55.390 --> 00:29:57.910
things we've noticed with some
of the quote unquote free

527
00:29:57.910 --> 00:30:02.410
apps, cause they're not free,
but when a kid is

528
00:30:02.410 --> 00:30:06.340
working with it is actually
is distracting from the educational

529
00:30:06.340 --> 00:30:11.290
content, right. Which is not
helping their learning. Like as

530
00:30:11.290 --> 00:30:13.300
you're doing something, you need
to be immersed in this

531
00:30:13.300 --> 00:30:15.850
experience. And if it's that,
Oh, in order to go

532
00:30:15.850 --> 00:30:18.730
to the next stage, there's
this ad, you've just broke

533
00:30:18.730 --> 00:30:22.000
that connection for learning. Right.
When you've taught that you've

534
00:30:22.000 --> 00:30:25.360
taught the child about these
idle tappers, where you're teaching

535
00:30:25.360 --> 00:30:28.180
them how to be advertised
to, which has been a

536
00:30:28.180 --> 00:30:32.050
big problem for my kids,
11 and 13, they are

537
00:30:32.050 --> 00:30:34.240
willing to tolerate this ad
and they'll sit there and

538
00:30:34.240 --> 00:30:39.100
passively watch it beam into
their brains, which is frankly,

539
00:30:39.100 --> 00:30:41.320
it's gotten me looking for
apps that don't have that

540
00:30:41.320 --> 00:30:47.410
kind of nonsense. Yeah. I
know. So folks can check

541
00:30:47.410 --> 00:30:54.070
out Zoomo the turtle@zumolearning.com and
STEM stories, his stories with

542
00:30:54.070 --> 00:30:55.480
a Z at the end,
I'm going to make sure

543
00:30:55.480 --> 00:30:57.910
that I include links to
all of this in the

544
00:30:57.910 --> 00:31:00.880
show notes. It's been really
nice catching up with you.

545
00:31:00.880 --> 00:31:02.500
It's been a while and
maybe a whole check in

546
00:31:02.500 --> 00:31:03.700
with you in a couple
of years and see how

547
00:31:03.700 --> 00:31:05.320
things are going. I don't
know. Let's not make it

548
00:31:05.320 --> 00:31:09.820
four though. That's absolutely true.
Hopefully I'll see you sooner

549
00:31:09.820 --> 00:31:13.210
than later. Yes. That would
be lovely. We've been talking

550
00:31:13.210 --> 00:31:16.930
with dr. Hannah Howard from siren
robotics. This has been another

551
00:31:16.930 --> 00:31:39.780
episode of handsome. We'll see
you again next week. <inaudible>.

