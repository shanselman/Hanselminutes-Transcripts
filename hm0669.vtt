WEBVTT FILE

1
00:00:00.270 --> 00:00:03.900
Hey folks. You're a very
good developer, probably regardless. You

2
00:00:03.900 --> 00:00:07.140
do write bugs. That's unavoidable.
What is affordable is wasting

3
00:00:07.140 --> 00:00:09.750
time trying to track down
the cause of those bugs

4
00:00:09.990 --> 00:00:14.190
century.io provides full stack error
tracking that lets you monitor

5
00:00:14.490 --> 00:00:16.980
and fix problems. In real
time, you can see the

6
00:00:16.980 --> 00:00:20.130
severity and the scope of
the error. Get immediate access

7
00:00:20.130 --> 00:00:22.770
to the stack, trace connect
the problem to the commit

8
00:00:22.770 --> 00:00:26.370
that caused it and fix
it without delay century to

9
00:00:26.370 --> 00:00:28.470
name so common that they
have to include the top

10
00:00:28.470 --> 00:00:30.660
level domain in their advertising
to make sure you remember

11
00:00:30.660 --> 00:00:37.650
it. century.io, that's century.io, open
source, full stack web apps,

12
00:00:37.710 --> 00:00:40.770
native apps, mobile games, smart
oven mitts. If you can

13
00:00:40.770 --> 00:00:43.800
program it, they can make
it far easier to fix

14
00:00:43.800 --> 00:00:46.530
any errors you encounter with
it. Your code may be

15
00:00:46.530 --> 00:01:04.460
broken. Let's fix it together.
Visit century.io. Hi, this is

16
00:01:04.460 --> 00:01:07.100
Scott Hanselman. This is another
one Hansel minutes. And today

17
00:01:07.100 --> 00:01:10.580
I'm talking with dr. Sophia
noble she's professor at the

18
00:01:10.580 --> 00:01:13.910
university of Southern California Annenberg
school for communication as well

19
00:01:13.910 --> 00:01:17.270
as at UCLA graduate school
of education and information studies.

20
00:01:17.840 --> 00:01:20.570
And I found you because
I read your book algorithms

21
00:01:20.630 --> 00:01:23.420
of oppression, which is a
book that you wrote with

22
00:01:23.420 --> 00:01:26.900
over six years of academic
research on Google search algorithms.

23
00:01:26.930 --> 00:01:29.030
Thanks so much for talking
to me today. Great to

24
00:01:29.030 --> 00:01:31.550
be here. So this is
not the first book you've

25
00:01:31.550 --> 00:01:33.320
written, but it's definitely a
book that has got a

26
00:01:33.320 --> 00:01:36.110
lot of people talking. You,
you went and did research

27
00:01:36.110 --> 00:01:38.360
into the power of the
algorithms that we use every

28
00:01:38.360 --> 00:01:41.060
day to, for something simple,
just find a picture or

29
00:01:41.060 --> 00:01:45.410
find pizza locally. I did,
you know, I was drawn

30
00:01:45.410 --> 00:01:49.100
to this work because I
was, you know, kind of

31
00:01:49.100 --> 00:01:53.960
serendipitously, I guess you could
say, looking for information that

32
00:01:53.960 --> 00:01:58.400
might be of interest to
my bonus daughter and my

33
00:01:58.400 --> 00:02:02.240
nieces. And it really started
this study started with a

34
00:02:02.240 --> 00:02:05.840
simple search on the keywords
black girls back in about

35
00:02:05.840 --> 00:02:11.930
2010. And I was stunned
to find that pornography and

36
00:02:11.930 --> 00:02:17.210
hyper-sexualized content was the primary
way that these two keywords

37
00:02:17.210 --> 00:02:22.880
together surfaced up content. And
that launched me into a

38
00:02:22.880 --> 00:02:26.810
multi-year study and looking at
all of the ways in

39
00:02:26.810 --> 00:02:29.840
which a whole host of
things that I think we

40
00:02:29.840 --> 00:02:33.050
might not have ever thought
about before, particularly when it

41
00:02:33.050 --> 00:02:39.800
comes to vulnerable people like
children, people of color, sometimes

42
00:02:39.800 --> 00:02:44.240
women in our society in
the U S are misrepresented.

43
00:02:44.300 --> 00:02:47.210
And I was thinking about
this in the context of

44
00:02:48.440 --> 00:02:54.050
other types of information institutions
like libraries and schools and

45
00:02:54.320 --> 00:02:57.500
the prominence that search engines
are starting to take kind

46
00:02:57.500 --> 00:03:00.790
of a bit in their
place and why we might

47
00:03:00.790 --> 00:03:03.280
want to pay more attention
to what happens in search

48
00:03:03.280 --> 00:03:08.680
engines when it comes to
representing people, communities, and concepts.

49
00:03:09.540 --> 00:03:11.940
And it seems like there's
a lot of, as there

50
00:03:11.940 --> 00:03:14.760
are lots of different kinds
of people with different educational

51
00:03:14.760 --> 00:03:17.820
backgrounds, a lot of different
people have different perspectives on

52
00:03:17.820 --> 00:03:20.430
what happens when you type
a word into a search

53
00:03:20.430 --> 00:03:23.160
engine and hit enter. Some
people might think that that

54
00:03:23.160 --> 00:03:26.160
it's curated that there's a
real person. Some people understand

55
00:03:26.160 --> 00:03:29.700
the algorithm and are forgiving
of it and others have

56
00:03:29.700 --> 00:03:33.780
no idea. It's just magic.
It's true. One of the

57
00:03:33.780 --> 00:03:38.490
things we know for example
is that from the Pew

58
00:03:38.880 --> 00:03:43.440
research on search engine users
is that many people think

59
00:03:43.440 --> 00:03:46.950
that the things they find
in a search engine, especially

60
00:03:46.950 --> 00:03:52.050
on the first page are
reliable or credible, you know,

61
00:03:52.050 --> 00:03:56.310
trustworthy. And I think that's
mostly because when we look

62
00:03:56.310 --> 00:04:01.620
for banal information, like where's
the closest Starbucks, for example,

63
00:04:01.620 --> 00:04:07.230
or some other kind of
fact, that's not disputed search

64
00:04:07.230 --> 00:04:10.860
engines can often give us
the right answers. So that

65
00:04:10.860 --> 00:04:13.530
bolsters our trust. And for
a lot of things, they

66
00:04:13.530 --> 00:04:17.310
are reliable. But when you
start getting to more nuanced,

67
00:04:17.310 --> 00:04:24.810
complicated contested information, when you
start thinking about identity and,

68
00:04:25.410 --> 00:04:31.860
and the way in which
misinformation or disinformation can be

69
00:04:32.070 --> 00:04:35.340
easily manipulated or kind of
gamed to the top of

70
00:04:35.400 --> 00:04:39.510
the pile of information, I
think that's where I'm really

71
00:04:39.510 --> 00:04:44.040
interested in thinking about what's
at stake and in what

72
00:04:44.040 --> 00:04:49.650
ways might that undermine the
public really having good, reliable,

73
00:04:49.770 --> 00:04:54.780
trustworthy information. And it makes
me wonder also what the,

74
00:04:54.840 --> 00:04:58.050
what the correct result would
be like. We all have

75
00:04:58.140 --> 00:05:01.740
a subjective perspective on when
these, when we search for

76
00:05:01.740 --> 00:05:04.380
something, what we expect to
get. We're delighted if it's

77
00:05:04.380 --> 00:05:07.800
correct and we're upset if
it's not, do you solve

78
00:05:07.800 --> 00:05:10.440
that by making better algorithms,
building different teams or do

79
00:05:10.440 --> 00:05:13.650
you just solve it through
curation? It's true. I mean,

80
00:05:13.680 --> 00:05:15.780
I will tell you one
of the things that I

81
00:05:15.810 --> 00:05:19.230
was thinking about for the
years that I was finding

82
00:05:19.230 --> 00:05:24.240
pornography as the primary representation
of black girls, Latina girls,

83
00:05:24.510 --> 00:05:28.230
Asian girls, is that I
don't think that's the way

84
00:05:28.470 --> 00:05:33.780
those communities, those people, those
groups of people would represent

85
00:05:33.780 --> 00:05:37.950
themselves. And of course these
are, you know, in this

86
00:05:37.950 --> 00:05:41.910
case of those examples, we're
talking about children and adolescents

87
00:05:42.240 --> 00:05:45.210
who don't have a lot
of money who don't understand

88
00:05:45.210 --> 00:05:49.680
that Google search is primarily
an advertising engine, right? It's

89
00:05:50.610 --> 00:05:52.650
those who have a lot
of capital. And there's a

90
00:05:52.650 --> 00:05:54.840
lot of research to kind
of bear this out. People

91
00:05:54.840 --> 00:05:59.390
with an organizations with the
most are often able to

92
00:05:59.390 --> 00:06:04.400
take advantage of let's say
ad words and, and helping

93
00:06:04.400 --> 00:06:07.190
to make their content more
visible. And in the case

94
00:06:07.190 --> 00:06:10.670
of the pornography industry, you
know, you didn't really even

95
00:06:10.670 --> 00:06:14.840
have to add the words,
sex or porn or pornography

96
00:06:15.050 --> 00:06:18.800
to make black girls, Asian
girls, Latina girls, synonymous with

97
00:06:18.800 --> 00:06:21.350
porn. And so those are
the kinds of things that

98
00:06:21.350 --> 00:06:25.100
I think people are less
aware of as part of

99
00:06:25.100 --> 00:06:31.100
the way in which powerful
industries and organizations really have

100
00:06:31.100 --> 00:06:35.780
a lot more ability to
have their version of black

101
00:06:35.780 --> 00:06:39.680
girls represented maybe then African
American or black or Latino

102
00:06:39.710 --> 00:06:42.500
girls themselves. And those are
the kinds of power dynamics

103
00:06:42.500 --> 00:06:46.150
that I'm really interested in
studying. Now, this is the,

104
00:06:46.150 --> 00:06:49.570
the, the focus of your
book and it's called algorithms

105
00:06:49.570 --> 00:06:53.440
of oppression using this search
term, as an example for

106
00:06:53.440 --> 00:06:58.420
underrepresented groups, does this apply
to any results that is

107
00:06:58.420 --> 00:07:00.970
an undesired that has an
undesirable result, any term that

108
00:07:00.970 --> 00:07:03.830
I might search for that,
whether it be like I

109
00:07:03.830 --> 00:07:05.230
might search for, I don't,
I have no idea I

110
00:07:05.230 --> 00:07:07.600
haven't searched for it yet,
but like, you know, Americans

111
00:07:07.600 --> 00:07:10.690
and then Google says are
dumb or, you know, so,

112
00:07:10.690 --> 00:07:13.150
and so actor that I
might like is, and then

113
00:07:13.150 --> 00:07:15.940
it starts to auto, complete
is an idiot, is whatever

114
00:07:17.140 --> 00:07:18.940
one of the, when are
these things correct? And when

115
00:07:18.940 --> 00:07:21.460
are they not? And when
are they problematic and when

116
00:07:21.460 --> 00:07:24.010
are they not? Well, I
think, you know, this has

117
00:07:24.010 --> 00:07:26.830
been one of the issues
that's been in the headlines

118
00:07:26.830 --> 00:07:32.350
here quite recently, for example,
you know, how Google auto

119
00:07:32.350 --> 00:07:37.570
suggests certain ideas, you know,
whether the word idiot let's

120
00:07:37.570 --> 00:07:42.280
say gets paired up with
a politician or someone in

121
00:07:42.280 --> 00:07:44.590
the white house or that
kind of thing. And these

122
00:07:44.590 --> 00:07:48.730
are, you know, of course
the new conversations that we're

123
00:07:48.730 --> 00:07:53.320
having are about particular types
of bias, whether there's certain

124
00:07:53.320 --> 00:07:57.580
kinds of political bias, right
around search engine results. And

125
00:07:57.580 --> 00:08:01.480
I think that, you know,
this is a new opening

126
00:08:01.720 --> 00:08:05.200
that here to for there
was a high degree of

127
00:08:05.200 --> 00:08:08.540
trust, not even a questioning
of whether there was a

128
00:08:08.560 --> 00:08:12.430
particular slant or bias happening
in search. Now, I think

129
00:08:12.820 --> 00:08:17.290
certainly with my book and
other investigative journalists and people

130
00:08:17.290 --> 00:08:21.100
like yourself who are opening
up the conversation, it's allowing

131
00:08:21.100 --> 00:08:25.420
us to say, well, what
are the primary influences in

132
00:08:25.420 --> 00:08:29.530
a commercial search engine what's
at stake? And how is

133
00:08:29.530 --> 00:08:32.680
that different for different people?
And certainly I think that

134
00:08:32.680 --> 00:08:37.270
celebrities and political figures, you
know, there's a long history

135
00:08:37.330 --> 00:08:42.010
of having their names kind
of associated with unfavorable content.

136
00:08:42.700 --> 00:08:44.710
But those are people who
have a lot of power

137
00:08:44.710 --> 00:08:48.970
in our society, celebrities and
politicians, I think much more

138
00:08:48.970 --> 00:08:52.630
is at stake for people
who are vulnerable, who don't

139
00:08:52.630 --> 00:08:56.760
have influence who aren't millionaires,
who don't have the ability

140
00:08:56.760 --> 00:09:01.380
for example, to affect public
policy, what what's happening for

141
00:09:01.380 --> 00:09:03.450
them and what kind of
bias is in place. And

142
00:09:03.450 --> 00:09:08.970
we see this, for example,
around immigration, you know, there's

143
00:09:08.970 --> 00:09:12.960
kind of, you know, negative
South of the border types

144
00:09:12.960 --> 00:09:16.470
of conversations that come up
and kind of dominate and

145
00:09:16.470 --> 00:09:19.800
search engines about, you know,
a word like that. So

146
00:09:19.800 --> 00:09:23.430
there's certainly kind of a,
a set of politics that

147
00:09:23.430 --> 00:09:27.420
you might say that are
popular and that circulate. And

148
00:09:27.420 --> 00:09:30.960
in some ways I think
what's a challenge is that

149
00:09:31.770 --> 00:09:35.100
the public relates to search
engines as a bit of

150
00:09:35.100 --> 00:09:38.010
a truth teller like that.
It will give them facts

151
00:09:38.040 --> 00:09:41.220
and it's less, the public
is less likely to understand

152
00:09:41.430 --> 00:09:45.480
that they're all are all
kinds of influences happening around

153
00:09:45.480 --> 00:09:47.280
what they may see. And
those are the things that

154
00:09:47.280 --> 00:09:50.780
I think are worth talking
about. One of the phrases

155
00:09:50.780 --> 00:09:52.850
that you use in the
book that I really like,

156
00:09:52.880 --> 00:09:54.560
I think I'm going to
start using all the time

157
00:09:54.560 --> 00:09:58.670
is this concept of algorithmic
literacy. I didn't think that

158
00:09:58.670 --> 00:10:01.370
anyone needed that, but do
you think that this is

159
00:10:01.370 --> 00:10:03.230
something that should be taught,
that the idea that one

160
00:10:03.230 --> 00:10:05.960
should be conscious of an
algorithm, know what it is

161
00:10:05.990 --> 00:10:08.390
that it exists and that
it may not necessarily give

162
00:10:08.390 --> 00:10:11.990
them what they want? Well,
I definitely think that we

163
00:10:11.990 --> 00:10:15.410
are moving into an era
in our kind of society

164
00:10:15.440 --> 00:10:21.740
where people may not understand
the very complex mathematical formulations

165
00:10:21.770 --> 00:10:26.720
that undergird the algorithms that
they're engaging with every day

166
00:10:26.720 --> 00:10:29.300
and all kinds of different,
let's say apps that they're

167
00:10:29.300 --> 00:10:33.500
using on their phones, or,
you know, at work and

168
00:10:33.500 --> 00:10:37.580
at play, they may not
understand the math of it.

169
00:10:37.610 --> 00:10:41.690
And in many cases, the
algorithm is, you know, it's

170
00:10:41.690 --> 00:10:44.540
black box, you can't see
it, it's proprietary, it's a

171
00:10:44.540 --> 00:10:49.010
trade secret, but they certainly
should understand how to evaluate

172
00:10:49.010 --> 00:10:53.750
the output of these systems
and what they might mean

173
00:10:53.810 --> 00:10:58.610
rather than I think blindly
trusting that there are no

174
00:10:58.610 --> 00:11:04.400
power dynamics or no effects
that, you know, could be

175
00:11:04.400 --> 00:11:06.920
manipulated in a way to
get a whole host of

176
00:11:07.220 --> 00:11:10.940
answers or solutions. And ultimately,
I think with many of

177
00:11:10.940 --> 00:11:14.960
the algorithms that we're engaging
with, the very first motivation

178
00:11:14.960 --> 00:11:19.100
is a profit motive. And
so whether it's collecting data

179
00:11:19.100 --> 00:11:22.910
on us as we use
these applications and these algorithms,

180
00:11:22.940 --> 00:11:27.890
or whether it's feeding back
a very, you know, consumeristic

181
00:11:28.640 --> 00:11:33.890
set of propositions for us,
our literacy is really important.

182
00:11:33.890 --> 00:11:36.080
And I think one of
the things that I try

183
00:11:36.080 --> 00:11:38.330
to stress in my work
is that the more we

184
00:11:38.330 --> 00:11:43.520
let algorithms make decisions for
us, rather than using kind

185
00:11:43.520 --> 00:11:47.210
of our own critical thinking
and our own ideas, our,

186
00:11:47.210 --> 00:11:50.960
and kind of human subjectivities.
I think the more we

187
00:11:51.710 --> 00:11:57.520
lose a sense of being
able to participate in our

188
00:11:57.520 --> 00:12:00.790
own decision making that affects
our ourselves, our families and

189
00:12:00.790 --> 00:12:03.810
our communities. Yeah. I think
that's a really significant point

190
00:12:03.810 --> 00:12:05.820
that you're making and I'm
seeing it in my, in

191
00:12:05.820 --> 00:12:09.870
my young sons that I'm
raising that they are taking

192
00:12:09.870 --> 00:12:14.790
things for granted and they're
taking autocomplete as, as gospel

193
00:12:14.790 --> 00:12:18.270
for, to mix my metaphors
a little bit. My 13

194
00:12:18.270 --> 00:12:19.920
year old was like, Hey,
I've got this thing in

195
00:12:19.920 --> 00:12:21.780
the back of my head,
is it cancer or whatever.

196
00:12:21.780 --> 00:12:24.000
And he'd been looking so,
you know, some something he

197
00:12:24.000 --> 00:12:25.830
typed in like brain or
head or something, and then

198
00:12:25.830 --> 00:12:29.040
it was like, might be
cancer auto-complete or something. And

199
00:12:29.040 --> 00:12:31.440
it was like, well, no,
like use your critical thinking.

200
00:12:31.440 --> 00:12:33.720
Let's he's like, well, doesn't
Google know all these things.

201
00:12:33.720 --> 00:12:37.440
Well, no, your common sense
is as important or more

202
00:12:37.440 --> 00:12:40.140
so than anything. Google could
give you a, and it

203
00:12:40.140 --> 00:12:43.500
was really surprising. Yes. Well,
you know, I work in

204
00:12:43.500 --> 00:12:46.080
a university and I teach,
I've been teaching university students

205
00:12:46.080 --> 00:12:48.990
now for some time, I
will tell you that many

206
00:12:48.990 --> 00:12:53.700
of my students say they
could never graduate from college

207
00:12:53.880 --> 00:12:57.540
without Google. Google's the first
place they go to help

208
00:12:57.540 --> 00:13:01.170
them write papers, you know,
through ideas. And one of

209
00:13:01.170 --> 00:13:04.200
the ways I challenged them,
as I say, listen, you

210
00:13:04.200 --> 00:13:07.890
know, you've gone through 12
years of education, K through

211
00:13:07.890 --> 00:13:10.410
12. Now you're in a
U S university for a

212
00:13:10.410 --> 00:13:15.450
few years, and there are
many vantage points that you

213
00:13:15.450 --> 00:13:20.160
could take up looking at
any particular phenomenon or object.

214
00:13:20.550 --> 00:13:26.670
And we spend time years
cultivating a complex way of

215
00:13:26.670 --> 00:13:31.710
thinking. Not all questions can
be answered in 0.03 seconds.

216
00:13:31.860 --> 00:13:34.230
In fact, there are many
forms of knowledge and ideas

217
00:13:34.500 --> 00:13:38.640
that are heavily contested have
been contested for thousands of

218
00:13:38.640 --> 00:13:43.650
years. So what we seed
in our own acculturation to

219
00:13:43.650 --> 00:13:48.150
algorithms is this idea of,
you know, an instant answer

220
00:13:48.300 --> 00:13:52.590
that everything can be known
that it's fixed. And that

221
00:13:52.590 --> 00:13:55.110
we don't have a say
that, you know, the machine

222
00:13:55.110 --> 00:13:57.450
is dictating to us what
the answer is. And I

223
00:13:57.450 --> 00:14:00.990
think that, you know, that
is not the way you

224
00:14:00.990 --> 00:14:04.620
want to educate people in
a democracy. You want people

225
00:14:04.620 --> 00:14:08.460
to have high levels of
literacy, be very well educated

226
00:14:08.760 --> 00:14:12.180
and not just kind of
function as a dumb box

227
00:14:12.210 --> 00:14:16.290
where, you know, the, the
machine gives you an answer.

228
00:14:16.500 --> 00:14:19.140
And I think where we
were losing something, we're putting

229
00:14:19.140 --> 00:14:23.550
something at stake in our
human education when we kind

230
00:14:23.550 --> 00:14:27.210
of moved to these, these
new processes. Yeah. It seems

231
00:14:27.210 --> 00:14:29.280
that the heat gets turned
up on the stove a

232
00:14:29.280 --> 00:14:31.980
little bit at a time.
My wife noticed recently that

233
00:14:31.980 --> 00:14:35.460
I'm using the GPS for
basically everything, even places that

234
00:14:35.460 --> 00:14:37.980
I know darn well, how
to get to. And she

235
00:14:37.980 --> 00:14:41.760
always chastises me for trusting
the GPS and not trusting

236
00:14:41.760 --> 00:14:45.720
my own common sense. Well,
you know, this is certainly

237
00:14:45.720 --> 00:14:49.590
important. And some of the
things that we know for

238
00:14:49.590 --> 00:14:56.900
example about mapping technologies is
that often maps will route

239
00:14:56.900 --> 00:15:00.800
us through certain neighborhoods, not
route us through other neighborhoods.

240
00:15:01.010 --> 00:15:05.090
I think there was recently
some stories about Uber drivers,

241
00:15:05.090 --> 00:15:09.920
for example, going through affluent
neighborhoods and people not liking

242
00:15:09.950 --> 00:15:15.470
apps like ways, taking them
through, you know, affluent again,

243
00:15:15.500 --> 00:15:18.110
neighborhoods that don't want to
see a lot of traffic

244
00:15:18.110 --> 00:15:22.190
down their streets, but also
conversely not going into certain

245
00:15:22.190 --> 00:15:26.810
neighborhoods that maybe have been
programmed or deemed somehow by

246
00:15:26.810 --> 00:15:30.380
the algorithm to be quote
unquote bad neighborhoods. So, you

247
00:15:30.380 --> 00:15:33.230
know, these things are really
interesting to me and I

248
00:15:33.230 --> 00:15:37.670
think they, I find it
fascinating to sit and think

249
00:15:37.670 --> 00:15:40.070
about these things, to research
them and to talk about

250
00:15:40.070 --> 00:15:42.770
them. And when I talk
to other people, they often

251
00:15:42.770 --> 00:15:44.420
scratch their head and they're
like, Hmm. You know, I

252
00:15:44.420 --> 00:15:48.350
never really thought about that.
And yeah, I should, maybe

253
00:15:48.350 --> 00:15:54.140
I should rethink my overreliance
upon, you know, where I'm

254
00:15:54.140 --> 00:15:57.740
being directed. And I think,
again, you know, this conversation

255
00:15:57.740 --> 00:16:00.620
is really interesting to think
about what might be at

256
00:16:00.620 --> 00:16:05.150
stake in concretizing and making
even more real, these ideas

257
00:16:05.150 --> 00:16:08.360
that some neighborhoods, which of
course we often know are

258
00:16:08.360 --> 00:16:11.660
like neighborhoods where a lot
of people of color live

259
00:16:11.660 --> 00:16:15.500
or poor communities are quote
unquote, the bad places and

260
00:16:15.500 --> 00:16:18.980
not the good places. And
I think people are studying

261
00:16:18.980 --> 00:16:23.620
how apps are reinforcing. Some
of these ideas too. This

262
00:16:23.620 --> 00:16:27.460
episode is sponsored by stack
overflow for teams, a private

263
00:16:27.460 --> 00:16:31.150
secure home for your team's
questions and answers. It's everything

264
00:16:31.150 --> 00:16:34.180
that you love about stack
overflow now for your company

265
00:16:34.600 --> 00:16:37.840
and get instant, reliable answers
from your coworkers without disrupting

266
00:16:37.840 --> 00:16:42.160
them unlike documentation or wikis,
long email threads, or distracting

267
00:16:42.160 --> 00:16:45.220
instant messages, Q and a
mimics the way that we

268
00:16:45.220 --> 00:16:49.450
naturally share and find information,
introduce stack overflow for teams

269
00:16:49.450 --> 00:16:52.450
to your company, and start
building products faster and better

270
00:16:52.780 --> 00:16:58.270
visit s.tk/hanselman it's to get
your first month free that's

271
00:16:58.330 --> 00:17:03.520
S dot T K slash
Hansel minutes. What do you

272
00:17:03.520 --> 00:17:06.250
say for people who might
be listening in might be

273
00:17:06.250 --> 00:17:09.250
thinking that, Oh gosh, you
know, she's making this all

274
00:17:09.250 --> 00:17:11.830
about color or all about
politics. I don't want color

275
00:17:11.830 --> 00:17:16.780
in my, in my neutral
computerized artificial intelligence. I want

276
00:17:16.780 --> 00:17:19.450
it just to simply be
neutral. Well, you know, there's

277
00:17:19.450 --> 00:17:24.490
such a strong set of
ideas in our society. That

278
00:17:24.520 --> 00:17:27.940
technology is neutral, and this
is, I'm not the first

279
00:17:27.940 --> 00:17:31.570
person to kind of question
whether that's true or not.

280
00:17:31.840 --> 00:17:34.630
I will say that I've
had a lot of computer

281
00:17:34.630 --> 00:17:39.040
scientists over the years, students
in my classes. And one

282
00:17:39.040 --> 00:17:42.250
of the things that I
find interesting is that after

283
00:17:42.550 --> 00:17:46.660
three or four or five
years of an engineering education,

284
00:17:47.320 --> 00:17:50.400
I mean, these are very
smart students of whom have

285
00:17:50.730 --> 00:17:54.060
AP tested in high school,
out of English, out of

286
00:17:54.060 --> 00:17:57.510
their humanities courses. They're not
taking history or that any

287
00:17:57.510 --> 00:18:01.050
social science or humanities courses
at the university level, they're

288
00:18:01.050 --> 00:18:04.650
in strict kind of math
and engineering education. And I

289
00:18:04.650 --> 00:18:07.230
often say to them, how
much confidence do you feel

290
00:18:07.380 --> 00:18:11.130
about designing technology for society
when you really have not

291
00:18:11.130 --> 00:18:17.430
studied society at the university
level now, and often I

292
00:18:17.430 --> 00:18:20.520
find that the kinds of
books and articles that I

293
00:18:20.520 --> 00:18:23.940
ask them to read about
the impact of technology in

294
00:18:23.940 --> 00:18:27.330
society are things they've never
been exposed to. And they

295
00:18:27.330 --> 00:18:32.280
find themselves, you know, in
some ways I think feeling

296
00:18:32.340 --> 00:18:37.560
ill prepared and incredibly responsible
for the kinds of products

297
00:18:37.560 --> 00:18:40.380
they're going to be designing
for people. And they really

298
00:18:40.380 --> 00:18:44.880
have never thought about any
adverse effects. So, you know,

299
00:18:45.660 --> 00:18:49.050
I often say, and I
say in the book that

300
00:18:49.860 --> 00:18:55.590
an interdisciplinary well-rounded kind of
preparation for engineering students is

301
00:18:55.950 --> 00:18:59.970
really crucial because I think
that there's always kind of

302
00:18:59.970 --> 00:19:03.510
a set of intended outcomes.
And there are often a

303
00:19:03.510 --> 00:19:07.260
set of unintended outcomes and
the more you know about

304
00:19:07.260 --> 00:19:11.280
history and society and people,
the more you can kind

305
00:19:11.280 --> 00:19:16.650
of be prepared to address
some of the unintended consequences

306
00:19:16.650 --> 00:19:20.370
and even Silicon Valley executives
now will often see that

307
00:19:20.370 --> 00:19:24.360
they are thinking about ethics
now thinking about the disparate

308
00:19:24.390 --> 00:19:27.270
impact of their products. But
I think it's been people

309
00:19:27.570 --> 00:19:30.780
like myself and others who
have raised some criticisms that

310
00:19:30.780 --> 00:19:36.750
have helped the industry rethink
some of its practices. And

311
00:19:36.750 --> 00:19:39.440
I think overall, that's a
great thing. One of the

312
00:19:39.440 --> 00:19:42.170
things that I appreciated particularly
about your book was that

313
00:19:42.170 --> 00:19:45.770
you stated something very simply
that search results are more

314
00:19:45.770 --> 00:19:49.010
than simply what is popular,
but somehow people are using

315
00:19:49.430 --> 00:19:53.870
objective and neutral and you're
using it as a synonym

316
00:19:53.870 --> 00:19:58.790
for popular. Yes. Well, you
know, you can find something

317
00:19:58.790 --> 00:20:01.370
that's popular in society and
it may or may not

318
00:20:01.370 --> 00:20:05.840
be true. So we know
this right. And irrespective of

319
00:20:05.840 --> 00:20:08.810
whether we're using technology or
not, that many things that

320
00:20:08.810 --> 00:20:13.280
are popular are not positive
for society, or are misrepresentative

321
00:20:13.280 --> 00:20:16.940
stereotypes, for example, about all
kinds of groups can be

322
00:20:16.940 --> 00:20:21.110
wildly popular. It doesn't actually
mean they're true or helpful

323
00:20:21.860 --> 00:20:27.020
in terms of having kind
of a, the kind of,

324
00:20:28.610 --> 00:20:31.640
you know, fair society maybe
that we want to have

325
00:20:31.640 --> 00:20:35.840
for everyone. So I think,
you know, many times the

326
00:20:35.840 --> 00:20:40.520
challenge here is that people
use search engines as if

327
00:20:40.520 --> 00:20:44.630
they're, let's say an online
library, right? And so the

328
00:20:44.630 --> 00:20:47.090
kinds of things you might
find in a library would

329
00:20:48.340 --> 00:20:50.590
a big context. And I'll
tell you, I give this

330
00:20:50.590 --> 00:20:52.840
exercise to my students. I'll
say, you know, go to

331
00:20:52.840 --> 00:20:55.600
a library database and look
up any topic you care

332
00:20:55.600 --> 00:20:58.870
about anyone. It doesn't matter.
You pick something and then

333
00:20:58.870 --> 00:21:01.600
go to the stacks of
books and find that book

334
00:21:01.600 --> 00:21:04.330
that you're looking for. And
then tell them, come back

335
00:21:04.330 --> 00:21:07.270
and tell me something about
the context within which you

336
00:21:07.270 --> 00:21:10.300
found it. And so they'll
often find like, wow, you

337
00:21:10.300 --> 00:21:14.620
know, this subject I was
looking for, let's say, you

338
00:21:14.620 --> 00:21:19.060
know, they are looking for
something about LGBTQ identity, but

339
00:21:19.060 --> 00:21:23.320
they find it in a
big context of books about

340
00:21:23.320 --> 00:21:26.890
sexual deviance. And that tells
them something about the way

341
00:21:26.890 --> 00:21:30.970
in which for a long
time we've seen sexual identity,

342
00:21:31.000 --> 00:21:36.790
right, in this very scientific
clinical psychological sense. And then

343
00:21:36.790 --> 00:21:38.740
they see that that's a
point of view, kind of

344
00:21:38.740 --> 00:21:41.050
a subjective point of view
that has a long history,

345
00:21:41.230 --> 00:21:43.930
but that there's a lot
more to it. And now

346
00:21:43.960 --> 00:21:45.880
in a search engine, you're
not going to find a

347
00:21:45.880 --> 00:21:49.420
context for finding that those
things that come to the

348
00:21:49.420 --> 00:21:51.460
front page, you're not going
to be able to see

349
00:21:51.460 --> 00:21:55.390
what's influencing the subjective nature
of the thing that you're

350
00:21:55.390 --> 00:21:58.160
looking for. And so even
in the book, I, I

351
00:21:58.210 --> 00:22:01.660
propose kind of other metaphors
and other ways of making

352
00:22:01.660 --> 00:22:08.200
information context much more available
and visible to us again,

353
00:22:08.200 --> 00:22:12.640
rather than just assuming that
the things we find are

354
00:22:12.640 --> 00:22:15.820
an absolute truth. I think
we need to know more

355
00:22:15.820 --> 00:22:19.390
about the things we're looking
for and what might be

356
00:22:19.390 --> 00:22:23.370
influencing, why we're finding them.
It seems like early versions

357
00:22:23.370 --> 00:22:28.800
of the Google algorithm pre
2007 were a lot more

358
00:22:28.800 --> 00:22:31.470
well understood. And the, and
the giant black box, it

359
00:22:31.470 --> 00:22:35.160
was quite a less sophisticated
algorithm in the past. And

360
00:22:35.190 --> 00:22:37.860
there was this notion of
Google bombing where you could

361
00:22:37.860 --> 00:22:41.130
get enough people to quote
something and you could basically

362
00:22:41.130 --> 00:22:44.370
prank the internet. So you'd
go and search for French

363
00:22:44.400 --> 00:22:47.850
military victories and you'd click
I'm feeling lucky. And suddenly

364
00:22:47.850 --> 00:22:50.070
you'd find yourself on a
fake version of Google that

365
00:22:50.070 --> 00:22:54.000
says, did you mean French
military defeats? And that's, you

366
00:22:54.000 --> 00:22:56.430
know, that's cute up to
a point until you realize

367
00:22:56.430 --> 00:23:00.090
you can entirely game the
system and put falsities in

368
00:23:00.090 --> 00:23:04.530
front of searches for crews.
Well, we know this happens

369
00:23:04.530 --> 00:23:08.040
quite a bit and, you
know, in the early days

370
00:23:08.040 --> 00:23:09.810
when I've been on the
internet for a long time,

371
00:23:09.900 --> 00:23:13.530
and, you know, we used
to have how we found

372
00:23:13.530 --> 00:23:17.430
information was we had a
lot of expert communities who

373
00:23:17.430 --> 00:23:23.640
might organize, you know, bodies
of information. You could go

374
00:23:23.640 --> 00:23:27.630
into parts, you know, kind
of the bulletin board system

375
00:23:27.660 --> 00:23:30.840
of the internet and, and
find people ask for expertise.

376
00:23:30.840 --> 00:23:34.020
There might've been people who
really were teachers or experts

377
00:23:34.410 --> 00:23:38.880
in an area. You know,
we, when we moved to

378
00:23:38.880 --> 00:23:43.530
this kind of white opaque
page where you could put

379
00:23:43.530 --> 00:23:46.790
in a search term and
then kind trust that the

380
00:23:46.790 --> 00:23:49.130
mechanics or the logics behind
it, we're going to give

381
00:23:49.130 --> 00:23:52.250
you the best thing. We've
removed a layer of kind

382
00:23:52.250 --> 00:23:56.840
of again, understanding that there
might be many points of

383
00:23:56.840 --> 00:24:02.480
view and many experts involved
in helping us find what

384
00:24:02.480 --> 00:24:04.790
we're looking for. So now
we, we, you know, we

385
00:24:04.790 --> 00:24:08.150
have a whole generation of
people who have never experienced

386
00:24:08.420 --> 00:24:12.080
that kind of library ask
type of search and have

387
00:24:12.080 --> 00:24:15.530
moved to this rather opaque
type of search. And I

388
00:24:15.530 --> 00:24:19.970
think, you know, we know
that there are people and

389
00:24:19.970 --> 00:24:23.660
organizations who often try to
game the system. There was

390
00:24:23.660 --> 00:24:27.380
a great story and by
great, I mean, terrible story

391
00:24:27.380 --> 00:24:31.790
in the guardian a few
months ago, about how white

392
00:24:31.790 --> 00:24:36.980
nationalists had taken over Boise
and anthropology. And of course

393
00:24:36.980 --> 00:24:41.090
we've seen white power and
right, white pride organizations take

394
00:24:41.090 --> 00:24:46.580
over different kinds of keywords
that would lead people to

395
00:24:46.640 --> 00:24:52.940
a Holocaust denial sites, right.
Just flat out disinflation sites.

396
00:24:53.120 --> 00:24:54.710
And I think those are
the kinds of things that

397
00:24:54.710 --> 00:24:58.010
again, are very difficult to
see. You really don't know

398
00:24:58.010 --> 00:25:01.280
that's happening. And, and, you
know, every once in a

399
00:25:01.280 --> 00:25:04.610
while, the news media will
pick up, for example, stories

400
00:25:04.610 --> 00:25:09.830
about students writing papers on
dr. King and, you know,

401
00:25:10.010 --> 00:25:15.560
providing a lot of really
terrible misinformation disinformation about dr.

402
00:25:15.560 --> 00:25:20.630
King, you know, and what
they find is that Stormfront

403
00:25:20.870 --> 00:25:24.590
and other organizations have been
able to, co-opt dr. King's

404
00:25:24.590 --> 00:25:29.360
name on the internet, and
basically try to discredit civil

405
00:25:29.360 --> 00:25:33.020
rights and the civil rights
movement. So I think those

406
00:25:33.020 --> 00:25:34.910
are the kinds of things.
These are very, like, I

407
00:25:34.910 --> 00:25:39.860
think it's fairly uncontested in
our modern democracy in 2019

408
00:25:39.860 --> 00:25:43.160
that the civil rights movement
was probably a good thing

409
00:25:43.190 --> 00:25:46.910
for our society. And yet
we still find people who

410
00:25:46.910 --> 00:25:52.460
fall into these crevices of
the web, where they're led

411
00:25:52.460 --> 00:25:57.710
to really harmful hateful, antidemocratic
information. And I think those

412
00:25:57.710 --> 00:25:59.750
are things we should, you
know, we should definitely care

413
00:25:59.750 --> 00:26:02.990
about, especially as our kids
are engaging with this content

414
00:26:03.020 --> 00:26:07.870
often quite unsupervised. Yeah. In
your book, you talk about,

415
00:26:07.870 --> 00:26:10.240
since you work in an
academia and you think about

416
00:26:10.240 --> 00:26:12.700
things in a scholarly way
with other scholars, you talk

417
00:26:12.700 --> 00:26:16.390
about the importance of citations.
I spent all those years

418
00:26:16.390 --> 00:26:19.210
in school and in college
with footnotes for everything and

419
00:26:19.210 --> 00:26:22.270
citing papers so that you
had a web of trust,

420
00:26:22.690 --> 00:26:24.910
but that web of trust
doesn't seem to scale to

421
00:26:24.910 --> 00:26:28.120
the actual web. Well, you
know, in the early days

422
00:26:28.180 --> 00:26:31.360
of Google in particular, but
I don't think just Google,

423
00:26:31.960 --> 00:26:36.730
you know, this idea of
hyperlinking to other sites was

424
00:26:36.730 --> 00:26:43.320
a way of signaling credibility
and trustworthiness in search. And

425
00:26:43.320 --> 00:26:45.690
so if I had a
site and a lot of

426
00:26:45.690 --> 00:26:47.940
people were linking to it
and pointing to it and

427
00:26:47.940 --> 00:26:50.850
saying, you should go to,
you know, a Sophia noble

428
00:26:50.850 --> 00:26:53.760
site, then, you know, that's,
that's something you can trust.

429
00:26:53.760 --> 00:26:56.520
That was something that was
available to us as a

430
00:26:56.520 --> 00:26:59.820
kind of practice of a
citation that was quite frankly

431
00:26:59.820 --> 00:27:03.810
borrowed from the field of
library and information science. You

432
00:27:03.810 --> 00:27:08.280
know, what scholars do now,
you have other kinds of

433
00:27:08.310 --> 00:27:14.340
imperatives like purchasing keywords and
trying to optimize content. And

434
00:27:14.340 --> 00:27:18.150
we have a huge, great
industry of search engine optimization

435
00:27:18.150 --> 00:27:22.440
and advertising companies that are,
that spend, you know, millions

436
00:27:22.440 --> 00:27:25.020
and millions of dollars trying
to optimize their content to

437
00:27:25.020 --> 00:27:28.110
the first page. Mostly because
most people don't go past

438
00:27:28.110 --> 00:27:31.560
the first page when they're
using search engines. So what

439
00:27:31.560 --> 00:27:36.240
lands there is incredibly important
and Google and big commercial

440
00:27:36.240 --> 00:27:40.770
search engine platforms are always
trying to, you know, keep

441
00:27:40.830 --> 00:27:45.180
their, their own algorithms from
being kind of subverted by

442
00:27:45.180 --> 00:27:50.850
this SEO industry. So now
you have, according to, you

443
00:27:50.850 --> 00:27:55.110
know, Google zone search lab,
over 200 different factors that

444
00:27:55.110 --> 00:28:00.780
go into helping kind of
their AI decide, you know,

445
00:28:01.440 --> 00:28:04.710
what to bring to the
first page. And oftentimes one

446
00:28:04.710 --> 00:28:08.730
of the things you'll find
is that large advertisers, large,

447
00:28:08.760 --> 00:28:11.790
you know, the clients of
theirs are show up on

448
00:28:11.790 --> 00:28:13.890
the first page. It's why,
when we look for certain,

449
00:28:13.890 --> 00:28:16.860
for example, news, we're probably
going to be led to

450
00:28:16.890 --> 00:28:22.740
very large news organizations rather
than say a teeny local,

451
00:28:22.860 --> 00:28:26.670
you know, campus university, campus
newspaper, or some very small

452
00:28:26.670 --> 00:28:30.420
community paper, right. We're going
to find the large platforms.

453
00:28:30.570 --> 00:28:32.790
So that kind of, you
know, the same with, you

454
00:28:32.790 --> 00:28:35.610
know, for years of, you
know, if you're looking for

455
00:28:35.790 --> 00:28:39.120
women's clothing, you're going to
be led mostly to, you

456
00:28:39.120 --> 00:28:42.270
know, large, you know, companies
that spend a lot of

457
00:28:42.270 --> 00:28:46.500
money to be made visible
around women's clothing. So ultimately,

458
00:28:46.500 --> 00:28:51.690
you know, it's advertising that
is driving search engines, not

459
00:28:51.720 --> 00:28:56.820
necessarily a quest for truth
or curated cited, you know,

460
00:28:56.820 --> 00:28:59.910
trustworthy information, which might be
something that you find more

461
00:28:59.910 --> 00:29:03.000
so in a library. And
I think Google search is

462
00:29:03.060 --> 00:29:08.190
excellent for commercial content. You
know, when you're looking to

463
00:29:08.190 --> 00:29:10.620
buy products and services, it's
very good for that. Or

464
00:29:10.620 --> 00:29:14.880
if you're looking for, you
know, certain types of undisputed

465
00:29:14.880 --> 00:29:17.340
facts, you know, it might
send you to Wikipedia first,

466
00:29:17.730 --> 00:29:23.340
but in many cases, it's,
it can be quite deceiving,

467
00:29:23.370 --> 00:29:27.360
whether you should trust, you
know, advertising as a primary

468
00:29:28.410 --> 00:29:32.810
source of information for making
good decisions. And you point

469
00:29:32.810 --> 00:29:35.420
out something very interesting that
I think children should understand

470
00:29:35.420 --> 00:29:38.900
is that they think that
Google and that search engines

471
00:29:38.900 --> 00:29:41.950
are part of the internet.
They are synonymous for the

472
00:29:41.950 --> 00:29:45.130
internet and they are treated
almost as a, a public

473
00:29:45.130 --> 00:29:49.540
utility, but they have interests.
They are a company that

474
00:29:49.540 --> 00:29:52.570
need to keep the lights
on. So as such, you

475
00:29:52.570 --> 00:29:54.670
need to kind of take
those Google results and put

476
00:29:54.670 --> 00:29:57.220
them in context. And that's
asking a lot, cause we

477
00:29:57.310 --> 00:30:01.770
search dozens of times a
day. It's true. And you

478
00:30:01.770 --> 00:30:05.880
know, even with my own
family, you know, I find

479
00:30:05.880 --> 00:30:08.940
my second grader saying, well,
mom, why don't we just

480
00:30:08.940 --> 00:30:11.070
Google it? And you know,
of course he doesn't know,

481
00:30:11.070 --> 00:30:13.110
his mom is just kind
of really written this book,

482
00:30:13.830 --> 00:30:17.550
you know, offering a set
of alternatives to just Googling

483
00:30:17.550 --> 00:30:19.110
it all the time. And
I say, well, why don't

484
00:30:19.110 --> 00:30:21.390
you instead, if you're interested
in that topic, why don't

485
00:30:21.390 --> 00:30:24.120
you talk to your librarian
at school? Or why don't

486
00:30:24.120 --> 00:30:26.790
you ask your teacher about
it? Why don't you call

487
00:30:26.790 --> 00:30:30.420
your grandparents? You know, why
don't you get multiple points

488
00:30:30.420 --> 00:30:32.700
of view? Your grandparents might
know something about that cause

489
00:30:32.700 --> 00:30:34.950
they were alive during that
time or they were young

490
00:30:34.950 --> 00:30:37.290
during that time. Do you
know, like even in my

491
00:30:37.290 --> 00:30:41.760
own home, I have to,
you know, try to teach

492
00:30:41.970 --> 00:30:45.960
my, my son out of
some of those habits, because

493
00:30:45.960 --> 00:30:48.120
I want him to kind
of learn that there are

494
00:30:48.120 --> 00:30:50.670
other ways of knowing besides
just going to a search

495
00:30:50.670 --> 00:30:54.540
engine. And I think that,
you know, again, a lot

496
00:30:54.540 --> 00:30:57.870
is to be learned and,
you know, society and ideas

497
00:30:58.080 --> 00:31:01.980
they're complex. And you know,
one of the things that

498
00:31:01.980 --> 00:31:04.620
I offer up as a
kind of in the conclusion

499
00:31:04.620 --> 00:31:06.990
of the book is that,
you know, I don't think

500
00:31:06.990 --> 00:31:11.520
the answer is getting rid
of Google. I actually think

501
00:31:11.520 --> 00:31:19.500
more competition, more search engines,
more investment in public libraries,

502
00:31:19.830 --> 00:31:26.250
public media, public education, public
universities is really kind of

503
00:31:26.340 --> 00:31:31.380
serves as the right way
to bolster our democracy. It's

504
00:31:31.380 --> 00:31:34.020
not that commercial search engines
don't have a role to

505
00:31:34.020 --> 00:31:37.080
play that can be helpful
in sorting through a lot

506
00:31:37.500 --> 00:31:40.500
of what's on the internet.
But I think to seed

507
00:31:40.530 --> 00:31:46.260
so much of our educational
space and our knowledge space

508
00:31:46.440 --> 00:31:50.850
to advertising platforms is probably
not in the best interest

509
00:31:50.880 --> 00:31:53.880
of having a well-informed democracy.
And so those are the

510
00:31:53.880 --> 00:31:57.060
kinds of things that I
really try to argue for

511
00:31:57.060 --> 00:31:59.940
is that, you know, it's
not just enough to have

512
00:31:59.940 --> 00:32:05.030
public policy and regulation around
big tech firms, social media

513
00:32:05.030 --> 00:32:08.310
and artificial intelligence firms. But
we also have to bolster

514
00:32:08.310 --> 00:32:14.370
that with deeper investments in
more democratic knowledge institutions like

515
00:32:14.370 --> 00:32:18.300
libraries. So as we reached
the conclusion of this show,

516
00:32:18.330 --> 00:32:21.390
let me ask you one
final, somewhat loaded question. Now

517
00:32:21.390 --> 00:32:24.420
that it's 2019. And if
we go to Google images

518
00:32:24.420 --> 00:32:26.340
and we search for black
girls or white girls or

519
00:32:26.340 --> 00:32:32.460
Asian girls, we see largely
positive representations that appear to

520
00:32:32.460 --> 00:32:36.570
me to be from the,
the rise of, of positivity

521
00:32:36.570 --> 00:32:40.400
and, and user generated content.
There's a lot very enthusiastic

522
00:32:41.180 --> 00:32:45.290
Beauty bloggers and people blogging
about, you know, pretty, pretty

523
00:32:45.290 --> 00:32:48.350
this and beauty that and
positiveness and these people rock

524
00:32:48.350 --> 00:32:51.880
and those people rock. Are
we affecting Google? Did, did

525
00:32:51.890 --> 00:32:55.190
we, did we, the community
of positivity affect them or

526
00:32:55.190 --> 00:32:59.660
did they change their algorithm
or both? It's both. I

527
00:32:59.660 --> 00:33:04.550
think I can tell you
that as I've seen kind

528
00:33:04.550 --> 00:33:09.860
of my own critiques, get
more attraction and visibility and

529
00:33:09.920 --> 00:33:14.630
other scholars who are, and
people on social media and

530
00:33:14.630 --> 00:33:18.920
journalists are saying, Hey, wait
a minute, big tech companies,

531
00:33:19.130 --> 00:33:23.360
this, these aren't fair representations,
just like other mass media

532
00:33:23.360 --> 00:33:27.260
of the past television and
radio kind of demands for

533
00:33:27.260 --> 00:33:33.530
better representation. I see tech
companies responding quietly in their

534
00:33:33.530 --> 00:33:39.470
own ways to tweaking their
algorithms and putting more people

535
00:33:39.470 --> 00:33:42.470
kind of on the case
of caring about these things.

536
00:33:42.470 --> 00:33:44.540
And so I think it's
a bit of a symbiotic

537
00:33:44.540 --> 00:33:49.010
relationship between the public and
critics and also people who

538
00:33:49.010 --> 00:33:51.050
work in those companies. I
know a lot of people

539
00:33:51.050 --> 00:33:55.220
who work in tech companies
who care deeply about these

540
00:33:55.220 --> 00:34:00.560
issues and who are also
inside those companies saying, Hey,

541
00:34:00.770 --> 00:34:03.230
we, we need to do
better. We need to care

542
00:34:03.230 --> 00:34:07.220
about misrepresentative information in the
search engine. We need to

543
00:34:07.220 --> 00:34:12.500
care about disinformation and hate
information and hate speech and

544
00:34:12.500 --> 00:34:16.340
social media. And I think
there are, you know, we're

545
00:34:16.340 --> 00:34:20.960
at a point now where
we can be in a

546
00:34:20.990 --> 00:34:26.570
more open conversations finally about
what's at stake when we

547
00:34:26.570 --> 00:34:30.230
aren't having the conversations. And
I see that as progress

548
00:34:30.230 --> 00:34:33.560
for sure. Fantastic. Well, thank
you so much for chatting

549
00:34:33.560 --> 00:34:37.850
with me today. You're welcome.
Thanks for the invitation. You

550
00:34:37.850 --> 00:34:42.320
can learn all about Sophia's
work at Sophia. You noble.

551
00:34:42.350 --> 00:34:44.630
I'll put a link in
the show notes and you

552
00:34:44.630 --> 00:34:47.000
can read her book as
well as her other books

553
00:34:47.000 --> 00:34:50.870
and other expertise and research.
As we explore these topics,

554
00:34:51.410 --> 00:34:54.380
this has been another episode
of Hanselminutes and we'll see

555
00:34:54.380 --> 00:35:15.820
you again. <inaudible>.

