WEBVTT FILE

1
00:00:00.510 --> 00:00:03.660
Hi, this is Scott. I
really appreciate our sponsors because

2
00:00:03.660 --> 00:00:06.600
they make the show possible.
Today's show is sponsored by

3
00:00:06.600 --> 00:00:10.800
developer express, become a UI
superhero with dev express controls

4
00:00:10.800 --> 00:00:15.330
and libraries. Deliver elegant.net solutions
that address customer needs today

5
00:00:15.720 --> 00:00:19.800
by leveraging your existing knowledge,
you can build next generation

6
00:00:19.800 --> 00:00:23.910
touch enabled solutions for tomorrow,
you can download your free

7
00:00:24.000 --> 00:00:48.750
30 day trial@dxdothanselminutes.com. That's dx.hanselminutes.com
from hanselminutes.com. It's Hansel minutes,

8
00:00:49.200 --> 00:00:53.490
a weekly discussion with web
developer and technologist Scott Hanselman.

9
00:00:53.850 --> 00:00:58.890
This is Lawrence Ryan announcing show
number 550. In this episode,

10
00:00:58.890 --> 00:01:03.450
Scott talks with trust CEO,
Everett Harper about infrastructure realism,

11
00:01:03.780 --> 00:01:06.270
not about how applying some
old ideas and new ways

12
00:01:06.480 --> 00:01:13.320
help them fix healthcare.gov. Hi,
this is Scott Hanselman. This

13
00:01:13.320 --> 00:01:15.990
is another episode of Hansel
minutes. And today I'm talking

14
00:01:15.990 --> 00:01:19.170
with Everett Harper. He's the
CEO and founder at trusts.

15
00:01:19.260 --> 00:01:21.480
How are you, sir? I
am great. And thanks for

16
00:01:21.480 --> 00:01:23.310
having me on your show.
Yeah, I am happy to

17
00:01:23.310 --> 00:01:26.220
chat with you today. So
we're going to talk about

18
00:01:26.250 --> 00:01:29.880
infrastructure realism, but talk to
me about trust first, you

19
00:01:29.880 --> 00:01:32.760
founded the idea, the original
DNA for this was around

20
00:01:32.760 --> 00:01:36.000
2011. How long have you
been around? Well, I've been

21
00:01:36.000 --> 00:01:39.660
personally around for 50 years,
but, but the company has

22
00:01:39.660 --> 00:01:43.260
been around since 2011. I
think we really hit our

23
00:01:43.260 --> 00:01:50.120
stride around the time that
healthcare.gov was breaking. And that's

24
00:01:50.120 --> 00:01:52.200
sort of one of the
people that kind of helped

25
00:01:52.890 --> 00:01:57.540
save that and work with
other great people to get

26
00:01:57.540 --> 00:01:59.940
that up and running and
provide a lot of people

27
00:01:59.940 --> 00:02:02.580
with healthcare. So, and now
we have 20 people we're

28
00:02:02.580 --> 00:02:08.580
based in San Francisco in
Oakland, and we help do

29
00:02:08.580 --> 00:02:12.860
software infrastructure. We consult with
federal agencies, large companies doing

30
00:02:12.930 --> 00:02:17.010
transformation and startups that are
scaling and really proud that

31
00:02:17.010 --> 00:02:21.210
we have 50% women who
are engineers on our team.

32
00:02:21.900 --> 00:02:23.280
And it speaks to a
lot of the things that

33
00:02:23.280 --> 00:02:26.340
we'll be talking about during
the podcast. Yeah, that's great.

34
00:02:26.610 --> 00:02:28.230
I just sent you, your
VP of engineering is a,

35
00:02:28.230 --> 00:02:30.750
is a woman as well.
She's one of the founders

36
00:02:31.410 --> 00:02:37.050
brilliant. And my other co
founders is they're both technical

37
00:02:37.050 --> 00:02:40.260
and I'm the nontechnical head
of a technical company, which

38
00:02:40.260 --> 00:02:43.860
is amusing in many respects.
That's great. Well, I've, I've

39
00:02:43.860 --> 00:02:46.530
gone and followed Jennifer leech,
your VP of engineering and

40
00:02:46.530 --> 00:02:49.140
founder and Mark for his
last name's for allotting, for

41
00:02:49.140 --> 00:02:53.130
that a lot as well
on Twitter. So you, you

42
00:02:53.130 --> 00:02:55.320
said that you just kinda,
you saved, you helped save

43
00:02:55.770 --> 00:02:58.170
healthcare.gov, but you said the
way that you phrased it

44
00:02:58.170 --> 00:03:00.070
was that you helped start
up scale. And I think

45
00:03:00.070 --> 00:03:02.770
That healthcare.gov was a kind
of a startup wasn't it?

46
00:03:02.770 --> 00:03:05.460
That was having trouble scaling.
In some ways it was

47
00:03:05.460 --> 00:03:08.850
the worst of both worlds.
It was a startup in

48
00:03:08.850 --> 00:03:11.460
that it was trying to
get something new done, but

49
00:03:11.460 --> 00:03:17.130
as being run by a
big entity, which had a

50
00:03:17.130 --> 00:03:24.270
history, more of doing command
and control procedures, and unfortunately

51
00:03:24.270 --> 00:03:28.500
those things clashed and when
they launched it, it didn't

52
00:03:28.500 --> 00:03:33.900
work, which is sort of
amazing. But quickly there's a

53
00:03:33.900 --> 00:03:36.210
couple of leaders in the
white house at USDS and

54
00:03:36.210 --> 00:03:42.390
18 F who were able
to recognize, we actually need

55
00:03:42.750 --> 00:03:46.410
a small team of lead
engineers. And my co founder,

56
00:03:46.410 --> 00:03:48.720
Mark Philip was one of
them. And they went in

57
00:03:48.720 --> 00:03:50.640
and did some amazing work
in a very, very short

58
00:03:50.640 --> 00:03:53.700
period of time. And was
there a private, was it

59
00:03:53.700 --> 00:03:55.890
just a matter of like
find the smartest people? And

60
00:03:55.890 --> 00:03:57.690
then you just, you know,
they, they drop in like

61
00:03:57.690 --> 00:04:00.360
mission impossible from the ceiling
and then the ninjas save

62
00:04:00.370 --> 00:04:05.280
everything. Or was it a
method that made this possible?

63
00:04:05.340 --> 00:04:08.370
Was it geniuses or was
it a way of thinking

64
00:04:09.660 --> 00:04:12.000
Was, was I think it's
actually a little bit of

65
00:04:12.000 --> 00:04:15.030
both. There was not a
genius in the corner who

66
00:04:15.030 --> 00:04:18.120
goes and fixes the whole
thing into it was gathering

67
00:04:18.120 --> 00:04:21.480
people who have a lot
of experience building modern systems

68
00:04:22.110 --> 00:04:25.680
who had the humility and
the passion to take their

69
00:04:25.680 --> 00:04:28.890
time out of their normal
jobs, because this was something

70
00:04:28.890 --> 00:04:31.710
important for the country. So
there's a mindset there and

71
00:04:31.710 --> 00:04:34.560
the ability to work together
and figure out who has

72
00:04:34.560 --> 00:04:37.680
the expertise and how can
I help. That probably was

73
00:04:37.680 --> 00:04:41.130
the biggest thing that made
it and drove the success

74
00:04:41.130 --> 00:04:44.280
of that project. W w
when I think of the

75
00:04:44.280 --> 00:04:48.300
government, I tend to think
about silos, silos of information,

76
00:04:48.810 --> 00:04:51.540
not information sharing. I think
of silos of groups, of

77
00:04:51.540 --> 00:04:55.260
people while the government's this
big giant system. It does

78
00:04:55.260 --> 00:05:00.750
seem to be very siloed.
It can be, I think

79
00:05:01.200 --> 00:05:05.700
there are places in agencies
where there's some very big

80
00:05:05.730 --> 00:05:07.920
distinctions and very big differences.
So I don't want to

81
00:05:08.610 --> 00:05:11.160
cast the entire government that
way. Of course, I do

82
00:05:11.160 --> 00:05:17.400
think there are some interesting
moves afoot where the systems

83
00:05:17.400 --> 00:05:21.360
of collaboration and communication are
starting to be recognized as

84
00:05:21.360 --> 00:05:25.410
the way to go forward,
rather than holding things back

85
00:05:25.410 --> 00:05:29.430
and not releasing information to
other groups who could potentially

86
00:05:29.430 --> 00:05:33.630
help or are affected by
it. When, when you're thinking

87
00:05:33.630 --> 00:05:38.610
about something as big as
healthcare.gov, it was not just

88
00:05:39.000 --> 00:05:42.960
one system, but multiple systems
interacting with each other, trying

89
00:05:42.960 --> 00:05:46.080
to present that information in
a simple way to the,

90
00:05:46.130 --> 00:05:48.540
to the user. So from
a user focused design perspective

91
00:05:48.540 --> 00:05:50.370
is go to a website
and sign up. But the

92
00:05:50.370 --> 00:05:52.440
back end of that and
all the systems behind it,

93
00:05:52.890 --> 00:05:55.830
weren't just systems of computers,
but there were systems of

94
00:05:55.830 --> 00:06:00.740
people and processes. Yeah. So,
and that system is really

95
00:06:00.740 --> 00:06:04.010
the core of where we
think about infrastructure realism. And

96
00:06:05.060 --> 00:06:10.400
you had multiple vendors, one
agency relying on multiple streams

97
00:06:10.400 --> 00:06:13.160
of information to do something
that should be actually relatively

98
00:06:13.160 --> 00:06:18.590
simple. Go here, sign up,
thumbs up. Thank you. And

99
00:06:18.620 --> 00:06:24.770
because people looked at the
problem as I do my

100
00:06:24.770 --> 00:06:28.040
little thing I handed off
and I'm done rather than

101
00:06:28.040 --> 00:06:30.650
looking at it at a
system level that, Hey, all

102
00:06:30.650 --> 00:06:32.870
these things need to work
together. And by the way,

103
00:06:33.380 --> 00:06:35.420
when you put them together,
we should test whether it

104
00:06:35.420 --> 00:06:40.700
works, that flipped when we
and other people came in.

105
00:06:41.270 --> 00:06:44.960
And so for me, the
whole infrastructure realism perspective is,

106
00:06:45.170 --> 00:06:49.100
is looking at systems of
interactions and is required to

107
00:06:49.100 --> 00:06:53.510
operate a society or an
enterprise. And in this case,

108
00:06:53.780 --> 00:06:56.630
the enterprise and society came
together in a really important

109
00:06:56.630 --> 00:07:00.710
way. And I think that
really galvanized my thinking around,

110
00:07:00.890 --> 00:07:03.740
Hey, this is something that
is urgent for us to

111
00:07:03.740 --> 00:07:07.280
talk about. So that's been
fun to kind of explore

112
00:07:07.280 --> 00:07:10.210
more, You know, it's interesting.
I know we are speaking

113
00:07:10.210 --> 00:07:13.060
in generalizations, so certainly we're
not trying to generalize all

114
00:07:13.060 --> 00:07:15.370
people. They're all engineers, or
certainly not all of the

115
00:07:15.370 --> 00:07:18.390
government, but I think it's
fair to say that, that

116
00:07:18.470 --> 00:07:21.130
we have worked with engineers
in the past that tend

117
00:07:21.130 --> 00:07:24.280
to want to compartmentalize. We're
taught as architects and software

118
00:07:24.280 --> 00:07:27.910
people to compartmentalize. And you
know, here's an interface by

119
00:07:27.910 --> 00:07:29.890
which you will talk to
me, Everett, and you will

120
00:07:29.890 --> 00:07:31.600
speak on this bus and
this is how we will

121
00:07:31.600 --> 00:07:33.370
speak. And you give me
this and I'll give you

122
00:07:33.370 --> 00:07:35.080
that. And I'll work on
my little thing and you

123
00:07:35.080 --> 00:07:37.510
work on your little thing
and then the larger system

124
00:07:37.510 --> 00:07:40.300
will, that's being architected by
someone else. I'm not going

125
00:07:40.300 --> 00:07:43.390
to worry about that. I'm
responsible for my, my thing.

126
00:07:43.750 --> 00:07:46.270
There's a lack of empathy
there though, for the, for

127
00:07:46.270 --> 00:07:48.040
the entire system and the
people who work in it.

128
00:07:48.700 --> 00:07:53.350
Yeah, Yeah, absolutely. And I
think that as a practice

129
00:07:53.680 --> 00:07:59.200
can be ameliorated by actually
asking the question across whatever

130
00:07:59.350 --> 00:08:02.770
interface and going to sit
next to the person who

131
00:08:02.770 --> 00:08:04.990
you're working with an operator
of a system and saying,

132
00:08:04.990 --> 00:08:10.150
Hey, what do you do?
Show me how you work.

133
00:08:11.080 --> 00:08:14.620
Show me the things that
drive you nuts. And tell

134
00:08:14.620 --> 00:08:17.350
me about the things that
you get excited about. What

135
00:08:17.350 --> 00:08:21.040
happens is a people love
to talk about what they

136
00:08:21.040 --> 00:08:25.150
know, right? B they get
excited. Like someone actually asked

137
00:08:25.150 --> 00:08:28.210
me that I know something.
In fact, I do know

138
00:08:28.210 --> 00:08:33.430
a lot of things. And
then they will tell you

139
00:08:33.490 --> 00:08:36.130
so much information. And then
as a person who is

140
00:08:36.130 --> 00:08:38.650
a consultant or a leader,
when you start to see

141
00:08:38.650 --> 00:08:42.580
those patterns, you start to
see, Oh, actually we don't

142
00:08:42.580 --> 00:08:45.520
have to talk through this
interface. We can do something

143
00:08:45.520 --> 00:08:49.780
very different. We can work
together and our communication can

144
00:08:49.780 --> 00:08:52.390
be trivial because of the
way we work together. It

145
00:08:52.390 --> 00:08:57.240
casts off enough notifications that
we don't actually need to

146
00:08:57.240 --> 00:09:00.690
sit and talk through email
or talk through certain interfaces.

147
00:09:00.810 --> 00:09:02.940
We can actually see each
other working. How cool is

148
00:09:02.940 --> 00:09:07.830
that? But more, most importantly,
it sort of shows a

149
00:09:07.830 --> 00:09:11.640
basic level of respect for
someone's knowledge and the ability

150
00:09:11.640 --> 00:09:15.510
for them to show what
they know that develops trust.

151
00:09:15.510 --> 00:09:18.300
And when you have that
kind of trust, the kind

152
00:09:18.300 --> 00:09:21.690
of problems that result from
silos start to melt away,

153
00:09:23.450 --> 00:09:27.140
It seems also that you're
going to the source of

154
00:09:27.140 --> 00:09:31.910
knowledge there and respecting the
frontline people. Absolutely. If you're

155
00:09:31.910 --> 00:09:33.650
going to, like, let's say
you hired me to go

156
00:09:33.650 --> 00:09:36.620
and build a call center.
It would be ridiculous for

157
00:09:36.620 --> 00:09:38.930
me not to sit with
people at the front lines,

158
00:09:38.930 --> 00:09:41.420
picking up the phones and
understand who they are and

159
00:09:41.420 --> 00:09:44.240
how they think and how
they work. Yeah. I think

160
00:09:44.310 --> 00:09:48.200
the contribution that, you know,
companies like IDEO with design

161
00:09:48.200 --> 00:09:52.160
thinking in particular made was,
Hey, go to the customers,

162
00:09:52.190 --> 00:09:55.190
look at customers, work walking
in stores or interface or

163
00:09:55.220 --> 00:10:01.490
interacting with, with computer systems
and so forth. And that

164
00:10:02.210 --> 00:10:05.510
idea I'm sort of saying,
you actually also have to

165
00:10:05.510 --> 00:10:09.080
think about the operators of
the systems and delivering those

166
00:10:09.080 --> 00:10:12.770
services, your example of a
call centers, perfect. And who

167
00:10:12.770 --> 00:10:16.490
developed the software that the
call center is operating on?

168
00:10:17.180 --> 00:10:20.120
How can you make their
job easier? How can you

169
00:10:20.120 --> 00:10:24.110
have them deal with higher
level problems? Well, you can

170
00:10:24.110 --> 00:10:25.790
actually sit there and sit
next to them and say,

171
00:10:25.790 --> 00:10:27.860
Hey, tell me how you
do what your work is.

172
00:10:28.340 --> 00:10:32.060
So it's taking that design
thinking idea and moving it

173
00:10:32.060 --> 00:10:35.480
back into people who develop
the backend systems, the operating

174
00:10:35.480 --> 00:10:39.770
systems, the development environments. And
there's a lot of knowledge

175
00:10:39.800 --> 00:10:46.550
captured there that I don't
think is, is being utilized

176
00:10:46.820 --> 00:10:48.820
as well as it could
be. And I think about

177
00:10:48.820 --> 00:10:52.610
our experience when you do
that, all sorts of new

178
00:10:52.670 --> 00:10:56.540
and innovative and exciting solutions
start to come up. Yeah.

179
00:10:57.080 --> 00:10:59.870
When I think of, of,
of running software, I start,

180
00:10:59.900 --> 00:11:03.080
I came up in the
last 20, 25 years just

181
00:11:03.080 --> 00:11:05.600
about when extreme programming, they
used to call it XP,

182
00:11:05.990 --> 00:11:08.660
and now we call it
agile, right. That idea of

183
00:11:08.660 --> 00:11:11.960
the active stakeholder, right. It
always seemed intuitive to me

184
00:11:11.960 --> 00:11:14.300
that the person who is
going to ultimately use the

185
00:11:14.320 --> 00:11:17.360
software should be an active
and fundamental stake stakeholder. And

186
00:11:17.660 --> 00:11:20.960
that the, the, the onsite
customer, right, the programmer should

187
00:11:20.960 --> 00:11:23.900
sit with the customer and
get constant feedback about whether

188
00:11:23.900 --> 00:11:27.530
or not they're doing the
right thing. Yeah. There's in

189
00:11:27.530 --> 00:11:30.230
a lot of ways, infrastructural
ism is really trying to

190
00:11:30.560 --> 00:11:32.510
take a lot of these
ideas that have been in

191
00:11:32.510 --> 00:11:36.020
existence for a long time
and apply them in a

192
00:11:36.020 --> 00:11:38.960
very particular way. And frankly,
in a really urgent way

193
00:11:40.010 --> 00:11:42.500
to deal with new problems
that are coming up, that

194
00:11:42.500 --> 00:11:47.630
we haven't seen before. So
where did this term infrastructure

195
00:11:47.630 --> 00:11:50.300
realism come from? Is there
a, have we gone all

196
00:11:50.300 --> 00:11:52.400
the way back and figure
out who had that idea

197
00:11:52.400 --> 00:11:58.540
or who coined that term?
There's a body of philosophical

198
00:11:58.540 --> 00:12:05.710
knowledge around structuralism and that's
existed. I think in the,

199
00:12:05.800 --> 00:12:09.640
in certainly the, by the
mid 20th century and infrastructure

200
00:12:09.640 --> 00:12:12.490
realism really is kind of
taking a little bit from

201
00:12:12.490 --> 00:12:17.170
that. And riffing off of
that, my inspiration, frankly, though,

202
00:12:17.560 --> 00:12:21.970
is coming from Robert Moses and
Jane Jacobs. And for folks who

203
00:12:21.970 --> 00:12:26.890
don't know these two did
battle in did battle in

204
00:12:26.890 --> 00:12:30.880
the 1950s, Robert Moses was the
urban planner for New York city

205
00:12:31.330 --> 00:12:34.240
was responsible for a lot
of the bridges. And his

206
00:12:34.240 --> 00:12:37.840
plan in the fifties in
a car centric culture was

207
00:12:37.840 --> 00:12:41.980
to put highways all through
New York city in particular, across

208
00:12:41.980 --> 00:12:45.190
Greenwich village and D and
wipe out Washington square park.

209
00:12:45.640 --> 00:12:49.420
Jane Jacobs was a resident of
Washington, I'm sorry, Everett Greenwich

210
00:12:49.420 --> 00:12:53.230
village. And she thought, wait
a minute, cities are magic

211
00:12:53.230 --> 00:12:56.170
because of the people that
are interacting on a block

212
00:12:56.170 --> 00:12:59.770
by block nook by Nick
level. And they have the

213
00:12:59.770 --> 00:13:03.670
ability to solve problems locally
because they are experiencing those

214
00:13:03.670 --> 00:13:07.210
problems. And the vibrancy that
comes from cities is from

215
00:13:07.210 --> 00:13:09.700
these people, figuring it out.
So you had a command

216
00:13:09.700 --> 00:13:13.930
and control and a what
I would call infrastructure perspective,

217
00:13:14.740 --> 00:13:19.870
doing battle Jane Jacobs, one, we
still have Washington square park.

218
00:13:20.440 --> 00:13:23.830
And the thing that's important
about this, I think is

219
00:13:23.830 --> 00:13:28.540
Moses, while he did amazing
things for New York city, he

220
00:13:28.540 --> 00:13:32.350
was essentially commanding control. And
unfortunately he could not have

221
00:13:32.350 --> 00:13:36.370
anticipated things like millennials, not
wanting to buy cars, or

222
00:13:36.370 --> 00:13:39.130
they wanting more connections or
people being back into cities,

223
00:13:39.130 --> 00:13:43.570
out of suburbs. Now we
have across the country roads

224
00:13:43.570 --> 00:13:46.690
and infrastructure and so forth,
that's crumbling that people don't

225
00:13:46.690 --> 00:13:49.390
use as much. And it's
only going to accelerate in

226
00:13:49.390 --> 00:13:51.850
the next 20 years as
you get autonomous cars and

227
00:13:51.850 --> 00:13:55.960
things like that. Jane Jacobs kind
of won the day. And

228
00:13:55.960 --> 00:14:00.580
I think understanding that the
lessons from that are things

229
00:14:00.580 --> 00:14:03.640
that we understand in software
and in building systems as

230
00:14:03.640 --> 00:14:06.430
well. So that's for me
is the fun part of,

231
00:14:06.490 --> 00:14:09.370
of was the inspiration for
the idea of infrastructural ism.

232
00:14:09.910 --> 00:14:13.500
Hmm. And, and certainly as
an aside, it's a really

233
00:14:13.500 --> 00:14:16.440
interesting book called the battle
for Gotham, that kind of

234
00:14:17.190 --> 00:14:20.970
expresses that, that battle between
Robert Moses and Jane Jacobs. And I

235
00:14:20.970 --> 00:14:22.620
think that the interesting part
of what, what you just

236
00:14:22.620 --> 00:14:24.720
said was the idea that
there are things that we

237
00:14:24.720 --> 00:14:29.910
couldn't have anticipated certainly in
the, in the sixties, early

238
00:14:29.910 --> 00:14:32.580
seventies, there was this idea
of that. The future that

239
00:14:32.580 --> 00:14:34.770
we expected in the fifties
was going to come to

240
00:14:34.770 --> 00:14:36.570
life in the seventies and
eighties. And we would have,

241
00:14:36.870 --> 00:14:39.720
you know, these perfect little
cars and working in perfect

242
00:14:39.720 --> 00:14:43.080
little grids and finding, you
know, in highway and international

243
00:14:43.260 --> 00:14:46.140
international weather Countrywide highway, and
everyone be able to get

244
00:14:46.140 --> 00:14:52.190
there smoothly. So certainly Jacobs
Moses rather thought it was

245
00:14:52.190 --> 00:14:54.260
going to be a beautiful
New York. If it had highways

246
00:14:54.260 --> 00:14:55.940
running all the way through
it we'd find, you know,

247
00:14:56.000 --> 00:14:58.640
it would be perfect, but
it, what it did, it

248
00:14:58.640 --> 00:15:02.500
didn't think about the people.
Nope, no people were going

249
00:15:02.500 --> 00:15:06.340
to adapt to that system
rather than the other way

250
00:15:06.340 --> 00:15:10.660
around systems adapt to people
and to context. And I

251
00:15:10.660 --> 00:15:15.220
think that's where the magic
is, is Jane Jacobs nor Moses

252
00:15:15.250 --> 00:15:19.630
could anticipate that problem. But
her perspective was at core

253
00:15:19.630 --> 00:15:23.860
adaptable. That's really, really good
point because now here we

254
00:15:23.860 --> 00:15:27.070
are in an environment where
it's 2016. And while I

255
00:15:27.100 --> 00:15:30.130
kind of felt like I
saw iPads coming, I really

256
00:15:30.130 --> 00:15:34.540
don't. Didn't think I saw
self driving cars coming. And,

257
00:15:34.550 --> 00:15:38.950
and now if we were
in an, an adaptable environment,

258
00:15:38.950 --> 00:15:42.280
we could potentially be in
trouble, but hopefully we'll, we'll

259
00:15:42.280 --> 00:15:44.380
figure it out. And we'll
design cities that will be

260
00:15:44.380 --> 00:15:46.210
able to deal with that.
I didn't see Uber coming.

261
00:15:46.300 --> 00:15:49.930
I don't know about, you
know Yeah. And things like

262
00:15:49.930 --> 00:15:52.540
climate change and the well
people saw it in the

263
00:15:52.540 --> 00:15:56.860
seventies. The acceleration, I think,
is taking everybody by surprise.

264
00:15:58.000 --> 00:16:00.820
Interesting. Does that brings up
your point that you mentioned

265
00:16:00.850 --> 00:16:04.630
earlier about urgency and it
comes out of nowhere sometimes,

266
00:16:04.630 --> 00:16:06.880
you know, there's a complex
problem, but then suddenly change

267
00:16:06.880 --> 00:16:11.500
accelerates and there's these unknown
factors that come up. Yeah.

268
00:16:12.460 --> 00:16:15.580
And I think so actually,
just to back up a

269
00:16:15.580 --> 00:16:18.310
second, I think that I
want to make sure that

270
00:16:18.310 --> 00:16:22.030
the idea behind a complex
problem versus a complicated problem

271
00:16:22.060 --> 00:16:26.050
gets talked about a little
bit. So a guy named

272
00:16:26.050 --> 00:16:30.190
Nassim, Nicholas Taleb wrote this
book, he talked about the

273
00:16:30.190 --> 00:16:34.000
black swans about these problems
that are extremely rare, can

274
00:16:34.000 --> 00:16:37.180
not be predicted advanced because
the infant testimony, small probability,

275
00:16:37.180 --> 00:16:43.840
but have extreme impact. And
typically people in retrospect say,

276
00:16:43.870 --> 00:16:45.700
Oh, we could have anticipated
that. So we're going to

277
00:16:45.700 --> 00:16:48.820
build all these systems to
prevent that problem. And they

278
00:16:48.820 --> 00:16:50.950
missed the point. The point
is, there are things you

279
00:16:50.950 --> 00:16:54.190
can't predict. So would you
do is you build systems

280
00:16:54.190 --> 00:16:57.670
and ways of thinking that
can adapt to change and

281
00:16:57.670 --> 00:17:01.330
adapt to complex problems. So
what's a complex problem. The

282
00:17:01.330 --> 00:17:04.750
best example is the difference
between building a seven 47.

283
00:17:04.750 --> 00:17:08.470
It's a complicated problem to
build a seven 47, but

284
00:17:08.470 --> 00:17:11.110
it's not a complex one
complex problems are things like,

285
00:17:11.110 --> 00:17:18.040
how do you design a
human society? There are multiple

286
00:17:18.040 --> 00:17:21.940
dependencies and so many dependencies
that is hard to predict

287
00:17:22.150 --> 00:17:25.930
when you interact one thing
with another what's going to

288
00:17:25.930 --> 00:17:31.180
happen. So things like the
financial crisis in 2008 is

289
00:17:31.180 --> 00:17:34.750
a perfect example of a
complex problem. There wasn't one

290
00:17:35.110 --> 00:17:39.610
single source of the problem,
but the impact was tremendous

291
00:17:40.210 --> 00:17:42.280
and we're still reverberating. We
were trying to figure out

292
00:17:42.280 --> 00:17:46.390
what to do with that.
Gotcha. So complex are things

293
00:17:46.390 --> 00:17:48.250
that have a lot of
components, a lot of parts,

294
00:17:48.250 --> 00:17:52.380
but it doesn't necessarily difficulty
the complicated. Definitely. Should have

295
00:17:52.380 --> 00:17:54.210
you thinking there's going to
be at the level of

296
00:17:54.210 --> 00:17:59.030
difficulty here? Yeah. I think
complicated is a known level

297
00:17:59.030 --> 00:18:05.540
of difficulty complex is an
unknown level of difficulty. And

298
00:18:05.750 --> 00:18:09.590
in trying to design a
system, if you don't appreciate

299
00:18:09.590 --> 00:18:14.480
the unknown part of it,
you'll probably build a system

300
00:18:14.480 --> 00:18:17.900
that is fragile or rigid.
And then it breaks when

301
00:18:17.900 --> 00:18:21.650
something that you don't anticipate
comes along. Okay. So then

302
00:18:21.650 --> 00:18:24.230
that maybe can bring us
to some real world examples.

303
00:18:24.560 --> 00:18:30.110
Sure. So there's a couple
examples that I can bring

304
00:18:30.110 --> 00:18:33.860
up just from, in my,
in my experience and, you

305
00:18:33.860 --> 00:18:36.020
know, working with trust and
so forth. So healthcare.gov is

306
00:18:36.020 --> 00:18:40.100
a really a good one
as we talked about earlier,

307
00:18:40.310 --> 00:18:44.180
lots of different inputs, but
couldn't, you should have been

308
00:18:44.180 --> 00:18:46.370
able to anticipate that you
need to test it before

309
00:18:46.370 --> 00:18:49.940
it doesn't work to test.
So it works when we

310
00:18:49.940 --> 00:18:54.020
went in, what problem do
you fix when everything's on

311
00:18:54.020 --> 00:18:56.870
fire? What do you put
the water on? First, what

312
00:18:56.870 --> 00:18:59.180
they did was sort of
set up a system that

313
00:18:59.180 --> 00:19:04.490
said, okay, let's first address
the issue of how decisions

314
00:19:04.490 --> 00:19:06.830
get made and who has
the ability to make those

315
00:19:06.830 --> 00:19:11.180
decisions. What they discovered was
engineers who really understood what

316
00:19:11.180 --> 00:19:13.550
the problem was had to
go through four levels of

317
00:19:13.700 --> 00:19:16.910
four or five levels of
approvals in order to be

318
00:19:16.910 --> 00:19:19.370
able to take an action.
Well, I'm sure everybody in

319
00:19:19.370 --> 00:19:23.840
your audience knows that's a
recipe for disaster. And so

320
00:19:23.930 --> 00:19:27.410
squishing that hierarchy and giving
an empowering engineers to solve

321
00:19:27.410 --> 00:19:31.460
problems that they see made
the complexity of all those

322
00:19:31.460 --> 00:19:35.690
decision trees reduce significantly. So
that was the first thing.

323
00:19:36.590 --> 00:19:41.300
The second thing was, well,
how do you choose which

324
00:19:41.300 --> 00:19:44.780
problem? Well, we took an
approach of risk, like creating

325
00:19:44.780 --> 00:19:48.440
a risk hierarchy. So what's
the thing, that's the biggest

326
00:19:48.440 --> 00:19:50.990
risk. Yeah. There's 17 things
on fire. Let's just pick

327
00:19:50.990 --> 00:19:54.260
the one that we believe
is the most risk address

328
00:19:54.260 --> 00:19:57.800
that this week, next week
we dressed the next level.

329
00:19:58.460 --> 00:20:00.710
And the measure of whether
you're doing the right thing

330
00:20:00.740 --> 00:20:03.860
is are you addressing a
higher level of problem this

331
00:20:03.860 --> 00:20:07.190
week than you did last
week, Mikey Dickerson sort of

332
00:20:07.190 --> 00:20:11.930
coined this almost like Maslow's
hierarchy of problem solving again,

333
00:20:11.930 --> 00:20:14.900
what it did was it
didn't say that we know

334
00:20:14.900 --> 00:20:17.780
what all the answers are,
where it did say is

335
00:20:17.780 --> 00:20:20.810
we can solve certain problems
and then look at the

336
00:20:20.810 --> 00:20:23.990
results and then solve the
next level of problems. And

337
00:20:23.990 --> 00:20:28.040
eventually became a practice that
empowered people to have momentum

338
00:20:28.250 --> 00:20:32.780
around solving something that seemed
insurmountable at the start and

339
00:20:32.780 --> 00:20:35.210
then started to be like,
Hey, I think we have

340
00:20:35.210 --> 00:20:38.480
a pattern here. I think
we have some, some progress

341
00:20:38.480 --> 00:20:42.860
that we're making. That was
an amazing collection of people

342
00:20:42.860 --> 00:20:45.320
who sort of were able
to do that and involve

343
00:20:45.350 --> 00:20:47.980
other people who were working
on problem for six months

344
00:20:47.980 --> 00:20:51.310
or 12 months beforehand and
engage them to help solve

345
00:20:51.310 --> 00:20:55.200
problems as well. How do
these, how did these, these

346
00:20:55.200 --> 00:20:57.720
I'm imagining these meetings happening
and people, I mean, all

347
00:20:57.720 --> 00:20:59.340
the, you know, the white
boarding and all the work

348
00:20:59.340 --> 00:21:01.920
that happens, the, the grunt
work, how does that then

349
00:21:01.920 --> 00:21:05.400
get communicated down throughout a
system so that everyone else

350
00:21:05.400 --> 00:21:07.260
feels that they have a
general direction to go and

351
00:21:07.260 --> 00:21:10.200
they understand the, the marching
orders for lack of a

352
00:21:10.200 --> 00:21:15.900
better word. So in the
general case, there are things,

353
00:21:15.960 --> 00:21:20.640
I mean, communication systems, whether
you use pivotal or jive

354
00:21:20.640 --> 00:21:25.290
to make the work you're
doing visible in a trivial

355
00:21:25.290 --> 00:21:30.300
way, Slack is brilliant for
the same reason. So that,

356
00:21:30.960 --> 00:21:34.140
and, and there's, there's many
others, but how in my

357
00:21:34.140 --> 00:21:36.120
mind is how do you
make the work that you're

358
00:21:36.120 --> 00:21:39.570
doing visible in a trivial
way that doesn't require you

359
00:21:39.570 --> 00:21:42.840
to put extra attention to
it? Cause like the core

360
00:21:42.840 --> 00:21:45.900
value here, right, is time
and attention. We all want

361
00:21:45.900 --> 00:21:49.230
our time and attention valued.
And if anything helps me

362
00:21:49.650 --> 00:21:52.110
put my time and attention
on higher level problems to

363
00:21:52.110 --> 00:21:58.200
solve, I feel great. The
second thing is really from

364
00:21:58.200 --> 00:22:03.270
sort of a leadership perspective,
which is saying a I'm

365
00:22:03.270 --> 00:22:08.880
listening B you don't have
to know everything. Like they're

366
00:22:08.880 --> 00:22:12.720
going to be mistakes made.
How quickly can you identify

367
00:22:12.720 --> 00:22:15.900
the mistake, communicate the mistake,
figure out a plan around

368
00:22:15.900 --> 00:22:19.650
the mistake and then ensure
that that same mistake doesn't

369
00:22:19.650 --> 00:22:26.460
repeat. There's a lot of
places where mistakes are punishable

370
00:22:27.510 --> 00:22:29.550
instead of encouraged, but you
don't want to make dumb

371
00:22:29.550 --> 00:22:33.150
mistakes course. Right? You're not
intending to, but when you

372
00:22:33.150 --> 00:22:38.340
have a culture that is
punishing all mistakes, how often

373
00:22:38.340 --> 00:22:40.410
do you think that leader
is hearing about mistakes that

374
00:22:40.410 --> 00:22:42.750
are in there in the,
that are happening on daily

375
00:22:42.750 --> 00:22:47.160
basis? That would be a
culture that would be incentivized

376
00:22:47.160 --> 00:22:49.320
on hiding mistakes. That's right.
That's right. And then you

377
00:22:49.320 --> 00:22:53.130
have a Wells Fargo situation.
Yeah. That's exactly right. That's

378
00:22:53.130 --> 00:22:56.460
exactly right. Yeah. I actually,
I, that makes me think

379
00:22:56.460 --> 00:22:57.810
about one of the great
things that I like about

380
00:22:57.810 --> 00:23:00.900
my boss was that he
said that if you're an

381
00:23:00.900 --> 00:23:02.730
actually a number of people
have said this, if you're

382
00:23:02.730 --> 00:23:04.800
not screwing up at least
twice a year and getting

383
00:23:04.800 --> 00:23:07.530
in trouble, then you're probably
not pushing hard enough. I'm

384
00:23:07.540 --> 00:23:10.860
trying to do your job.
That's right. And that, that

385
00:23:10.980 --> 00:23:15.360
attracts certain people. And it's
not something that it attracts

386
00:23:15.360 --> 00:23:17.760
certain people. And I think
it's an adaptable organization. They

387
00:23:17.760 --> 00:23:20.760
find with the edges and
then they bump up against

388
00:23:20.760 --> 00:23:24.030
it and okay, great. That
smacked me, right. My head

389
00:23:25.410 --> 00:23:28.110
let's duck next time. Okay,
cool. Great. So I'm hearing

390
00:23:28.110 --> 00:23:30.390
you say that people should
be, would be listening more

391
00:23:30.420 --> 00:23:33.300
as opposed to just declaring
from on high and commanding

392
00:23:33.300 --> 00:23:35.520
that this should be dollar
shall be done like this,

393
00:23:36.000 --> 00:23:38.820
and then this, the system
of constant improvement, but then

394
00:23:38.820 --> 00:23:41.340
setting up the system so
that it doesn't break when

395
00:23:41.340 --> 00:23:43.260
something does break and it
needs to be improved. And

396
00:23:43.260 --> 00:23:45.620
then you just, you know,
fail fast fail, but you

397
00:23:45.620 --> 00:23:49.450
always moving forward and always
getting back. Yeah. And you

398
00:23:49.450 --> 00:23:53.710
know, you're trying not to
fail, but you will. Right.

399
00:23:53.710 --> 00:23:56.770
That's just, that's the thing.
There's one of the other,

400
00:23:56.920 --> 00:23:58.990
other inspirations for me was
a guy named John is

401
00:23:58.990 --> 00:24:00.730
a guy named John <inaudible>,
who is a chancellor of

402
00:24:00.730 --> 00:24:04.150
Georgetown. And he was tasked
with building a 21st century

403
00:24:04.150 --> 00:24:07.480
university. And he said, huh,
I'm trying to build a

404
00:24:07.480 --> 00:24:12.490
21st century university with 200
years of academic history. And

405
00:24:12.490 --> 00:24:15.850
in 2000 years of religious
history, since it's a Jesuit

406
00:24:15.850 --> 00:24:19.090
university and his whole framework,
he boiled it down to

407
00:24:19.090 --> 00:24:24.430
this tolerance for uncertainty. Can
you say, I might make

408
00:24:24.430 --> 00:24:28.660
a mistake as a leader
and will you follow me

409
00:24:28.690 --> 00:24:32.950
anyway, we have this goal.
We have this, we have

410
00:24:32.950 --> 00:24:35.740
this vision. Do you trust
me enough as a leader?

411
00:24:36.370 --> 00:24:39.580
And can I develop systems
to make sure to, to

412
00:24:39.760 --> 00:24:43.630
earn that trust so that
as we make mistakes, you

413
00:24:43.630 --> 00:24:47.080
still will follow me and
you still, we feel solve

414
00:24:47.080 --> 00:24:50.650
this problem together. I think
that's a really amazing statement.

415
00:24:51.310 --> 00:24:55.300
And because it's embedded in
humility and it's also embedded

416
00:24:55.300 --> 00:24:57.460
in respect for the people
who are doing the daily

417
00:24:57.460 --> 00:25:01.420
work. And that for me
is sort of a core

418
00:25:01.810 --> 00:25:04.420
practice here where you're a
leader, not necessarily a manager,

419
00:25:04.420 --> 00:25:08.860
you're listening, you're not commanding,
you're looking for improvement and

420
00:25:08.860 --> 00:25:12.400
you're looking for a practice.
So that every day you're

421
00:25:12.400 --> 00:25:15.490
getting a little bit better.
It's such a challenge though,

422
00:25:15.490 --> 00:25:18.970
because I think particularly from
an American culture perspective, we

423
00:25:18.970 --> 00:25:23.860
do sometimes lean towards leaders
that show up and say,

424
00:25:23.860 --> 00:25:25.930
we've got it all figured
out, you know, here, look,

425
00:25:25.960 --> 00:25:30.370
follow me over that Hill,
but a more empathetic in

426
00:25:30.370 --> 00:25:32.800
the listener, you know, a
leader more than a manager

427
00:25:33.040 --> 00:25:36.100
is, is really what we
need. I think so. And

428
00:25:36.100 --> 00:25:39.820
I think I'd invite you
and others to sort of

429
00:25:39.820 --> 00:25:44.410
say, so, is that true
in your own experience? It

430
00:25:44.620 --> 00:25:50.020
looks good. It's sounds right.
But I'm sure you've interviewed

431
00:25:50.020 --> 00:25:53.620
leaders on your podcasts and
privately when they say to

432
00:25:53.620 --> 00:25:57.970
you, when they present this
sort of, I know everything

433
00:25:57.970 --> 00:26:01.570
kind of mode in private,
there's a lot, they don't

434
00:26:01.570 --> 00:26:05.440
know. And there's a lot
they're concerned about and people's

435
00:26:05.440 --> 00:26:07.660
experience, I think will tell
them a little bit more

436
00:26:07.660 --> 00:26:11.080
of the truth about do
you know everything that's going

437
00:26:11.080 --> 00:26:14.350
to happen? Yeah. I definitely
keep an eye out for

438
00:26:14.350 --> 00:26:19.150
anyone who makes unsupported declarative
statements. You know, if you're,

439
00:26:19.390 --> 00:26:20.860
there's a, there's an old
joke. I think it was

440
00:26:20.860 --> 00:26:23.470
Quintin Tarantino who said that
he was most likely to

441
00:26:23.470 --> 00:26:25.660
make declarative statements is most
likely to be called a

442
00:26:25.660 --> 00:26:29.140
fool in retrospect. Yep. I
absolutely. Yeah, there's a lot.

443
00:26:29.140 --> 00:26:33.340
There's a lot there. So
then where can people who

444
00:26:33.340 --> 00:26:37.750
are listening start like this
might be an inspirational podcast.

445
00:26:37.750 --> 00:26:40.720
This is a great discussion.
But from the perspective of

446
00:26:40.720 --> 00:26:44.130
someone who is literally at
this moment as we driving

447
00:26:44.130 --> 00:26:46.860
To work and thinking about
how they can apply that

448
00:26:46.860 --> 00:26:49.800
in their job at wherever
they're headed to, it might

449
00:26:49.800 --> 00:26:53.460
be overwhelming. Yeah. And I'm
glad you asked it and

450
00:26:53.460 --> 00:26:56.040
framed it as sort of
you're going to work because

451
00:26:56.370 --> 00:26:59.340
while this is a longterm
perspective and we're looking at

452
00:26:59.340 --> 00:27:02.670
longterm problems, it's really a
practice that you can start

453
00:27:02.670 --> 00:27:05.520
every day, any day. So
if you're driving into work

454
00:27:05.520 --> 00:27:10.200
and you get into work,
think about in your meeting,

455
00:27:11.490 --> 00:27:15.870
how much you ask questions
versus give answers and see

456
00:27:15.870 --> 00:27:19.860
if you can flip it
and ask the questions of

457
00:27:19.860 --> 00:27:22.830
the people in the room
about a certain aspect in

458
00:27:22.860 --> 00:27:25.440
almost don't even give any
answers, don't even make any

459
00:27:25.440 --> 00:27:30.750
declarative statements. The second one
is a instead of an

460
00:27:30.780 --> 00:27:32.910
email or a Slack conversation,
what if you go to

461
00:27:32.910 --> 00:27:36.900
somebody's desk, who is an
operator of the system that

462
00:27:36.900 --> 00:27:39.840
you use, and you just
sit next to him and

463
00:27:39.840 --> 00:27:42.990
say, can you show me
how you do a simple

464
00:27:42.990 --> 00:27:47.480
thing And then Sit back
and just observe. And maybe

465
00:27:47.480 --> 00:27:51.500
you see some things that
you're like, wow, I had

466
00:27:51.500 --> 00:27:53.570
no idea I can solve
that for you. I didn't

467
00:27:53.570 --> 00:27:59.360
know. That was frustrating. I
think the, the, the, another

468
00:27:59.360 --> 00:28:04.310
one is kind of, if
you starting a project, look

469
00:28:04.310 --> 00:28:06.560
at the list of things
that you have to do

470
00:28:06.560 --> 00:28:09.440
and really think about and
collaborate on this. What's the

471
00:28:09.440 --> 00:28:12.920
biggest risk. Where is the
biggest risk here? And then

472
00:28:12.920 --> 00:28:16.280
have the courage to say,
what if we do that

473
00:28:16.280 --> 00:28:19.850
thing first? And it's going
to shock me, may shock

474
00:28:19.850 --> 00:28:21.620
people like, Oh no, that's
too big a problem. You

475
00:28:21.620 --> 00:28:23.690
want to do other things
like, no, no, let's, let's

476
00:28:23.720 --> 00:28:27.470
do that because if we
can tackle this thing, everything

477
00:28:27.470 --> 00:28:30.680
else becomes easier and simpler.
What an amazing thing to

478
00:28:30.680 --> 00:28:34.460
accomplish right away. And then
the last I would say

479
00:28:35.260 --> 00:28:40.190
Is If you're trying to
improve something, either for your

480
00:28:40.190 --> 00:28:44.870
customer, your collaborator, or the
person operating your system, or

481
00:28:44.900 --> 00:28:48.890
even your boss, Think about
what is it, The thing

482
00:28:48.890 --> 00:28:53.990
that can deliver time and
attention back to them. How

483
00:28:53.990 --> 00:28:56.510
can you take their attention
off of something that really

484
00:28:56.510 --> 00:29:00.800
doesn't need to be dealt
with instead giving them more

485
00:29:00.800 --> 00:29:03.350
of that attention to something
that might be more interesting,

486
00:29:03.740 --> 00:29:11.720
more meaningful and more purposeful?
I think those are like

487
00:29:11.720 --> 00:29:14.780
some sort of concrete things
that really hit on some

488
00:29:14.780 --> 00:29:16.880
of the themes that we're
talking about, but I'm also

489
00:29:16.880 --> 00:29:19.100
going to be writing a
lot more about this on

490
00:29:19.190 --> 00:29:23.780
our blog@trust.works and I'm on
medium as well. And I'll

491
00:29:23.780 --> 00:29:25.760
be writing a little bit
more about that as well.

492
00:29:25.760 --> 00:29:28.760
And we'll make sure to
have some resources, cause a

493
00:29:28.760 --> 00:29:31.310
lot of this comes from
inspirations, from books and things

494
00:29:31.310 --> 00:29:34.490
that I've read and people
that I've talked to and

495
00:29:34.820 --> 00:29:37.010
we'll have it on the
blog on your block. Sorry.

496
00:29:38.480 --> 00:29:40.880
Yeah, it sounds like you
are a, you are a

497
00:29:40.880 --> 00:29:45.310
pro you're a prolific reader
and you're absorbing and discarding

498
00:29:45.310 --> 00:29:47.950
and absorbing and picking what
makes sense and discarding what

499
00:29:47.950 --> 00:29:50.590
doesn't make sense as you
move forward. I think it's

500
00:29:50.590 --> 00:29:52.690
another reminder to people who
may be heading to work

501
00:29:52.690 --> 00:29:55.450
right now that there can
be dry spells and you

502
00:29:55.450 --> 00:29:58.180
might go a couple of
months without reading a new

503
00:29:58.180 --> 00:30:00.370
book or thinking about a
new idea or discovering a

504
00:30:00.370 --> 00:30:03.610
new concept. Here's an opportunity
to maybe turn that around

505
00:30:03.760 --> 00:30:07.930
and explore an entirely new
space. Yeah. I think being

506
00:30:07.930 --> 00:30:12.430
able to make connections between
different genres and different modes

507
00:30:12.430 --> 00:30:17.680
of practice, it gets really,
really interesting in, I know

508
00:30:17.680 --> 00:30:20.710
there's a very brief time,
but I'll give one other

509
00:30:20.710 --> 00:30:22.630
thing that you might want
to check out, which is

510
00:30:22.780 --> 00:30:26.290
Dave Brailsford. Who's the coach
of team sky who won

511
00:30:26.290 --> 00:30:28.060
the tour de France in
three years when he was

512
00:30:28.060 --> 00:30:30.430
tasked to do it for
five, has this concept called

513
00:30:30.430 --> 00:30:35.560
getting 1% better? It is
phenomenal because it really addresses

514
00:30:35.560 --> 00:30:38.620
this idea of improvement and
starting small and getting better.

515
00:30:38.620 --> 00:30:41.380
And I'll put the, we'll
put the link in the,

516
00:30:41.860 --> 00:30:44.650
in the podcast. All right.
Well, thank you so much,

517
00:30:44.650 --> 00:30:47.260
Everett Harper for chatting with
me today. Yeah. This has

518
00:30:47.260 --> 00:30:49.900
been really, really fun. I
appreciate being able to talk

519
00:30:49.900 --> 00:30:52.180
about this is for me,
this is really fun stuff

520
00:30:52.180 --> 00:30:54.700
and I hope it's been
useful and meaningful and I

521
00:30:54.700 --> 00:30:57.820
encourage people to come on
in and let's talk on

522
00:30:57.820 --> 00:31:01.210
this figure more of it
out. All right. We'll have

523
00:31:01.210 --> 00:31:03.700
links to all of Everett's
work and his team work

524
00:31:03.700 --> 00:31:07.900
and you can check them
out@trust.works. This has been another

525
00:31:07.900 --> 00:31:10.720
episode of Hansel minutes. See
you again next week.

