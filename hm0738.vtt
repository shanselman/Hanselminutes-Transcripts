WEBVTT FILE

1
00:00:00.150 --> 00:00:04.140
Today's sponsor is Datadog a
monitoring and analytics platform for

2
00:00:04.140 --> 00:00:08.910
cloud scale infrastructure and applications.
Datadog provides seamless integrations with

3
00:00:08.910 --> 00:00:13.830
more than 400 technologies, including
AWS Postgres sequel, my sequel

4
00:00:14.370 --> 00:00:17.670
Docker. So you can start
collecting and visualizing performance metrics

5
00:00:17.670 --> 00:00:21.150
quickly distributed tracing an APM.
Give you end to end

6
00:00:21.150 --> 00:00:24.900
visibility into requests wherever they
go across hosts across containers,

7
00:00:24.900 --> 00:00:29.970
across service boundaries. With rich
dashboards, algorithmic alerts and collaboration

8
00:00:29.970 --> 00:00:33.240
tools. Datadog will give your
team the tools they need

9
00:00:33.270 --> 00:00:37.410
to quickly troubleshoot and optimize
modern applications. See for yourself.

10
00:00:37.530 --> 00:00:41.550
Start a 14 day free
trial today by visiting Datadog

11
00:00:41.580 --> 00:00:46.230
hq.com/hansel minutes. And they'll send
you a free tee shirt.

12
00:00:46.770 --> 00:01:05.460
That's Datadog hq.com/hansel minutes. Hi,
this is Scott Hanselman. This

13
00:01:05.460 --> 00:01:07.590
is another episode of Hansel
minutes today. I'm talking with

14
00:01:07.590 --> 00:01:11.250
dr. Reham monster. She's a
software engineering manager working in

15
00:01:11.250 --> 00:01:15.830
the world of natural language
processing. How are you? Oh,

16
00:01:15.830 --> 00:01:19.220
I'm good. How are you,
Scott? I am doing okay.

17
00:01:19.220 --> 00:01:21.830
I am learning and living
and reading as much as

18
00:01:21.830 --> 00:01:24.530
I possibly can. And I'm
excited to talk to you

19
00:01:24.530 --> 00:01:28.640
because I want to understand
what natural language processing is

20
00:01:28.640 --> 00:01:33.890
because it's linguistics, but it's
computer science, but it's artificial

21
00:01:33.890 --> 00:01:37.130
intelligence. It seems like it's
a Venn diagram of multiple

22
00:01:37.580 --> 00:01:41.420
Disciplines. Oh, you, you seem
to have a lot of

23
00:01:41.420 --> 00:01:48.080
knowledge about it then. So,
so reality is let's take

24
00:01:48.080 --> 00:01:54.440
the, the easy, simple definition.
Okay. So how do humans

25
00:01:55.190 --> 00:01:59.630
like kind of connect and
communicate? It's a natural language

26
00:01:59.660 --> 00:02:02.510
is their power right there.
It's the main power that

27
00:02:02.510 --> 00:02:06.130
we do have even, even
when we, yes, there is

28
00:02:06.140 --> 00:02:09.500
a, the face expressions that
is our gestures that contribute

29
00:02:09.500 --> 00:02:14.060
to like how you understand
what we mean, but the

30
00:02:14.120 --> 00:02:17.260
power of natural language and
our, and our ability and

31
00:02:17.260 --> 00:02:20.060
the ability of our brains
to kind of put together,

32
00:02:20.690 --> 00:02:26.390
given the word vocabulary and
some smart ideas that, that

33
00:02:26.390 --> 00:02:31.130
become like the innovations and
all the nice literature books

34
00:02:31.130 --> 00:02:34.850
and whatnot. This is exactly
the power right that humans

35
00:02:34.850 --> 00:02:39.590
can put together, vocabulary that
together, make sense and add

36
00:02:39.590 --> 00:02:44.600
value, right? And describe things
and whatever. So with that

37
00:02:44.600 --> 00:02:49.340
in mind, natural language processing
is exactly the interaction between

38
00:02:49.340 --> 00:02:52.490
humans and machines. We want
to, we are dreaming of

39
00:02:52.490 --> 00:02:57.560
a world where machines can
understand natural language and has

40
00:02:57.560 --> 00:03:01.120
a similar power to understanding
that your language. And here's

41
00:03:01.120 --> 00:03:05.740
why, like there isn't anything
that can be better than

42
00:03:05.770 --> 00:03:10.210
talking to a human smiling
and joking and having all

43
00:03:10.210 --> 00:03:13.300
the gestures, which you might
not have with the machine.

44
00:03:13.840 --> 00:03:19.270
Right? However, there's a lot
of the task completion work

45
00:03:19.300 --> 00:03:22.780
that we do day to
day in our daily lives,

46
00:03:23.320 --> 00:03:26.830
that if we get some
automation out of, I think

47
00:03:26.890 --> 00:03:30.100
we can be more efficient
as humans. So I don't

48
00:03:30.100 --> 00:03:33.730
think of natural language processing
is a, or getting computers

49
00:03:33.760 --> 00:03:37.780
to understand human language as
a way to replace human

50
00:03:37.780 --> 00:03:41.740
connections and human interactions, because
nothing can replace that, right.

51
00:03:42.130 --> 00:03:44.680
It is what makes us
happy. It's how much we

52
00:03:44.680 --> 00:03:48.340
connect with people, our friends,
or family or kids, but

53
00:03:48.340 --> 00:03:50.890
it's more on like, how
can we get more efficient

54
00:03:50.920 --> 00:03:55.030
by getting our machines to
understand us the way I

55
00:03:55.030 --> 00:03:59.470
tell my coworker to do
something, or I tell my

56
00:03:59.470 --> 00:04:02.710
friend to help me with
something or even give some

57
00:04:02.710 --> 00:04:06.490
instructions to my son on
his homework or whatever, like

58
00:04:06.550 --> 00:04:09.850
kind of task after. And
I use my natural language

59
00:04:09.850 --> 00:04:13.410
to describe it to the
other human. It seems like

60
00:04:13.410 --> 00:04:15.720
we spend a lot of
time and I've had this

61
00:04:15.720 --> 00:04:18.600
happening recently. As I try
to teach my children in

62
00:04:18.600 --> 00:04:21.900
a remote working world, we
spend so much time trying

63
00:04:21.900 --> 00:04:25.490
to make ourselves understood. Like
I have intent. I have

64
00:04:25.740 --> 00:04:27.870
a thing I intend you
to understand I needed to

65
00:04:27.870 --> 00:04:30.660
get out of it, but
I'm dancing around. I'm trying

66
00:04:30.660 --> 00:04:34.200
to find the right natural
language in my vocabulary that

67
00:04:34.200 --> 00:04:37.350
fits the person's worldview so
that I can get them

68
00:04:37.350 --> 00:04:39.840
to do what I want.
I'm having trouble mapping the

69
00:04:39.840 --> 00:04:43.170
words in my mouth to
getting them to say yes,

70
00:04:43.170 --> 00:04:45.750
I understand what you said.
And I know your intent.

71
00:04:47.310 --> 00:04:52.050
Yeah. I think machines are
not as sophisticated as humans.

72
00:04:52.130 --> 00:04:55.290
The Le the level the
machine can understand out of

73
00:04:55.320 --> 00:04:59.970
natural language is a way
less sophisticated. I think human

74
00:04:59.970 --> 00:05:05.790
languages, when humans interact with
the are more for, yes,

75
00:05:05.790 --> 00:05:09.330
you were expressing your intent,
and you might be saying,

76
00:05:09.730 --> 00:05:13.800
you're showing empathy to nation
of the voice is helping

77
00:05:13.800 --> 00:05:18.840
you with some, some other
aspects that would help the

78
00:05:18.840 --> 00:05:22.650
other human understand your intent.
The machine is a, is

79
00:05:22.650 --> 00:05:27.030
a less sophisticated than that.
It can make sense of

80
00:05:27.330 --> 00:05:32.010
the power of variations of
natural language. However, it, it's

81
00:05:32.010 --> 00:05:36.450
not at the level of
building that human connections that

82
00:05:36.450 --> 00:05:39.780
you build with, with other
humans, right? So that's why

83
00:05:39.780 --> 00:05:42.660
I said at the beginning,
natural language has done a

84
00:05:42.660 --> 00:05:46.470
lot of progress. NLP has
done a lot of progress

85
00:05:46.740 --> 00:05:49.320
in terms of using the
compute power, to get machines,

86
00:05:49.320 --> 00:05:52.320
to be helping humans in
their day to day activities

87
00:05:52.320 --> 00:05:56.550
and become more efficient. However,
there, the effort on making

88
00:05:56.860 --> 00:06:00.680
human connections is still on
its way. We will get

89
00:06:00.680 --> 00:06:06.140
there, like to help people
with autism or people who

90
00:06:06.140 --> 00:06:10.640
are lonely, or like helping
with these social interactions East

91
00:06:10.640 --> 00:06:15.380
coming, its natural language is
on its way, making progress

92
00:06:15.380 --> 00:06:19.070
in these domains. But it's
not as much progress that

93
00:06:19.070 --> 00:06:23.300
we have today. Like the
task completion work, like where,

94
00:06:23.420 --> 00:06:25.130
as you said that you
were talking to your son,

95
00:06:25.130 --> 00:06:26.840
you want your son to
do something and you had

96
00:06:26.840 --> 00:06:29.930
an intent. This is very
task oriented. If you think

97
00:06:29.930 --> 00:06:32.840
about it, right. It's not
really like you're connecting or

98
00:06:32.840 --> 00:06:37.490
chitchatting with your emotional. It's
exactly. It's do this. I

99
00:06:37.490 --> 00:06:39.710
think we have done a
lot of progress on do

100
00:06:39.710 --> 00:06:43.610
this kind of work between
humans and machines. We have

101
00:06:43.700 --> 00:06:46.700
done, we did a lot
of progress as well on

102
00:06:46.700 --> 00:06:49.290
human connections, but it's not
at the stage where it's,

103
00:06:49.290 --> 00:06:52.190
it's like a de facto
thing or something that everybody

104
00:06:52.190 --> 00:06:55.310
is not ready for. COVID
for being a commodity yet.

105
00:06:55.870 --> 00:06:58.180
Right? Another thing that I'm
noticing is that as I

106
00:06:58.180 --> 00:07:01.840
use my Alexis and my
Cortana's and my Google, whatever,

107
00:07:02.020 --> 00:07:04.060
and I talk to these
different things to say, do

108
00:07:04.060 --> 00:07:08.890
this the way that I,
and my nontechnical spouse interact

109
00:07:08.890 --> 00:07:14.860
is, Hey, I don't know,
Alexa thing, do this. And

110
00:07:14.860 --> 00:07:17.140
then it doesn't understand. And
then I simply repeat the

111
00:07:17.140 --> 00:07:19.990
entire thing it's as if
I only get one chance.

112
00:07:20.050 --> 00:07:23.980
And then I reset and
I attempt to call the

113
00:07:23.980 --> 00:07:27.760
function again, as opposed to
what I'd like to do,

114
00:07:27.760 --> 00:07:32.110
which is clarify there's, doesn't
seem to be any back

115
00:07:32.110 --> 00:07:34.300
and forth. You get one
chance to nail it and

116
00:07:34.300 --> 00:07:36.970
get it right. And then
it's either wrong or it's

117
00:07:36.970 --> 00:07:40.270
not. And then you just
try all over again. Yeah.

118
00:07:40.280 --> 00:07:42.430
I think we can do
better than that in terms

119
00:07:42.430 --> 00:07:46.930
of technology advancement. I think
we're at a stage in

120
00:07:46.930 --> 00:07:54.160
science where like things around
asking clarification questions conversing with

121
00:07:54.160 --> 00:07:59.620
humans. The way humans are
used to converse is, is,

122
00:07:59.710 --> 00:08:02.170
is, is good. We're getting
close to that. Like I

123
00:08:02.170 --> 00:08:05.470
would say, for example, especially
with respect to do that

124
00:08:05.710 --> 00:08:08.980
thing, right? So if you're
Cortana or Alexa, and if

125
00:08:08.980 --> 00:08:13.870
you're asking to play Michael Jackson,
for example, on, on, on

126
00:08:13.870 --> 00:08:18.430
Cortana or Alexa, then you
might, I mean, maybe your

127
00:08:18.430 --> 00:08:22.210
voice was a little noisy.
It was a little soft.

128
00:08:22.210 --> 00:08:24.400
It was from four when
you were talking to your

129
00:08:24.400 --> 00:08:29.620
Alexa. So if, if the
device doesn't get the content,

130
00:08:29.620 --> 00:08:34.360
the audio are clearly, then
that translation from the audio

131
00:08:34.570 --> 00:08:38.800
to something the machine can
understand becomes harder. And that's

132
00:08:38.800 --> 00:08:42.490
what the machine is good
at. Right? Given audio that

133
00:08:42.490 --> 00:08:45.610
you said it takes that
and passes it through a

134
00:08:45.610 --> 00:08:49.390
bunch of processing steps. And
I can maybe give a,

135
00:08:49.400 --> 00:08:54.010
a high level description of
what that looks like. But,

136
00:08:54.310 --> 00:08:57.960
but the thing is if
that processing, if the input

137
00:08:57.960 --> 00:09:00.630
is noisy, expect the output
to be noisy and the

138
00:09:00.630 --> 00:09:03.140
machine won't understand. That makes
sense. That makes sense. And

139
00:09:03.140 --> 00:09:04.700
that's how it is in
life. Right? If I was

140
00:09:04.700 --> 00:09:07.460
talking to you now and
suddenly that started to break

141
00:09:07.520 --> 00:09:10.400
up, then you can't understand,
you have to go and

142
00:09:10.460 --> 00:09:13.670
build that back from scratch.
Exactly. And the point is,

143
00:09:13.670 --> 00:09:17.840
now that these machines are
better at asking clarifying questions,

144
00:09:17.840 --> 00:09:21.760
things like, did you mean
this or that? Did, I

145
00:09:21.830 --> 00:09:24.170
mean, it gives you more
choices now because it, because

146
00:09:24.170 --> 00:09:28.070
it faintly figured out what
you're saying, but it needs

147
00:09:28.070 --> 00:09:32.600
confirmation before taking the action.
Right. So even asking for

148
00:09:32.600 --> 00:09:35.450
confirmations is what humans do
all the time. Right? So

149
00:09:35.450 --> 00:09:40.220
we learned this behavior from
humans. So you, you tell

150
00:09:40.220 --> 00:09:43.370
your son, can you grab
me a cup of water?

151
00:09:43.640 --> 00:09:45.460
And then if he was
not sure if she, if

152
00:09:45.470 --> 00:09:48.560
she, if she was not
sure they would like a

153
00:09:48.560 --> 00:09:51.710
goal and, and get you
something, and then do you

154
00:09:51.710 --> 00:09:55.910
need any drink or just
water, right. So the machines

155
00:09:55.910 --> 00:10:00.670
are doing better that with
some limited capability, but, but

156
00:10:00.740 --> 00:10:03.590
we, we, we should be
doing more of that, given

157
00:10:03.590 --> 00:10:06.260
the power of NLP that
we do have today, right?

158
00:10:06.440 --> 00:10:08.630
Yeah. Let me get back
a little bit to the

159
00:10:08.630 --> 00:10:12.620
stack of processing at a
high level. Like what happens

160
00:10:12.650 --> 00:10:16.640
when, when I talk to
Alexa or cortex, right? So

161
00:10:17.060 --> 00:10:20.420
basically what happens is I
get an audio file right

162
00:10:20.480 --> 00:10:24.680
on the machine or on
the service. And, and then

163
00:10:24.770 --> 00:10:28.610
that audio file goes through
a process of trans transforming

164
00:10:28.610 --> 00:10:32.000
it from audio to text
what we call automatic speech

165
00:10:32.000 --> 00:10:36.080
recognition, right? ASR, it's a,
it's a known technique known

166
00:10:36.080 --> 00:10:40.670
technique for translating audio to
text because machines are better

167
00:10:40.670 --> 00:10:46.430
at understanding text than audio.
Right? Some translation might be

168
00:10:46.430 --> 00:10:51.020
lost in that trance, in
that transformation, right. Because maybe

169
00:10:51.020 --> 00:10:55.430
I said, can you give
I'm at a Starbucks and

170
00:10:56.510 --> 00:11:01.490
maybe buying some coffee and
a Madeline. And maybe I

171
00:11:01.490 --> 00:11:03.170
did. I pronounce it that
in a way that the

172
00:11:03.170 --> 00:11:06.590
machine did not figure out
the right way so that

173
00:11:06.810 --> 00:11:09.920
the translation was lost in
the middle, right. In that

174
00:11:09.920 --> 00:11:12.770
transformation, right. This is the
first layer, second layer. Now

175
00:11:12.770 --> 00:11:14.810
I got the text. What
can I do with the

176
00:11:14.810 --> 00:11:19.040
text? As you said, because
you say, do this, you

177
00:11:19.040 --> 00:11:22.310
want to figure out the
intent, right? So for example,

178
00:11:22.430 --> 00:11:25.550
grab me a cup of
coffee, right? So the intent

179
00:11:25.550 --> 00:11:28.940
is for the other human
to give me something. But

180
00:11:28.940 --> 00:11:32.300
what that something is, is
equally important as your intent.

181
00:11:32.810 --> 00:11:36.110
Yes. The grabbing is the
action that you want, but

182
00:11:36.110 --> 00:11:38.000
what is it exactly that
you want to grab? So

183
00:11:38.000 --> 00:11:41.630
it's not enough to know
the intent that the natural

184
00:11:41.630 --> 00:11:45.740
language processing needs more than
that, what we call entities.

185
00:11:46.220 --> 00:11:49.520
So, or like a key
pieces of information in a,

186
00:11:49.520 --> 00:11:53.350
in your order or your
So it's context. If I

187
00:11:53.350 --> 00:11:56.470
went into a Starbucks and
asked for a pizza, it

188
00:11:56.470 --> 00:11:59.470
would take a moment for
that person to go, no,

189
00:11:59.470 --> 00:12:01.990
there's no pizza here, but
if I order a kind

190
00:12:01.990 --> 00:12:04.390
of coffee, they're more likely
to be successful and understand

191
00:12:04.390 --> 00:12:07.440
exactly what I mean. Correct.
And, and not only that,

192
00:12:07.470 --> 00:12:10.620
there is more nuance to
it. And as even if

193
00:12:10.620 --> 00:12:13.380
like the Starbucks person understand
that you want a coffee,

194
00:12:13.500 --> 00:12:16.200
what type of coffee do
you want? What options do

195
00:12:16.200 --> 00:12:18.690
you want to change on
top of your coffee? What

196
00:12:18.690 --> 00:12:20.910
kind of milk do you
want? Right. So there is

197
00:12:20.910 --> 00:12:24.060
a bunch of things that
you, if you really want

198
00:12:24.860 --> 00:12:27.510
a good service, the same
way a human would do

199
00:12:27.510 --> 00:12:31.080
it to you, then you
were really at a point

200
00:12:31.080 --> 00:12:33.870
where, Oh, I need to
identify all these pieces. Right?

201
00:12:34.380 --> 00:12:36.810
Same when you say your,
when you tell your son

202
00:12:36.840 --> 00:12:39.810
to grab a specific thing
for you, and then maybe

203
00:12:39.810 --> 00:12:42.780
your son would ask some
questions to get more details,

204
00:12:42.930 --> 00:12:47.340
right. And these details matter
in, in how good the

205
00:12:47.340 --> 00:12:49.650
task completion is or the
results you get out of

206
00:12:49.650 --> 00:12:54.030
the machine. Right. Interesting. Okay.
What about when things get

207
00:12:54.030 --> 00:12:56.130
mixed up? If I go
to order a coffee and

208
00:12:56.130 --> 00:12:58.590
I say, I'd like a
cafe con Leche, when I'm

209
00:12:58.590 --> 00:13:01.620
mixing English and another language,
do you have, you know,

210
00:13:01.620 --> 00:13:04.290
you can glean that. Can
it learn that? Or does

211
00:13:04.290 --> 00:13:06.900
it need to have all
that context upfront? Okay. Oh

212
00:13:06.900 --> 00:13:11.160
yeah. I think the power
of machine translation, as well

213
00:13:11.160 --> 00:13:15.210
as natural language processing, we
can, we, we are at

214
00:13:15.210 --> 00:13:18.720
the point where we can
mix and understand the different

215
00:13:18.720 --> 00:13:20.970
languages. Even if they are
mixed it in the same

216
00:13:20.970 --> 00:13:23.820
sentence, it becomes a harder
on the machine. It might

217
00:13:23.820 --> 00:13:27.450
take more processing, power and
whatnot, but it's pretty doable.

218
00:13:28.320 --> 00:13:32.340
Wow. Okay. So I say
something like, give me a

219
00:13:32.340 --> 00:13:35.580
coffee. That audio goes through
the processing. It turns into

220
00:13:35.580 --> 00:13:39.150
text a query that then
says in a string in

221
00:13:39.150 --> 00:13:45.060
computer terms, does it have
a confidence? Does it say,

222
00:13:45.060 --> 00:13:47.400
well, I'm 90% sure that
that's what they said. Yeah.

223
00:13:47.400 --> 00:13:51.450
Good question. So it depends
on how the models in

224
00:13:51.450 --> 00:13:55.890
the background have been built.
So there are two ways

225
00:13:55.920 --> 00:14:01.950
of introducing, of, of processing
text or doing AI because

226
00:14:01.950 --> 00:14:05.190
this is artificial intelligence, right?
For machines to understand human

227
00:14:05.190 --> 00:14:09.720
language, this is artificial intelligence.
So there are two ways

228
00:14:09.720 --> 00:14:12.330
of doing things. And there
is a hybrid between the

229
00:14:12.330 --> 00:14:15.830
two, the first part, which
has started very early in,

230
00:14:15.880 --> 00:14:18.330
I would say the sixties
and seventies is what we

231
00:14:18.330 --> 00:14:22.080
call rule based systems. So
behind the back of that,

232
00:14:22.170 --> 00:14:25.770
Alexa or Cortana, there's a
bunch of AI models that

233
00:14:25.830 --> 00:14:29.160
are understanding the intent of
the texts that they understand,

234
00:14:29.160 --> 00:14:31.650
the key pieces, like the
entities we talked about, like,

235
00:14:31.650 --> 00:14:33.300
what type of coffee do
you want and what milk

236
00:14:33.330 --> 00:14:36.330
do you want and so
forth. So these models are

237
00:14:36.360 --> 00:14:39.210
either a bunch of rules
because you might argue that

238
00:14:39.240 --> 00:14:42.330
I know all the lists
of coffee or that a

239
00:14:42.330 --> 00:14:46.020
certain coffee shop is offering.
Right. We're not confused about

240
00:14:46.020 --> 00:14:48.480
that. They have their menu.
And if we're building that

241
00:14:48.480 --> 00:14:52.010
for a specific coffee shop,
then the, probably they know

242
00:14:52.010 --> 00:14:55.010
what the offer. And we
might have that list, a

243
00:14:55.010 --> 00:14:58.190
bunch of strings. And I,
and I do exact match

244
00:14:58.220 --> 00:15:00.950
right. Between what the user
has said, or the text

245
00:15:00.980 --> 00:15:04.480
that I got from the
processing and that, that, that

246
00:15:04.490 --> 00:15:07.820
list. Right. So I call
this rule based because it's,

247
00:15:07.850 --> 00:15:11.360
it's based on rules, right?
It's not super smart. It's,

248
00:15:11.490 --> 00:15:14.510
it's like either you match
or not, or I might

249
00:15:14.510 --> 00:15:17.870
build regular expressions, right. Things
like, as you may, you're

250
00:15:17.870 --> 00:15:20.630
asking me about my phone
number, right? My phone number

251
00:15:20.630 --> 00:15:24.230
can be easily detected with
a, with an irregular expression,

252
00:15:24.230 --> 00:15:27.650
right? So these are all
rules that if we put

253
00:15:27.650 --> 00:15:31.730
together, we will, they are
deterministic with rules. I mean,

254
00:15:31.730 --> 00:15:34.550
they are deterministic, like given
the same input, I will

255
00:15:34.550 --> 00:15:39.140
get every time, the same
output, right. Then the other

256
00:15:39.470 --> 00:15:43.220
school of, of thought here
for building these models is

257
00:15:43.250 --> 00:15:47.690
the testicle models and statistical
models is the way machine

258
00:15:47.690 --> 00:15:50.870
learning works. Right? So many
people hear about that word

259
00:15:50.870 --> 00:15:55.100
machine learning, what machine learning
is it's, it's using statistical

260
00:15:55.100 --> 00:16:01.730
models to make provability, eh,
predictions, right? So basically what

261
00:16:01.730 --> 00:16:05.690
we do in pro in
probability based models or statistical

262
00:16:05.690 --> 00:16:12.170
models in natural language processing
context is we get the

263
00:16:12.170 --> 00:16:16.670
machine to see a bunch
of patterns upfront, right. A

264
00:16:16.670 --> 00:16:19.250
bunch of patterns that it
can see in real time.

265
00:16:19.760 --> 00:16:25.250
Right? So if I'm building
that pizza ordering system, then

266
00:16:25.280 --> 00:16:28.940
I would show the machine
a bunch of different ways

267
00:16:28.940 --> 00:16:33.110
of ordering Peter, right? Different
ways customers would order pizza,

268
00:16:33.140 --> 00:16:37.580
different pizza options and different
toppings and whatnot. Then train

269
00:16:37.580 --> 00:16:42.740
it for some time on
that. Then build a statistical

270
00:16:42.740 --> 00:16:47.420
model out of that, that
when put to real traffic,

271
00:16:47.420 --> 00:16:51.890
real orders, it will say,
Oh, you know what? This

272
00:16:51.890 --> 00:16:56.030
is probably a pepperoni pizza
with a confidence score of

273
00:16:56.030 --> 00:17:00.770
a 90%. Right. And, and
this is the topping here

274
00:17:00.770 --> 00:17:04.760
is vegetable topping with a,
with a confidence score of

275
00:17:05.690 --> 00:17:11.120
84% and so forth. And
why are we generating this

276
00:17:11.660 --> 00:17:15.860
like a prediction score or
confidence score it's because of

277
00:17:15.860 --> 00:17:20.390
the system is built statistically,
right? So given the patterns

278
00:17:20.390 --> 00:17:23.150
it has seen at training
time when we trained the

279
00:17:23.150 --> 00:17:27.830
model, before we put it
in action, how given that

280
00:17:27.830 --> 00:17:32.240
knowledge about the world, how
much do we anticipate this

281
00:17:32.240 --> 00:17:36.680
new order that I haven't
seen before as a machine,

282
00:17:37.580 --> 00:17:41.510
it would be right. That's
basically what it is. And

283
00:17:41.720 --> 00:17:45.080
that word of machine learning
alongside with the world of

284
00:17:45.320 --> 00:17:50.850
rule-based or mix it together
to build very highly efficient

285
00:17:50.880 --> 00:17:53.670
bunch of AI models for
natural gas processing. And here

286
00:17:53.670 --> 00:17:57.420
is why, like, while there's
a huge power in statistical

287
00:17:57.420 --> 00:18:00.780
models, there are many, many
problems that we can be

288
00:18:00.810 --> 00:18:04.260
easily solved in a deterministic
way as well. So I

289
00:18:04.260 --> 00:18:07.710
don't think like there's one
solution versus the other machine

290
00:18:07.710 --> 00:18:11.850
learning is a more scalable
solution. It covers a larger

291
00:18:11.850 --> 00:18:15.000
set of patterns. It can
deal with uncertainty that comes

292
00:18:15.000 --> 00:18:18.060
in real orders. Like for
example, if the, if the

293
00:18:18.060 --> 00:18:22.200
machine hasn't seen an, that
pattern of an order before

294
00:18:22.470 --> 00:18:25.980
it can still generate a
prediction score for it, with

295
00:18:25.980 --> 00:18:30.690
some confidence, given that it
relates to other patterns that

296
00:18:30.690 --> 00:18:34.100
it has seen. So it,
it, it pretty much works

297
00:18:34.100 --> 00:18:37.470
similar to how humans work.
Right. So if you have,

298
00:18:37.500 --> 00:18:41.370
if you're new to a
new pizza restaurant, right, and

299
00:18:41.370 --> 00:18:43.830
it's your first time to
see a pizza with a

300
00:18:43.860 --> 00:18:46.500
broccoli's, for example, you haven't
seen that before as a

301
00:18:46.500 --> 00:18:49.350
human, what would you, how
would you react to that?

302
00:18:49.410 --> 00:18:52.530
You would try to relate
it to the closest possible

303
00:18:52.530 --> 00:18:55.830
reference point you had in
mind, which is any other

304
00:18:55.950 --> 00:19:00.450
pizza with any vegetable, right.
And, and from that perspective

305
00:19:00.450 --> 00:19:03.720
you would anticipate, or probably
it would taste, well, let

306
00:19:03.720 --> 00:19:07.890
me try it. Right. So
that's exactly what the statistical

307
00:19:07.890 --> 00:19:11.160
models are doing. They try
to relate to the nearest

308
00:19:11.850 --> 00:19:16.200
pattern they have seen, and
statistically generate the score for

309
00:19:16.200 --> 00:19:19.410
that, for the, the machine
confidence in that, in that

310
00:19:19.410 --> 00:19:23.780
prediction. That makes me wonder
if we, as humans or

311
00:19:23.780 --> 00:19:28.760
myself as a, as me,
I'm probably internally keeping a

312
00:19:28.790 --> 00:19:32.600
confidence score. When I, when
my wife yells something down,

313
00:19:33.020 --> 00:19:35.750
I am 85% sure. She
said this. I don't actually

314
00:19:35.750 --> 00:19:39.800
think that consciously, but those,
that confidence number is there

315
00:19:39.800 --> 00:19:42.320
in all of my interactions
with natural language and as

316
00:19:42.320 --> 00:19:45.440
a human. Absolutely. Yeah. The
machine is not far from

317
00:19:45.720 --> 00:19:51.110
how you operate it. Just
that you're smarter. Yeah. You

318
00:19:51.110 --> 00:19:54.350
have way more knowledge about
the world than the, what

319
00:19:54.350 --> 00:19:57.500
the machine has. The machine
has to be told what

320
00:19:57.500 --> 00:20:00.890
knowledge it would have, but
you as a human, with

321
00:20:00.890 --> 00:20:04.340
education, with interaction with other
people. And that's why we

322
00:20:04.340 --> 00:20:07.820
have difference in depth in
levels of knowledge, between people

323
00:20:07.820 --> 00:20:10.730
across the ages, right? You
haven't had the same level

324
00:20:10.730 --> 00:20:13.970
of knowledge 15 years ago,
or when you were a

325
00:20:13.970 --> 00:20:18.140
kid in school, right. No
one wants to manage databases

326
00:20:18.140 --> 00:20:21.170
as they can avoid it.
And that's why Mongo DB

327
00:20:21.200 --> 00:20:25.640
made Mongo DB Atlas, a
global cloud database service that

328
00:20:25.640 --> 00:20:29.810
runs on AWS, GCP and
Azure. You can deploy a

329
00:20:29.810 --> 00:20:33.560
fully managed Mongo DB database
in minutes with just a

330
00:20:33.560 --> 00:20:37.430
few clicks or a few
API calls, Mongo DB, Atlas,

331
00:20:37.520 --> 00:20:41.870
automates deployment, automates updates, handles
scaling, and more so that

332
00:20:41.870 --> 00:20:44.810
you can focus on your
application instead of taking care

333
00:20:44.810 --> 00:20:51.910
of your database, you can
get started@mongodb.com slash Atlas. Now,

334
00:20:51.910 --> 00:20:54.940
if you're already managing a
Mongo DB deployment, Atlas has

335
00:20:54.940 --> 00:20:58.600
a live migration service. So
you can migrate it easily

336
00:20:58.630 --> 00:21:01.300
with minimal downtime and then
get back to what matters.

337
00:21:01.510 --> 00:21:05.770
Stop managing your database and
start using Mongo DB Atlas.

338
00:21:06.550 --> 00:21:09.400
This is really significant because
I've got a 12 year

339
00:21:09.400 --> 00:21:12.280
old and a 14 year
old. And they've just in

340
00:21:12.280 --> 00:21:14.170
the last two or three
years. I think you may

341
00:21:14.170 --> 00:21:17.020
have had this experience as
well when your child suddenly

342
00:21:17.020 --> 00:21:19.360
knows something that you don't
know. And the sum total

343
00:21:19.360 --> 00:21:23.650
of their existence is more
than, than you. My three

344
00:21:23.650 --> 00:21:26.260
year old, when he, when
he was three, he only

345
00:21:26.260 --> 00:21:27.820
knew everything that I told
him. He didn't do any,

346
00:21:27.820 --> 00:21:30.010
he had no context outside
of what we fed him.

347
00:21:30.430 --> 00:21:32.420
And now the context of
a 14 year old or

348
00:21:32.420 --> 00:21:34.570
a third year is massive.
And they're telling me things

349
00:21:34.570 --> 00:21:37.290
I've never heard. Where did
you pick that up? Ah,

350
00:21:37.320 --> 00:21:40.320
you're bringing a very interesting
point here. If I map

351
00:21:40.320 --> 00:21:45.330
this to how machines work,
there's an interesting analogy that

352
00:21:45.330 --> 00:21:49.230
we can draw here with
machines and statistical models. If

353
00:21:49.230 --> 00:21:51.720
we take the machine learning
route, which is most of

354
00:21:52.020 --> 00:21:56.070
how we scale our AI,
it's like, yes, the look

355
00:21:56.070 --> 00:22:00.240
at some knowledge, limited knowledge
at the beginning, that's what

356
00:22:00.240 --> 00:22:02.580
you train them on. But
once you put them in

357
00:22:02.580 --> 00:22:06.060
action and they start seeing
real, real orders in the

358
00:22:06.060 --> 00:22:11.340
pizza shop, right over time,
they would get to learn

359
00:22:11.430 --> 00:22:14.520
more and more like they
will get more knowledge if

360
00:22:14.520 --> 00:22:17.610
we reuse some of that
traffic back in the model

361
00:22:17.970 --> 00:22:20.820
and reeducate the model, given
what it has seen in

362
00:22:20.820 --> 00:22:24.330
real life. And that's exactly
how your son operated right?

363
00:22:24.660 --> 00:22:27.900
When he was only at
home or in the nursery,

364
00:22:28.170 --> 00:22:30.810
when he's a scope of
visibility to the world was

365
00:22:30.810 --> 00:22:34.890
very limited and controlled by
you that give him a

366
00:22:34.890 --> 00:22:38.640
limited set of pieces of
knowledge. But then once he

367
00:22:38.640 --> 00:22:42.720
went into school and like,
do the, do the exercise

368
00:22:42.720 --> 00:22:46.710
and do the swimming game
and whatnot, then he started

369
00:22:46.710 --> 00:22:50.700
interacting with more people, gaining
more experiences and learning more

370
00:22:51.180 --> 00:22:55.530
information, right. And that, and,
and basically you've deployed your

371
00:22:55.530 --> 00:22:59.370
son in, in real life.
There's the same way we

372
00:22:59.370 --> 00:23:03.450
deploy. Yeah. The same way
you deploy the model after

373
00:23:03.450 --> 00:23:07.620
you train it in, in
reality in production. Ah, and

374
00:23:07.620 --> 00:23:11.400
this model, this, this child
that I've deployed has the,

375
00:23:11.940 --> 00:23:16.890
has the implicit ability to
absorb and grow while a

376
00:23:16.890 --> 00:23:19.620
model that remains static does
not. So we have to

377
00:23:19.620 --> 00:23:21.990
decide whether or not we
want these models to grow

378
00:23:21.990 --> 00:23:25.950
or not. Oh, but that's
exactly where the human is

379
00:23:26.060 --> 00:23:30.420
very needed in that circle
of building natural language processing

380
00:23:30.420 --> 00:23:36.570
systems. So a it like
I'm in, I'm working in

381
00:23:36.570 --> 00:23:39.540
the past five years on
a system called language understanding

382
00:23:39.540 --> 00:23:42.270
service, which is part of
the Azure services for AI.

383
00:23:42.960 --> 00:23:46.700
And that system is based
on a technology called machine

384
00:23:46.700 --> 00:23:51.020
teaching and machine teaching is
a bit different from machine

385
00:23:51.020 --> 00:23:53.450
learning in the following sense,
it's a new way of

386
00:23:53.450 --> 00:23:58.280
building machine learning models. However,
it pivots on the human

387
00:23:58.280 --> 00:24:01.730
being part of the loop,
because we always think that

388
00:24:01.730 --> 00:24:05.420
for many, many problems are
that knowledge is in the

389
00:24:05.420 --> 00:24:08.630
human experts, rather than in
the data. We don't have

390
00:24:08.810 --> 00:24:12.290
lots of data for many
problems. There are problems. We

391
00:24:12.290 --> 00:24:14.930
do have lots of data
for, but there are many

392
00:24:14.930 --> 00:24:18.740
other problems that we don't
have problems enough data for.

393
00:24:19.310 --> 00:24:21.140
And for those problems where
we don't have a lot

394
00:24:21.140 --> 00:24:24.080
of data, how would you
build an AI model that

395
00:24:24.080 --> 00:24:27.290
understand the world? How would
you teach the machine? Right.

396
00:24:27.740 --> 00:24:31.940
So the only way is
to create a new ecosystem

397
00:24:31.940 --> 00:24:35.000
where you teach the machine
the same way you teach

398
00:24:35.000 --> 00:24:38.420
humans. How do you teach
humans? You teach humans through,

399
00:24:38.450 --> 00:24:43.220
through a language. So in
the LinkedIn selling service that

400
00:24:43.700 --> 00:24:48.940
I'm working on it, we
create a new tool for,

401
00:24:48.940 --> 00:24:52.610
for developers and domain experts
to come and teach the

402
00:24:52.610 --> 00:24:57.440
machine about their domain expertise
in a way that w

403
00:24:57.490 --> 00:25:01.400
with an easy language, with
an easy interaction, with a

404
00:25:01.430 --> 00:25:05.390
portal where you end up
building an AI model, even

405
00:25:05.390 --> 00:25:07.460
if you don't have a
lot of machine learning expertise.

406
00:25:07.790 --> 00:25:10.730
So I'm sorry. I talked
a little bit more about

407
00:25:11.690 --> 00:25:14.450
the service I'm working on,
but the meta point I'm

408
00:25:14.450 --> 00:25:17.880
trying to make here is
like, you can, the model

409
00:25:17.900 --> 00:25:21.560
will always be static because
the machine is not proactive.

410
00:25:21.770 --> 00:25:24.650
The human has to come
back in the loop and

411
00:25:24.770 --> 00:25:28.010
decide to improve the model
based on the real traffic

412
00:25:28.010 --> 00:25:31.760
after the model got deployed.
This is the difference between

413
00:25:31.760 --> 00:25:35.180
the model when deployed and
your child when deployed, because

414
00:25:35.180 --> 00:25:39.530
your child has that feedback
loop happening naturally. But the

415
00:25:39.530 --> 00:25:42.860
model won't have that natural.
Right? You have, the human

416
00:25:42.860 --> 00:25:44.690
has to come back. The
human expert has to come

417
00:25:44.690 --> 00:25:47.740
back and say, Hey, these
are the, the, the, the,

418
00:25:47.920 --> 00:25:51.980
the, the examples of ordinary
that my system did not

419
00:25:51.980 --> 00:25:55.160
predict well, or the customer
was not satisfied with because

420
00:25:55.160 --> 00:25:58.280
we did not get the
order. Right. And then I

421
00:25:58.280 --> 00:26:01.240
have to fix that. Right.
Right. Right. And when my

422
00:26:01.240 --> 00:26:03.370
son is deployed, I love
that I'm going to be

423
00:26:03.370 --> 00:26:06.100
using that all the time.
Now, when he goes out

424
00:26:06.100 --> 00:26:08.800
and he collects knowledge and
comes back and then makes

425
00:26:08.800 --> 00:26:12.130
declarative statements to me about
how the world works. And

426
00:26:12.130 --> 00:26:15.220
we go well, and we
try to gently adjust his

427
00:26:15.430 --> 00:26:18.910
belief system, his well that's
one way, but you might

428
00:26:19.060 --> 00:26:21.370
want to think about it
this other way. That's where

429
00:26:21.370 --> 00:26:24.370
that feedback loop happens. He
then changes his model deploys

430
00:26:24.370 --> 00:26:27.940
again, and comes back and
preferably, and hopefully becomes a

431
00:26:27.940 --> 00:26:30.860
better person as, as he
iterates through, through this, this,

432
00:26:30.890 --> 00:26:35.350
this machine learning slash machine
teaching process. That's exactly right.

433
00:26:35.890 --> 00:26:40.780
And that's how humans are
a crucial part of building

434
00:26:40.780 --> 00:26:44.220
a model that adapts to
the world. Because imagine like,

435
00:26:44.250 --> 00:26:47.880
even when, when you deploy
a model at first, the

436
00:26:47.880 --> 00:26:52.020
semantic of the problem changes
over time. Like pizza orders,

437
00:26:52.020 --> 00:26:55.800
for example, have changed over
the years, we've come across

438
00:26:55.800 --> 00:26:58.740
new topics. We've come across
a new cross. We've come

439
00:26:58.740 --> 00:27:02.160
across. I'm pigging a very
small example here, but we,

440
00:27:02.190 --> 00:27:06.000
but the whole definition and
semantic of what forms a

441
00:27:06.000 --> 00:27:10.320
pizza has, has evolved with
overtime, right? Maybe not, not

442
00:27:10.320 --> 00:27:16.590
significantly but slightly. Right. And
how can you get the,

443
00:27:17.910 --> 00:27:21.300
like people to, to act,
to keep accomplishing the same

444
00:27:21.300 --> 00:27:25.080
task, interacting with the machine,
if the semantic is changing,

445
00:27:25.080 --> 00:27:27.780
or if the distribution is
changing, or even if the

446
00:27:27.780 --> 00:27:31.020
language is changing, I can
do like, I'm sure you

447
00:27:31.020 --> 00:27:34.770
you've come across that. The
type of words that we

448
00:27:34.770 --> 00:27:37.140
used to use when, when
I was a kid have

449
00:27:37.140 --> 00:27:39.750
achieved, the vocabulary has changed
the little bit over, over

450
00:27:39.750 --> 00:27:44.430
the years. And the way
I now like express something,

451
00:27:44.430 --> 00:27:48.000
there's a bunch of sayings
that are very, when you

452
00:27:48.000 --> 00:27:49.950
say it to a human,
they will understand what you

453
00:27:49.950 --> 00:27:53.850
mean. Although it doesn't mean
literally the words, right? So

454
00:27:53.880 --> 00:27:56.480
that, that, that is what
we call a change in

455
00:27:56.490 --> 00:28:00.240
distribution of the data, right?
Because when you train the

456
00:28:00.240 --> 00:28:03.720
model the same way, when
you teach your son some

457
00:28:03.720 --> 00:28:08.190
concepts, if, if these concepts
the change in the world,

458
00:28:08.550 --> 00:28:11.940
your son adapts, because his
mind now collects what we

459
00:28:11.940 --> 00:28:15.210
call model labels. He has,
he sees more examples, more

460
00:28:15.210 --> 00:28:19.560
patterns while the model is
seeing those. But he, the

461
00:28:19.560 --> 00:28:22.710
best the model can do
is to relate this, the

462
00:28:22.710 --> 00:28:26.190
models, the new models, or
the new patterns to the

463
00:28:26.490 --> 00:28:30.060
nearest possible pattern they have
been trained on. And unless

464
00:28:30.060 --> 00:28:33.930
you update your model with
new trend examples, including, especially

465
00:28:33.930 --> 00:28:38.400
the vocabulary, right? The slang
vocabulary, the new words, and

466
00:28:38.670 --> 00:28:41.790
the new trends, even the
new names of singers that,

467
00:28:41.790 --> 00:28:45.450
that appear in the world,
the new albums, like there's

468
00:28:45.450 --> 00:28:48.900
a lot of update of
information that is happening in

469
00:28:48.900 --> 00:28:51.690
the world. Exactly. So how
do you expect your model

470
00:28:51.690 --> 00:28:55.410
to learn about all that
when deployed after 10 years?

471
00:28:55.940 --> 00:28:59.330
Okay. Right. Let me ask
then a, an incredibly naive

472
00:28:59.390 --> 00:29:01.970
question. Let me take this
problem in an inverted, because

473
00:29:01.970 --> 00:29:05.900
people might be listening and
thinking, well, my area that

474
00:29:05.900 --> 00:29:08.930
the bot, the thing, the
natural language device bot that

475
00:29:08.930 --> 00:29:11.810
I need to create is
so constrained. It only needs

476
00:29:11.810 --> 00:29:14.420
to know about 10 words
or 20 words. It only

477
00:29:14.420 --> 00:29:16.550
needs to express, turn this
on and turn that off.

478
00:29:17.030 --> 00:29:19.730
Why can't I use a
simple off the shelf library,

479
00:29:19.730 --> 00:29:21.800
figure out that I got
a string out of this

480
00:29:21.830 --> 00:29:25.520
and a switch statement. How
quickly does that fall, where

481
00:29:25.520 --> 00:29:29.540
you have a deeply constrained
vocabulary of possible intent? How

482
00:29:29.540 --> 00:29:32.720
quickly does that follow? Ah,
that's that will fall over

483
00:29:32.750 --> 00:29:36.710
very quickly if you rely
on it, because basically you're

484
00:29:36.710 --> 00:29:40.760
diminishing the, the power of
natural language. People don't expect

485
00:29:40.760 --> 00:29:43.540
people to talk to your
bot in one way or

486
00:29:43.540 --> 00:29:46.420
two ways or five ways.
If you get your bot

487
00:29:46.420 --> 00:29:49.600
to be deployed, to help
people with, I don't know,

488
00:29:49.630 --> 00:29:55.810
questions about packages in their
internet connection, right. Do you

489
00:29:55.810 --> 00:30:00.520
ex how, how many ways
do you expect your customers

490
00:30:00.760 --> 00:30:04.110
to be, to be asking
the same question? Why did

491
00:30:04.110 --> 00:30:06.480
they meet that same thing?
They're all different. So they'll

492
00:30:06.480 --> 00:30:09.900
ask many different ways. Exactly.
Right? If you, if you

493
00:30:09.900 --> 00:30:12.990
decide that, Oh, you tell
the human like IVR systems,

494
00:30:12.990 --> 00:30:15.210
for example, this is how
we started right. In the

495
00:30:15.210 --> 00:30:18.480
eighties. Right. So if you
go back a little bit

496
00:30:18.480 --> 00:30:21.720
in time, you see that
we've tried to simulate that,

497
00:30:21.750 --> 00:30:24.120
but we diminish the value
of natural eggs. Because back

498
00:30:24.120 --> 00:30:26.940
in the time we did
not have enough technology advancement

499
00:30:27.000 --> 00:30:29.700
that would let us to
do what we do today

500
00:30:29.760 --> 00:30:32.700
with natural language. So in
the past, IVR gives you

501
00:30:32.700 --> 00:30:35.100
a bunch of options and
it's, We are as an

502
00:30:35.100 --> 00:30:37.710
interactive voice response, it's talking
to a bot on the

503
00:30:37.710 --> 00:30:40.950
phone, right? Yeah. And then
it gives you a menu.

504
00:30:40.950 --> 00:30:43.500
And then you can not
say anything outside that menu.

505
00:30:43.650 --> 00:30:46.800
Right. And you even choose
the menu is with numbers.

506
00:30:47.220 --> 00:30:49.650
Sometimes it's not even fair
to express the word in

507
00:30:49.650 --> 00:30:51.690
the menu. You have to
choose a number because it's

508
00:30:51.690 --> 00:30:54.630
easy to interpret a number,
then a word that is

509
00:30:54.660 --> 00:31:00.330
open-ended right. Or like generic.
So with that, that era

510
00:31:00.570 --> 00:31:04.320
is kind of over. And
we are in a new

511
00:31:04.320 --> 00:31:09.380
era where natural language can
be understood by, by, by

512
00:31:09.390 --> 00:31:12.690
machines, given the statistical models
and the advancement in machine

513
00:31:12.690 --> 00:31:17.070
learning. And back to your
example of that, you, you

514
00:31:17.070 --> 00:31:19.380
said, I can write a
switch statement and put it

515
00:31:19.680 --> 00:31:23.160
to, to deploy to the
world that won't be far

516
00:31:23.160 --> 00:31:26.730
from what IVR could have
done. Right. But if you

517
00:31:26.730 --> 00:31:31.650
really want a bot that
simulates an agent that I'm

518
00:31:31.650 --> 00:31:35.730
talking to, then it's a
different story because you cannot

519
00:31:36.000 --> 00:31:39.510
anticipate how many ways people
would mean the same thing

520
00:31:39.540 --> 00:31:41.700
would say different things to
mean the same thing, the

521
00:31:41.700 --> 00:31:46.050
same intent or same number
of context, words that make

522
00:31:46.050 --> 00:31:50.880
that intent understood by the
machine. It sounds like from

523
00:31:50.880 --> 00:31:52.920
the point of view of
a developer who is interested

524
00:31:52.920 --> 00:31:55.060
in this space, of course,
they could dig deeply in

525
00:31:55.060 --> 00:31:57.090
it and get a PhD
or multiple PhDs in this

526
00:31:57.090 --> 00:31:58.830
space. But if I want
to be a user of

527
00:31:58.830 --> 00:32:03.660
this language, understanding as a
service is a thing that

528
00:32:03.660 --> 00:32:08.010
exists, I could take advantage
of this power without understanding

529
00:32:08.010 --> 00:32:11.760
it as deeply. Hm. You
know, that has been exactly

530
00:32:11.760 --> 00:32:15.900
the reason five, six years
ago, we've started that project

531
00:32:15.930 --> 00:32:18.930
of the language on the
same service that became later

532
00:32:18.930 --> 00:32:21.780
a product that, that that's
as part of the Azure

533
00:32:21.780 --> 00:32:25.470
products for AI, that has
been exactly the motivation. I

534
00:32:25.470 --> 00:32:29.550
think AI has been limited
to people who have PhDs

535
00:32:29.550 --> 00:32:33.420
in the past, and that
limited what we can do

536
00:32:33.420 --> 00:32:37.270
as a community and as
a society to push the

537
00:32:37.320 --> 00:32:41.180
boundaries of how can help
humans in their day to

538
00:32:41.180 --> 00:32:44.420
day, right. That was a
limitation, a huge limitation, because

539
00:32:44.480 --> 00:32:48.020
how many PhDs do you
get every year? And every

540
00:32:48.020 --> 00:32:51.140
company be able to hire
those people, because those people

541
00:32:51.170 --> 00:32:54.410
typically would go to other
like the tech giants and

542
00:32:54.410 --> 00:32:58.760
work for them. Right. So
how much can we do

543
00:32:58.760 --> 00:33:03.740
to scale, basically, this is
the question and that that

544
00:33:03.830 --> 00:33:08.210
question has been bothering. So
it's like I'm growing a

545
00:33:08.210 --> 00:33:13.010
plant. The first stage was
can we create the science

546
00:33:13.040 --> 00:33:16.250
to, to get machines, to
understand natural language with different

547
00:33:16.250 --> 00:33:20.300
variations. We spent 20 years
doing that, right? And then

548
00:33:20.330 --> 00:33:22.130
we got to a stage
where, Oh, we can do

549
00:33:22.130 --> 00:33:25.490
that. But only people who
really, really have deep knowledge

550
00:33:25.490 --> 00:33:28.010
about machine learning and natural
language processing can do this,

551
00:33:28.520 --> 00:33:32.330
or now the next stages,
how can we enlarge the

552
00:33:32.390 --> 00:33:36.320
base of users who can
build these systems without that

553
00:33:36.320 --> 00:33:40.370
knowledge? Now we're at that
stage. And I think that's

554
00:33:40.370 --> 00:33:43.220
why, for example, we, we
got to work on that

555
00:33:43.220 --> 00:33:48.620
machine teaching technology like 15
years ago. And it became

556
00:33:48.710 --> 00:33:51.680
apparent after 10 years that
we, Oh, it's ready to

557
00:33:51.680 --> 00:33:55.790
be deployed to some users.
And that's how we created

558
00:33:55.790 --> 00:34:00.830
the product. But the goal
there was to democratize our

559
00:34:00.860 --> 00:34:05.000
natural language processing so that
bots and automations can be

560
00:34:05.000 --> 00:34:10.010
possible for like a regular
developers. People who want to

561
00:34:10.040 --> 00:34:14.300
even business analysts and domain
experts who really want to

562
00:34:14.300 --> 00:34:18.820
automate some of their workflows.
Fantastic. And folks can learn

563
00:34:18.820 --> 00:34:25.480
about that at Lewis, L
I s.ai or lewis.ai. Thank

564
00:34:25.480 --> 00:34:28.300
you so much for chatting
with me today. Thank you.

565
00:34:28.390 --> 00:34:32.380
That was very entertaining. I've
been chatting with dr. Rihad

566
00:34:32.380 --> 00:34:35.830
Monsour and this has been
another episode of Hanselminutes and

567
00:34:35.830 --> 00:34:49.090
we'll see you again next
week. <inaudible>.

