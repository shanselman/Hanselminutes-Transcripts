WEBVTT FILE

1
00:00:00.330 --> 00:00:03.480
Hi, this is Scott. I
really appreciate our sponsors because

2
00:00:03.480 --> 00:00:06.450
they make the show possible.
Today's show is sponsored by

3
00:00:06.450 --> 00:00:10.620
developer express, become a UI
superhero with dev express controls

4
00:00:10.620 --> 00:00:15.150
and libraries. Deliver elegant.net solutions
that address customer needs today

5
00:00:15.540 --> 00:00:19.620
by leveraging your existing knowledge,
you can build next generation

6
00:00:19.650 --> 00:00:23.730
touch enabled solutions for tomorrow,
you can download your free

7
00:00:23.850 --> 00:00:45.990
30 day trial@dxdothanselminutes.com. That's dx.hanselminutes.com.
Hi, this is Scott Hanselman.

8
00:00:45.990 --> 00:00:48.390
This is another episode of
Hansel minutes. And today I

9
00:00:48.390 --> 00:00:51.960
am talking with Camille Eddy.
She is a mechanical engineering

10
00:00:52.050 --> 00:00:55.830
intern at a mysterious company
called X. I think it's

11
00:00:55.830 --> 00:00:58.470
part of the alphabet and
a, but it was also

12
00:00:58.470 --> 00:01:01.560
a machine learning into turn
at HP and worked on

13
00:01:01.560 --> 00:01:04.020
robots there for a couple
of years as well. How

14
00:01:04.020 --> 00:01:07.980
are you? I'm good. Thank
you. You're you're LinkedIn is

15
00:01:08.250 --> 00:01:10.890
way cooler than my LinkedIn
and very nicely curated. I'm

16
00:01:10.890 --> 00:01:13.440
actually gonna use your LinkedIn
as the guide to update

17
00:01:13.440 --> 00:01:15.630
my own. You have done
a ton of cool stuff.

18
00:01:16.020 --> 00:01:20.490
Thanks. Yeah. So your focus
has been listening to it's

19
00:01:20.490 --> 00:01:23.340
kind of split in the
sense of it's robots and

20
00:01:23.340 --> 00:01:26.040
electronics and mechanical engineering, but
also machine learning. Are those

21
00:01:26.040 --> 00:01:28.050
separate things or are they
the same thing that worked

22
00:01:28.050 --> 00:01:31.850
together? I like to think
of my kind of career

23
00:01:31.850 --> 00:01:36.350
path as the idea of
integrating machine learning with robots.

24
00:01:36.650 --> 00:01:39.560
So it's bringing the hardware
and the software together and

25
00:01:39.560 --> 00:01:42.470
being able to shuttle that
middle line and look in

26
00:01:42.470 --> 00:01:45.350
both directions and see what's
coming down the road. Hmm.

27
00:01:45.380 --> 00:01:46.820
Does that mean that you
have to do like double

28
00:01:46.820 --> 00:01:48.920
the work? Cause I know
people who do robots and

29
00:01:48.920 --> 00:01:51.830
they think about soldering and
electronics and firmware, and I

30
00:01:51.830 --> 00:01:53.420
know people who do the
software for them, but never

31
00:01:53.420 --> 00:01:56.210
have to touch the physical
robot you're in the middle.

32
00:01:56.690 --> 00:01:58.760
Yeah. I don't think I
have to do double the

33
00:01:58.760 --> 00:02:01.880
work, but it does allow
me to kind of look

34
00:02:01.880 --> 00:02:06.440
at the ideas as they're
coming down the pipeline and

35
00:02:06.440 --> 00:02:10.070
start looking at how we
would implement them. So, you

36
00:02:10.070 --> 00:02:13.220
know, sometimes I do work
on teams though, where a,

37
00:02:13.220 --> 00:02:15.950
especially like at HP someone,
what else would come in?

38
00:02:15.950 --> 00:02:18.350
And I say, this is
my vision. And they'd help

39
00:02:18.360 --> 00:02:20.750
me kind of get there.
And I focus maybe more

40
00:02:20.750 --> 00:02:22.700
on the hardware part while
they're focusing more on the

41
00:02:22.700 --> 00:02:26.240
software part. So it allows
me to still be able

42
00:02:26.240 --> 00:02:29.030
to direct some of the
vision, even as an intern,

43
00:02:29.720 --> 00:02:34.190
as a very young engineering
person, but be able to,

44
00:02:34.490 --> 00:02:37.820
to help. And, and then
also focus on my expertise,

45
00:02:37.820 --> 00:02:41.930
which is hardware and mechanical
engineering. And you have a

46
00:02:41.930 --> 00:02:44.510
degree from Boise state, but
you were, it looks like

47
00:02:44.510 --> 00:02:47.660
you've gone to schools and
visited schools like MIT and

48
00:02:47.660 --> 00:02:51.860
participated in different undergraduate programs
and worked at multiple schools.

49
00:02:52.580 --> 00:02:54.650
Yeah, actually I don't have
my degree yet. I'm still

50
00:02:54.650 --> 00:02:57.530
in the process of getting
my degree, but yeah, I'm

51
00:02:57.530 --> 00:03:01.360
really big on traveling. Idaho
is a pretty small place.

52
00:03:02.290 --> 00:03:06.190
It's definitely an isolating place.
So I take advantage of

53
00:03:06.190 --> 00:03:10.930
every opportunity I get requests
to speak somewhere or I'll

54
00:03:10.930 --> 00:03:13.660
apply to a program and
I'll definitely take the opportunity

55
00:03:13.660 --> 00:03:16.960
to go outside of my
community and explore. And I

56
00:03:16.980 --> 00:03:20.770
found a lot of friends
that way. And then also

57
00:03:20.770 --> 00:03:23.560
learned a lot. I don't
think I really would've learned

58
00:03:24.040 --> 00:03:28.960
about culture bias or artificial
intelligence and robots. If I

59
00:03:28.960 --> 00:03:31.240
had just stayed in my
one community where, you know,

60
00:03:31.240 --> 00:03:33.840
it's pretty self sufficient. So
being able to go outside

61
00:03:33.850 --> 00:03:37.660
my community, learn from others,
hear other stories that's been

62
00:03:37.660 --> 00:03:40.080
really helpful. That's a really
good point. So let's talk

63
00:03:40.080 --> 00:03:42.420
about that. Let's talk about
going outside your community. So,

64
00:03:42.630 --> 00:03:46.170
you know, I'm in Portland,
Oregon, which is a certain

65
00:03:46.170 --> 00:03:49.650
flavor of town and you're
in Boise, which is even

66
00:03:49.650 --> 00:03:53.640
more intensely a certain flavor.
What kinds of cultural bias

67
00:03:53.640 --> 00:03:55.950
does one bump into when
they, when they're living in

68
00:03:55.950 --> 00:03:57.480
a small town like that
and how does it affect

69
00:03:57.480 --> 00:04:03.030
your life? Sure. So very
small changes tend to make

70
00:04:03.210 --> 00:04:07.260
really big ripple effects. So
for example, Boise is Boise,

71
00:04:07.260 --> 00:04:11.460
Idaho, and the general area
around Boise is a refugee

72
00:04:11.460 --> 00:04:15.300
relocation site. So refugees, when
they're coming to America, Boise

73
00:04:15.300 --> 00:04:18.300
is one of those places
that they're they're dropped off

74
00:04:18.300 --> 00:04:21.570
at. And so that, for
that reason, we have a

75
00:04:21.570 --> 00:04:25.500
really large refugee population, but
Boise is one of those

76
00:04:25.500 --> 00:04:31.830
places where the African American
population is under 2%. So

77
00:04:31.860 --> 00:04:35.640
for us, we start to
see differences in how people

78
00:04:35.640 --> 00:04:37.680
interact with us. So even
if I'm walking into a

79
00:04:37.680 --> 00:04:41.370
coffee shop, I can notice
some of the stigmas around

80
00:04:41.370 --> 00:04:44.730
being a refugee because it's
directed at me. People think

81
00:04:44.730 --> 00:04:49.050
that I'm a refugee. And
so that, that that's bad.

82
00:04:49.050 --> 00:04:52.320
And I would totally wear
that mantle, but you can

83
00:04:52.320 --> 00:04:54.630
start to see where people
are at, like asking you

84
00:04:54.630 --> 00:04:56.430
if you know, more than
one language asking you what

85
00:04:56.430 --> 00:04:59.370
country you're from, different things
like that, that pop up

86
00:04:59.370 --> 00:05:02.550
a little bit more often.
And it's because of that,

87
00:05:02.550 --> 00:05:06.540
just kind of slow ripple
effect, but that also provides

88
00:05:06.540 --> 00:05:11.250
the opportunity to do really
great community outreach. So at

89
00:05:11.250 --> 00:05:15.240
my school at Boise state,
we noticed the problems. And

90
00:05:15.240 --> 00:05:17.700
so one of the things
we did to help welcome

91
00:05:17.700 --> 00:05:20.430
more people was actually changed
the name of our club.

92
00:05:20.670 --> 00:05:23.190
It was the black student
union and it became the

93
00:05:23.190 --> 00:05:27.630
Afro black student Alliance. And
so there, we recognized coming

94
00:05:27.630 --> 00:05:31.020
together, two cultures, the African
American culture and the African

95
00:05:31.020 --> 00:05:35.550
culture. And that's been really
helpful to bring a sense

96
00:05:35.550 --> 00:05:38.880
of identity for all students
in our schools that had

97
00:05:38.880 --> 00:05:42.930
that African heritage. And again,
it makes small ripple effects

98
00:05:42.960 --> 00:05:46.650
because from there now we're
visible where we're taking up

99
00:05:46.650 --> 00:05:50.910
space. And I think that's
helping, at least our small

100
00:05:50.910 --> 00:05:55.350
committee within the university have
a better, more inclusive atmosphere.

101
00:05:56.760 --> 00:05:59.210
Now someone might hear that
story and think, Oh, well,

102
00:05:59.270 --> 00:06:01.340
I mean, she's gone. She
changed the name of a

103
00:06:01.340 --> 00:06:04.280
club from one thing to
another. Did that make a

104
00:06:04.280 --> 00:06:06.260
ripple effect? Did people notice
and say, wait a second,

105
00:06:06.260 --> 00:06:08.900
maybe I could join. Did
you have, you know, you

106
00:06:08.900 --> 00:06:12.290
know, allies and accomplices, Joanne.
And did you have Africans

107
00:06:12.320 --> 00:06:14.210
say, Oh, I didn't realize
this was for me because

108
00:06:14.210 --> 00:06:16.940
black meant African-American, but now
I realized that it is

109
00:06:16.940 --> 00:06:19.930
for me and then come
and join your club. Yeah.

110
00:06:20.620 --> 00:06:24.970
There's this thing about identity
and being able to just

111
00:06:24.970 --> 00:06:28.240
recognize someone's identity and say
you belong as you are.

112
00:06:28.570 --> 00:06:32.650
And yeah, we definitely, the
club itself was filling this

113
00:06:32.650 --> 00:06:36.430
divide and this polarization where
we would walk past, like

114
00:06:36.550 --> 00:06:39.070
me and African American student
would walk past an African

115
00:06:39.070 --> 00:06:41.200
student and we wouldn't say
hello to each other. We

116
00:06:41.200 --> 00:06:43.330
wouldn't talk to each other.
In fact, there might even

117
00:06:43.330 --> 00:06:46.210
be a little bit of
animosity and ignoring each other.

118
00:06:46.390 --> 00:06:49.870
And so the small thing
helped us bridge a gap

119
00:06:49.870 --> 00:06:53.620
between cultures and in a
way that said, you're welcome

120
00:06:53.830 --> 00:06:57.520
with your identity. And you
don't have to take on

121
00:06:57.580 --> 00:06:59.800
like the African American identity.
If you don't want to,

122
00:06:59.800 --> 00:07:02.320
you don't need to assimilate
to this. You can still

123
00:07:02.320 --> 00:07:05.260
retain kind of your identity
while being a part of

124
00:07:05.260 --> 00:07:09.190
a greater community. And I
think that really helped, helped

125
00:07:09.190 --> 00:07:13.420
us all because then also
myself, right, as more students

126
00:07:13.420 --> 00:07:17.170
are coming in, you know,
where do I belong? Do

127
00:07:17.170 --> 00:07:19.990
I still belong as an
African American student? So I

128
00:07:19.990 --> 00:07:23.410
think both sides were able
to kind, kinda kind of

129
00:07:23.410 --> 00:07:26.590
grow together. And hopefully you
get to the point where

130
00:07:26.590 --> 00:07:29.020
there's a goal where everyone
talks to everyone. Then when

131
00:07:29.020 --> 00:07:31.570
they walk by and respects
their, where they came from

132
00:07:31.570 --> 00:07:33.910
and their identity, and everyone
can walk into a coffee

133
00:07:33.910 --> 00:07:37.630
shop and you know, feel
comfortable. Exactly. And they don't

134
00:07:37.630 --> 00:07:40.570
have to feel like there's
some type of like puzzle

135
00:07:40.600 --> 00:07:43.690
that's too hard to solve.
Like who are you? Like,

136
00:07:43.720 --> 00:07:46.420
does it matter? No, actually
it doesn't, we're actually just

137
00:07:46.420 --> 00:07:49.330
having great times and, and
we all have similar interests

138
00:07:49.330 --> 00:07:51.040
and we have ways that
we can learn from each

139
00:07:51.040 --> 00:07:55.420
other. So I'm imagining the
person at the coffee shop.

140
00:07:55.540 --> 00:08:00.070
Okay. And I'm imagining them
as a machine learning algorithm

141
00:08:00.250 --> 00:08:02.800
and people are coming in
and they are training their

142
00:08:02.800 --> 00:08:07.660
algorithm and developing biases that
they might have based on

143
00:08:07.660 --> 00:08:10.240
a person comes in and
they say, Oh, Brown person,

144
00:08:10.840 --> 00:08:12.640
do I have to deal
with their accent? Do I

145
00:08:12.640 --> 00:08:15.070
have to, you know, who
is this person? Where are

146
00:08:15.070 --> 00:08:17.560
they from? They they're building
a model in their minds.

147
00:08:17.560 --> 00:08:19.690
And then of course you
open your mouth, which gives

148
00:08:19.690 --> 00:08:23.530
them additional data that builds
their, their model. Is that

149
00:08:23.530 --> 00:08:28.180
a reasonable, a reasonable sense
of what a machine learning

150
00:08:28.180 --> 00:08:30.160
model might be doing if
it were a barista at

151
00:08:30.160 --> 00:08:33.280
a coffee shop? Yeah. I
kinda like that. Yeah. There's

152
00:08:33.280 --> 00:08:37.210
actually this conversation about data
sets and how diverse are

153
00:08:37.210 --> 00:08:41.350
your data sets? So in
a more machine learning type

154
00:08:41.350 --> 00:08:44.590
example, I remember I went
to a conference in San

155
00:08:44.590 --> 00:08:48.040
Francisco and there was an
engineer from Pinterest and he

156
00:08:48.040 --> 00:08:50.560
talked about how he had
built a model to recognize

157
00:08:50.560 --> 00:08:53.650
boats, boats, boats, boats, and
that's what it was supposed

158
00:08:53.650 --> 00:08:58.770
to recognize. He had a
data training set and it

159
00:08:58.770 --> 00:09:04.020
wasn't as effective as when
he added a lot more

160
00:09:04.470 --> 00:09:07.290
points to the satisfied. So
maybe he had a hundred

161
00:09:07.290 --> 00:09:10.350
and he added a thousand
to the status set. And

162
00:09:10.350 --> 00:09:14.580
then he started to see
an increased familiarity with these

163
00:09:14.580 --> 00:09:16.860
objects so that it was
even more correct in that

164
00:09:16.890 --> 00:09:19.920
it was spot on. So
going back to the barista

165
00:09:19.950 --> 00:09:26.100
idea, not having exposure to
other cultures and other accents

166
00:09:26.340 --> 00:09:28.890
might make it more of
an ordeal like, Oh, this

167
00:09:28.890 --> 00:09:31.550
is unfamiliar to me. I
don't know how to, I

168
00:09:31.550 --> 00:09:34.140
don't know how to understand
their intentions. I don't know

169
00:09:34.350 --> 00:09:37.980
if they're happy or sad
or if they're they're, you

170
00:09:37.980 --> 00:09:42.090
know, impatient, but as they
gained more exposure to them,

171
00:09:42.090 --> 00:09:46.560
then they can be more
realistic in their guesses. Like,

172
00:09:46.830 --> 00:09:49.800
Oh, I seen this before
and I've interacted with them.

173
00:09:49.950 --> 00:09:52.980
We've had great conversations. We've
had bad conversations, but now

174
00:09:53.130 --> 00:09:58.230
overall, I don't walk into
it with the wrong preconceived

175
00:09:58.230 --> 00:10:00.900
notions. Cause we all have
preconceived notions about each other.

176
00:10:00.900 --> 00:10:03.720
That's not going to change,
but what can change is

177
00:10:03.720 --> 00:10:07.130
if we're wrong about our
guesses, You know, I feel

178
00:10:07.130 --> 00:10:10.580
like you and I are
developing a very effective extended

179
00:10:10.580 --> 00:10:12.650
metaphor here. Cause I, I
really am trying to think

180
00:10:12.650 --> 00:10:15.440
that this works. I've had
situations where I've come into

181
00:10:15.440 --> 00:10:18.740
contact with folks in my
wife's family. My wife is

182
00:10:18.740 --> 00:10:23.360
African and I felt like,
gosh, maybe uncle so-and-so doesn't

183
00:10:23.360 --> 00:10:25.940
like me. And then my
wife will be like, Nope,

184
00:10:26.000 --> 00:10:29.360
that's just him. Right. And
then you meet more, you

185
00:10:29.360 --> 00:10:31.250
know, to be, to be
an over, to be a

186
00:10:31.250 --> 00:10:33.260
generalist here, you meet more
African uncles and you go,

187
00:10:33.290 --> 00:10:36.800
Oh, okay, well that's how
Zimbabwean uncles act. Or I've

188
00:10:36.800 --> 00:10:41.300
met a hundred now 1,012
hundred over the last 20

189
00:10:41.300 --> 00:10:43.280
years. And I go, okay,
I'm pretty sure that I

190
00:10:43.280 --> 00:10:46.730
have a good dataset of
how people act and how

191
00:10:46.730 --> 00:10:49.220
people behave so that I'm
not going to necessarily assume

192
00:10:49.220 --> 00:10:53.810
that one bad interaction ruins
the dataset. Absolutely. Yeah. That's

193
00:10:53.810 --> 00:10:57.620
brilliant. Yeah. That's interesting. Okay.
So then when thinking about

194
00:10:57.620 --> 00:11:00.950
machine learning and then applying
it to things like cultural

195
00:11:00.950 --> 00:11:03.890
bias, if you have a
small data set, what kind

196
00:11:03.890 --> 00:11:06.230
of bad stuff can happen
when you're training an algorithm?

197
00:11:07.370 --> 00:11:09.830
So some of the things
I've seen pointed out before

198
00:11:09.860 --> 00:11:13.820
are, you'll, you'll be wrong
of course, about your guesses.

199
00:11:13.820 --> 00:11:19.430
So it will overreach another
example of overreaching though, besides

200
00:11:19.430 --> 00:11:23.540
just being wrong is making
the wrong assertions. So there

201
00:11:23.540 --> 00:11:26.690
is an example of this
training set called the to

202
00:11:26.720 --> 00:11:29.660
back. And it was actually
a training set that was

203
00:11:29.660 --> 00:11:34.910
used on the Google news
posts from a certain time

204
00:11:34.910 --> 00:11:39.110
period. And they wanted to
access training set, you know,

205
00:11:39.830 --> 00:11:42.770
about, about the information that
taken in from Google news.

206
00:11:42.770 --> 00:11:47.510
So it basically, it, it,
it got all this data

207
00:11:47.510 --> 00:11:50.720
from Google news. Like they
imported it in and they

208
00:11:50.720 --> 00:11:55.930
asked a basic question, man
is to computer programmer, as

209
00:11:55.930 --> 00:12:00.040
woman is to what an
answered homemaker. And this is

210
00:12:00.040 --> 00:12:02.800
actual training set and a
story that you can look

211
00:12:02.800 --> 00:12:06.400
up online. But the answers
that it gave were inherently

212
00:12:06.400 --> 00:12:13.480
sexist. So overreached into its
answers were too simplistic. You

213
00:12:13.480 --> 00:12:17.890
know, it was interpreting our
own language, our English language

214
00:12:17.890 --> 00:12:20.230
and the way we write
and talk in the news

215
00:12:21.160 --> 00:12:25.060
to have an overtly sexist
meaning. And that is a

216
00:12:25.060 --> 00:12:30.160
very interesting problem when you're
not necessarily trying to hit,

217
00:12:30.370 --> 00:12:32.830
you know, the wrong answer,
but you inadvertently do that.

218
00:12:33.070 --> 00:12:38.080
You mistakenly make a social
taboo, or you're not as

219
00:12:38.080 --> 00:12:40.180
progressive as you think you're
making choices that you don't

220
00:12:40.180 --> 00:12:42.700
understand why you're making them.
That's kind of what can

221
00:12:42.700 --> 00:12:45.970
happen if you have a
limited training. That is that

222
00:12:46.710 --> 00:12:52.860
Interesting. So what about that,
that, that legendary joke, this

223
00:12:52.860 --> 00:12:54.870
sounds like that legendary joke
where someone says, you know,

224
00:12:54.870 --> 00:12:57.450
a child, a boy is
taken in for an operation

225
00:12:57.750 --> 00:13:00.030
and the surgeon says, I
can't do this surgery. This

226
00:13:00.030 --> 00:13:03.030
is my son. And then
we all go, Oh, how's

227
00:13:03.030 --> 00:13:04.770
that possible? You know, it's
like, well, the surgeon is

228
00:13:04.770 --> 00:13:10.530
a woman, historically that may
have been true. That's why

229
00:13:10.710 --> 00:13:14.160
a riddle or a joke
like that would work. And

230
00:13:14.160 --> 00:13:17.100
then if you take a
large enough dataset, that makes

231
00:13:17.100 --> 00:13:19.920
that true, but it isn't
ideal or it is inherently

232
00:13:19.920 --> 00:13:23.760
sexist. How do you, how
do you resolve the, it

233
00:13:23.760 --> 00:13:26.310
is, you know, it has
historical truth or some surgical

234
00:13:26.310 --> 00:13:28.950
truth, but it is definitely
not cool. And it's not

235
00:13:28.950 --> 00:13:31.800
the kind of society that
we want to build. How

236
00:13:31.800 --> 00:13:34.380
do you answer people who
might say, well, you know,

237
00:13:34.380 --> 00:13:37.380
but it's kind of see
where that's true. Right? I

238
00:13:37.380 --> 00:13:42.780
think that's where signaling comes
in. Like where you, you

239
00:13:42.780 --> 00:13:44.910
say like what your standing
wants to be. And that's,

240
00:13:45.090 --> 00:13:47.310
that's a tricky part. That's
where people might not. Can

241
00:13:47.310 --> 00:13:49.680
you explain signaling? I don't
understand what that means. Yeah.

242
00:13:49.680 --> 00:13:52.560
So signaling is saying, I'm
taking a stand. This is

243
00:13:52.560 --> 00:13:54.330
a line in the sand
that I'm going to draw.

244
00:13:54.450 --> 00:13:57.150
And I want to be
on this side of the

245
00:13:57.150 --> 00:14:00.330
line. I want my world
to be better. And that's

246
00:14:00.330 --> 00:14:03.300
where I think the power
and the influence of engineers

247
00:14:03.300 --> 00:14:08.880
come in is because we're
creating paradigms are creating trends

248
00:14:09.510 --> 00:14:12.840
before it even goes out
to mass distribution, before it

249
00:14:12.840 --> 00:14:15.270
even goes out to public.
How I design my product

250
00:14:15.510 --> 00:14:18.810
to interact with someone's culture
or what someone's body sees

251
00:14:19.920 --> 00:14:24.000
is going to set. It's
actually maybe even going to

252
00:14:24.000 --> 00:14:26.520
set what truth is for
the next 10 to 20

253
00:14:26.520 --> 00:14:28.500
years, I product is going
to be used based on

254
00:14:28.500 --> 00:14:33.360
how P how my, my
product is interpreting information and

255
00:14:33.360 --> 00:14:36.120
using information and giving that
to people because we believe

256
00:14:36.120 --> 00:14:39.810
what we see. So that's
where you can signal and

257
00:14:39.810 --> 00:14:42.060
say, this is what I
want life to be. And

258
00:14:42.060 --> 00:14:44.040
that's where people get, you
know, if the, you know,

259
00:14:44.310 --> 00:14:48.450
that's tricky, you're, you're studying
set, setting, ethical standards that

260
00:14:48.450 --> 00:14:51.350
maybe I don't necessarily agree
with, but I think it's

261
00:14:51.350 --> 00:14:54.680
something that even if we
don't like that, we have

262
00:14:54.680 --> 00:14:57.230
to make that choice. We
shouldn't act like we're not

263
00:14:57.230 --> 00:15:00.350
making those choices every day.
We are making choices about

264
00:15:00.500 --> 00:15:05.090
who we're excluding and including
every day. And it's better

265
00:15:05.090 --> 00:15:08.630
to go into that intentionally
versus just letting whatever happened

266
00:15:08.750 --> 00:15:12.760
happened. Okay. That's, that's, that's
very, that's a very good

267
00:15:12.760 --> 00:15:18.130
explanation. So then signaling is
imposing our, our, our ideal

268
00:15:18.160 --> 00:15:22.270
scenario, our belief system onto
the model and steering it

269
00:15:22.270 --> 00:15:25.540
correctly. It might go in
determine that, you know, babies

270
00:15:25.540 --> 00:15:28.180
are a great source of
protein or something bizarre like

271
00:15:28.180 --> 00:15:30.220
that, which may or may
not be true. And babies

272
00:15:30.220 --> 00:15:32.320
are cute and I want
to eat their cheeks, but

273
00:15:32.350 --> 00:15:35.350
still we know don't eat
babies. Right, Right. We have,

274
00:15:35.380 --> 00:15:38.770
we have our own standard
and our own blood filters

275
00:15:38.980 --> 00:15:41.680
for what's right. And what's
wrong. And we can still

276
00:15:41.680 --> 00:15:46.120
impose it. I heard two
good examples of machine learning,

277
00:15:46.120 --> 00:15:48.730
gone wrong that I thought
I'd run by you. One

278
00:15:48.730 --> 00:15:51.160
of them was someone trying
to train an image recognition

279
00:15:51.160 --> 00:15:54.700
system to, to figure out
Russian tanks versus us tanks.

280
00:15:55.450 --> 00:15:56.650
But then when they did
it out in the real

281
00:15:56.650 --> 00:15:59.050
world, it failed. And it
turned out that all the

282
00:15:59.050 --> 00:16:00.670
pictures of the U S
tanks were taken in the

283
00:16:00.670 --> 00:16:02.950
daytime and all the Russian
tanks were at night. So

284
00:16:02.950 --> 00:16:04.570
all they did was teach
the thing how to tell

285
00:16:04.570 --> 00:16:07.570
day from night. Yes. Oh,
they do. That's a wonderful

286
00:16:07.570 --> 00:16:12.700
example. Another example I heard
was university of Washington, their

287
00:16:12.700 --> 00:16:16.300
mascot is the Huskies. And
they had researchers that wanted

288
00:16:16.300 --> 00:16:19.840
to detect Huskies from dogs.
So they put it through

289
00:16:19.840 --> 00:16:22.900
an algorithm test and it
was super accurate. And so

290
00:16:22.930 --> 00:16:24.550
they said, Oh, this is
a Husky. This is a

291
00:16:24.550 --> 00:16:27.280
dog. And it was correct.
But then they came to

292
00:16:27.280 --> 00:16:30.340
look at what it did
to make those decisions. And

293
00:16:30.340 --> 00:16:31.870
they found out that they
had made a really good

294
00:16:31.870 --> 00:16:34.810
snow detector. If there was
snow, it was a Husky.

295
00:16:34.930 --> 00:16:36.610
And if there wasn't, it
was a dog. I see.

296
00:16:36.700 --> 00:16:39.610
Yeah. Yeah. That's another one
that did the same exact

297
00:16:39.610 --> 00:16:41.920
thing where they were training
the subway platform to see

298
00:16:41.920 --> 00:16:44.560
whether it has people or
not, but they forgot that

299
00:16:44.560 --> 00:16:47.470
the wall clock was visible.
So they just taught the

300
00:16:47.470 --> 00:16:51.160
law, the model, how to
read the clock. Oh, that's

301
00:16:51.160 --> 00:16:56.530
amazing. So that leads into
transparency and that transparency in

302
00:16:56.530 --> 00:16:59.410
AI is actually an up
and coming technology you can

303
00:16:59.410 --> 00:17:01.570
think of, but that's the
technology now. And what it

304
00:17:01.570 --> 00:17:04.900
seeks to do with what
this particular technology seeks to

305
00:17:04.900 --> 00:17:08.470
do is to understand why
your algorithm makes the choice

306
00:17:08.470 --> 00:17:11.680
that it makes. So the
example I usually give when

307
00:17:11.680 --> 00:17:13.870
I'm giving a talk is
I'll take a picture of

308
00:17:13.870 --> 00:17:18.220
a speed sign and say,
you have an image recognition

309
00:17:18.250 --> 00:17:21.220
software. That's using machine learning
and it's supposed to detect

310
00:17:21.220 --> 00:17:24.340
the speed sign and how
it works is it breaks

311
00:17:24.340 --> 00:17:27.640
up the speed science, recognizable
parts. Like I recognize the

312
00:17:27.640 --> 00:17:30.670
font. I recognize bold the
bold lettering, like the black

313
00:17:30.670 --> 00:17:34.000
on white. I recognize like
the general layout of it.

314
00:17:34.210 --> 00:17:36.100
I recognize that there's a
five and they're, what's the

315
00:17:36.100 --> 00:17:39.490
word speed. So these little
parts are broken up and

316
00:17:39.490 --> 00:17:42.850
put into this, think of
it like a database. Then

317
00:17:42.910 --> 00:17:46.630
it makes a prediction of
whether it's a speed sign

318
00:17:46.690 --> 00:17:48.820
or it's not a speed
sign. And from there you

319
00:17:48.820 --> 00:17:52.350
get accuracy, like, so this
many times it gets the

320
00:17:52.350 --> 00:17:56.520
right number. It has this
amount of accuracy. So what

321
00:17:56.760 --> 00:18:00.420
explainable AI does, and it
seeks to pull out that

322
00:18:00.420 --> 00:18:04.440
database, what it used to
make those choices in order

323
00:18:04.440 --> 00:18:06.300
to say that it was
a speed sign or not.

324
00:18:06.690 --> 00:18:09.000
And that's really important. Something
that we haven't really thought

325
00:18:09.000 --> 00:18:12.330
about as of yet is
why did our algorithm make

326
00:18:12.330 --> 00:18:14.910
the choice that it made?
Why wasn't affirmative? Why was

327
00:18:14.910 --> 00:18:18.120
it negative? And sometimes like
in the examples we just

328
00:18:18.120 --> 00:18:21.900
showed, it's not actually basing
it on what we think

329
00:18:21.900 --> 00:18:24.180
it's basing it on. And
it can still have a

330
00:18:24.180 --> 00:18:28.650
great accuracy, but not necessarily
be detailed on the details

331
00:18:28.650 --> 00:18:32.060
that we want it to
be. Okay. So if I

332
00:18:32.060 --> 00:18:36.160
understand that correctly, right now,
we think of AI and,

333
00:18:36.160 --> 00:18:38.900
and we, we, the, the
non machine learning experts like

334
00:18:38.900 --> 00:18:42.440
yourself, me just regular programmer.
It's like, there's a function.

335
00:18:42.500 --> 00:18:45.380
And there's a giant brain
that I don't understand. And

336
00:18:45.380 --> 00:18:47.420
I tell it stuff and
a true or false comes

337
00:18:47.420 --> 00:18:51.530
out. Right. It's very mysterious.
It's very black box. This

338
00:18:51.530 --> 00:18:55.070
is a cat. Okay. Computer
all-knowing computer. Thank you for

339
00:18:55.070 --> 00:18:56.810
the information. That is a
cat. It's totally not a

340
00:18:56.810 --> 00:19:00.290
cat, right? Yeah. So then
you're saying explainable, AI would

341
00:19:00.290 --> 00:19:02.030
be like, it's a cat,
it's got furry, it's got

342
00:19:02.030 --> 00:19:04.400
whiskers, it's got claws. And
it's got those point of

343
00:19:04.400 --> 00:19:07.970
years that, that's why I
think it's a cat. And

344
00:19:07.970 --> 00:19:09.830
then we could say, well,
you were close, but not

345
00:19:09.830 --> 00:19:12.320
really, which really means it's
like, we're teaching little children.

346
00:19:12.410 --> 00:19:15.170
Exactly. And one of the
ways that it can be

347
00:19:15.170 --> 00:19:19.580
used today is actually in
social media because we interact

348
00:19:19.580 --> 00:19:22.970
with algorithms all the time
on like Facebook I'm on

349
00:19:22.970 --> 00:19:27.140
Instagram, I'm on Google. And
what they could do here

350
00:19:27.140 --> 00:19:30.650
in transparency is actually show
users why they see the

351
00:19:30.650 --> 00:19:33.680
search results they see. So
for example, on Facebook, we

352
00:19:33.680 --> 00:19:36.680
tend to think what's on
the internet is true. Right.

353
00:19:37.070 --> 00:19:40.460
But as in the case
of all the reasons, right,

354
00:19:40.670 --> 00:19:42.980
as in the case of
all the recent elections and

355
00:19:42.980 --> 00:19:44.900
the stuff that we're going
around there, what if we

356
00:19:44.900 --> 00:19:46.730
knew why it popped up
on our feed? What if

357
00:19:46.730 --> 00:19:49.400
we knew if it was
something, it was an ad.

358
00:19:49.400 --> 00:19:51.950
If it was paid by
a certain organization to show

359
00:19:51.950 --> 00:19:54.380
up in our need, what
if we knew that it

360
00:19:54.380 --> 00:19:57.650
was because we're friends with
this particular person and this

361
00:19:57.650 --> 00:20:01.190
particular person likes this. If
we could control those lovers

362
00:20:01.190 --> 00:20:03.950
a little bit and say,
well, instead of just offering

363
00:20:03.950 --> 00:20:06.800
me, like, for example, on
Google, instead of just offering

364
00:20:06.830 --> 00:20:11.060
things to me, things that
reiterate my own worldview, go

365
00:20:11.060 --> 00:20:12.920
ahead and offer me things
that challenged my worldview as

366
00:20:12.920 --> 00:20:15.500
well, so that I can
see that because we know

367
00:20:15.500 --> 00:20:18.950
that as we click more
on social media, on Facebook,

368
00:20:18.980 --> 00:20:22.160
on Google, it's trained to
give us more of what

369
00:20:22.160 --> 00:20:25.580
we want to see, but
we can maybe change it

370
00:20:25.580 --> 00:20:27.140
and say like, give me
more of what I don't

371
00:20:27.140 --> 00:20:29.240
want to see. Can we
something else, give me something

372
00:20:29.240 --> 00:20:32.720
based on my geographical location
versus not my geographic location,

373
00:20:33.050 --> 00:20:36.890
things like that. Right. That's
really interesting. So I know

374
00:20:36.890 --> 00:20:39.650
as I travel and I
know as I go on

375
00:20:39.650 --> 00:20:42.320
Amazon and I look at
stuff, and then I find

376
00:20:42.710 --> 00:20:45.740
Facebook trying to sell me
that stuff, that stuff I

377
00:20:45.740 --> 00:20:50.200
can, as a non technical
user, determine that, okay, it

378
00:20:50.200 --> 00:20:52.690
must be showing me nails
because I was looking at

379
00:20:52.690 --> 00:20:54.910
a nail gun on Amazon.
Like I can figure that

380
00:20:54.910 --> 00:20:58.780
causality there, but like lately
I've been interested in climate

381
00:20:58.780 --> 00:21:01.540
change. And I believe that,
you know, we, we humans

382
00:21:01.540 --> 00:21:03.610
have caused climate change, did
it, you know, we've done

383
00:21:03.610 --> 00:21:06.220
it. So then I noticed
that all of the things

384
00:21:06.220 --> 00:21:09.010
in Facebook are agreeing with
me. Like you said, I'd

385
00:21:09.010 --> 00:21:12.700
be interested in a challenging
article. That's that, that, that

386
00:21:12.700 --> 00:21:15.880
said here is the science
against climate change, but not

387
00:21:16.120 --> 00:21:19.180
like a big fake news
article about someone's rant and

388
00:21:19.180 --> 00:21:22.000
opinion. I would like a
legitimate action. There must be

389
00:21:22.000 --> 00:21:24.610
a scientist who doesn't think
it's the case. Sure. If

390
00:21:24.610 --> 00:21:26.410
they could offer that and
tell me, this is a

391
00:21:26.410 --> 00:21:29.710
challenging point to your view
that, you know, why are

392
00:21:29.710 --> 00:21:32.830
we not seeing transparency in
AI? That's you're describing what

393
00:21:32.830 --> 00:21:35.280
exactly what I want. Exactly.
I think it would be

394
00:21:35.280 --> 00:21:38.490
really fun. It would, I
think it would get people

395
00:21:38.490 --> 00:21:40.740
engaged with the site even
more, because then they're also

396
00:21:40.740 --> 00:21:44.940
fine tuning their, their choices
and, and looking at it.

397
00:21:44.970 --> 00:21:48.510
And this idea, I got
this idea from Eli Pariser

398
00:21:48.750 --> 00:21:51.270
who gave a Ted talk,
a brilliant Ted talk on

399
00:21:51.270 --> 00:21:53.670
filter bubbles. And so he
talks about this idea more,

400
00:21:53.670 --> 00:21:56.160
and I really suggest anyone
interested in this idea more

401
00:21:56.160 --> 00:22:00.120
to go check out his
Ted talk. I will add

402
00:22:00.120 --> 00:22:03.600
a link to Eli Pariser.
He is the chief executive

403
00:22:03.660 --> 00:22:07.020
at Upworthy, which is a
place that has used AI.

404
00:22:07.020 --> 00:22:10.290
So smartly that they create
a huge amount of viral

405
00:22:10.290 --> 00:22:15.810
content. Right. Interesting. Yeah. I
feel like is you think

406
00:22:15.810 --> 00:22:17.880
we're moving to that world?
Are we going to get

407
00:22:17.970 --> 00:22:22.860
this level of, of transparency
or, or is there a

408
00:22:22.860 --> 00:22:25.570
reason for them to not
give us transparency and in

409
00:22:25.590 --> 00:22:29.220
AI, I think it depends
on how viral Eli's talk

410
00:22:29.220 --> 00:22:32.310
goes. Right? I think these
are ideas that live in

411
00:22:32.310 --> 00:22:35.550
these bodies and lives in
the people's minds. And if

412
00:22:35.550 --> 00:22:39.540
they're not in the, if
they're not where they can

413
00:22:39.540 --> 00:22:42.360
influence these decisions at these
bigger tech companies, and again,

414
00:22:42.360 --> 00:22:46.020
we're right back to technology
being used to influence the

415
00:22:46.020 --> 00:22:48.960
future world, to be able
to draw that line in

416
00:22:48.960 --> 00:22:51.720
the sign to signal. If
we don't have people right

417
00:22:51.720 --> 00:22:55.110
now in our technology companies
signaling, this is the side

418
00:22:55.440 --> 00:22:57.600
of the line that they
want to be on. It's

419
00:22:57.600 --> 00:22:59.760
going to be really hard
to see that happen because

420
00:23:00.060 --> 00:23:02.760
you don't have these ideas
up there. So it's, that's

421
00:23:02.760 --> 00:23:06.420
why it's really important to
not only have, you know,

422
00:23:06.690 --> 00:23:10.320
your developers be diverse, your
executives be diverse, but also

423
00:23:10.320 --> 00:23:13.170
your users to be diverse.
And to give that feedback,

424
00:23:13.350 --> 00:23:17.820
if we could somehow make
this pipeline and this feedback

425
00:23:17.820 --> 00:23:23.160
loop really short and more,
more effective, then we could

426
00:23:23.160 --> 00:23:26.250
see that happen. But it
really depends on where these

427
00:23:26.460 --> 00:23:30.090
ideas exist and how much
influence the people who have

428
00:23:30.090 --> 00:23:33.810
these ideas actually have to
change the world around them.

429
00:23:34.680 --> 00:23:37.830
I saw a video on
Twitter where it was a

430
00:23:37.830 --> 00:23:41.010
Nigerian guy who was in
a bathroom trying to dry

431
00:23:41.010 --> 00:23:44.280
his hands. And I was
watching it and the dryer

432
00:23:44.280 --> 00:23:47.270
wasn't turning. And I was
like, man, what is this?

433
00:23:47.270 --> 00:23:49.820
I'm kind of prank. And
then out of the side

434
00:23:49.820 --> 00:23:53.060
of the frame, a white
guy's hands came in and

435
00:23:53.120 --> 00:23:55.790
it totally worked. And then
the Nigerian guy tried to

436
00:23:55.850 --> 00:23:58.550
do it again. It didn't
work. And, you know, as

437
00:23:58.550 --> 00:24:00.500
a white guy with white
hands, I, it took me,

438
00:24:00.770 --> 00:24:02.240
it took the white guy
doing it for me to

439
00:24:02.240 --> 00:24:06.410
go, Whoa, that's a problem
because I can't personally, I'm

440
00:24:06.410 --> 00:24:08.300
sure that's the fact that
none of us can get

441
00:24:08.300 --> 00:24:10.790
the water thing to work.
I've never been able to

442
00:24:10.790 --> 00:24:12.470
get the bat. You know,
we just know those don't

443
00:24:12.470 --> 00:24:15.140
work for anyone, but from
the dryers perspective, I've never

444
00:24:15.140 --> 00:24:18.380
had any problem at all
with automatic dryers. Is that,

445
00:24:18.440 --> 00:24:20.390
is that a trained model?
Is that something that like,

446
00:24:20.450 --> 00:24:24.190
was it lack of user
testing? I think it depends.

447
00:24:24.190 --> 00:24:27.790
Right? So light reflects maybe
off of your skin different

448
00:24:27.790 --> 00:24:31.330
than is as a black
woman. And, but there's definitely

449
00:24:31.330 --> 00:24:35.560
cases where different, you know,
the way you develop and

450
00:24:35.560 --> 00:24:37.900
the science that's behind it,
right? The physics and the

451
00:24:37.900 --> 00:24:42.250
lights interaction is different. And
so maybe they didn't have

452
00:24:42.250 --> 00:24:45.760
a diverse, like physical user
base to test to see,

453
00:24:45.760 --> 00:24:49.720
like in poor lighting, will
this work as well. A

454
00:24:49.720 --> 00:24:53.710
really cool researcher named joy,
Bo Amina. She was at

455
00:24:53.710 --> 00:24:56.980
the MIT media lab. She
had research where she was

456
00:24:56.980 --> 00:25:00.430
trying to work with a
social robot that uses a

457
00:25:00.440 --> 00:25:06.130
face recognition camera. And she
was, she's also a black

458
00:25:06.160 --> 00:25:10.870
female in research and she
wasn't able to be able

459
00:25:10.870 --> 00:25:15.040
to be detected by her
own experiment, that it wouldn't

460
00:25:15.040 --> 00:25:17.590
recognize her face. She actually
had to wear a white

461
00:25:17.590 --> 00:25:20.650
mask in order to complete
her research. And she actually

462
00:25:20.650 --> 00:25:23.620
shows a video. Her website
is called the algorithmic justice

463
00:25:23.620 --> 00:25:27.490
league, very cool website. And
she shows a video of

464
00:25:27.490 --> 00:25:30.040
the example of her. She
saw it in the camera,

465
00:25:30.100 --> 00:25:33.370
in the camera's view without
the mask, and then put

466
00:25:33.370 --> 00:25:36.460
the mask on and then
it detected her. Or one

467
00:25:36.460 --> 00:25:39.280
of her colleagues would sit
in the camera, her, her

468
00:25:39.280 --> 00:25:41.680
white colleagues was suing the
camera. It would detect them.

469
00:25:41.860 --> 00:25:43.720
Then she'd go into the
camera view and then wouldn't

470
00:25:43.720 --> 00:25:48.670
detect her. So very these,
these kind of examples pop

471
00:25:48.670 --> 00:25:51.070
up every once in a
while where something just doesn't

472
00:25:51.070 --> 00:25:55.120
connect. And it creates a
different experience for people who

473
00:25:55.120 --> 00:26:00.010
have different skin tones. Yeah.
When, when the Microsoft connect

474
00:26:00.010 --> 00:26:02.410
first came out many, many
years ago, my kids weren't

475
00:26:02.410 --> 00:26:04.930
going to be two or
three, two or three. None

476
00:26:04.930 --> 00:26:07.630
of my family or my
Brown kids could be seen

477
00:26:07.630 --> 00:26:10.660
by the connect. But then
a call went out for

478
00:26:10.660 --> 00:26:13.300
a more diverse user base.
They retrain the algorithm. It

479
00:26:13.300 --> 00:26:16.540
wasn't a hardware issue. It
was a algorithmic issue. And

480
00:26:16.540 --> 00:26:18.670
now the connect works just
fine. And we have things

481
00:26:18.670 --> 00:26:22.060
like the iPhone X, which
has facial recognition, which I'm

482
00:26:22.060 --> 00:26:26.470
sure I'm sure that in
today's in today's world, they

483
00:26:26.470 --> 00:26:28.210
probably did a lot of
testing on all kinds of

484
00:26:28.210 --> 00:26:32.200
people. Very interesting problem. Yeah.
And I'm glad to see

485
00:26:32.200 --> 00:26:35.620
that they seem to have
looked at that looked at

486
00:26:35.620 --> 00:26:41.890
that case case scenario. And
another example is when there

487
00:26:41.890 --> 00:26:45.570
was an example in Google
photos, it was one of

488
00:26:45.570 --> 00:26:48.660
their apps and it was
built, you know, like Facebook

489
00:26:48.660 --> 00:26:52.980
and other Google apps that
detects faces and it accidentally

490
00:26:53.820 --> 00:26:58.110
detected this one person's face.
It was an African American

491
00:26:58.110 --> 00:27:00.240
male with an African American
female and it detected them

492
00:27:00.240 --> 00:27:02.580
as gorilla. So instead of
gripping them as people, I

493
00:27:02.580 --> 00:27:05.880
group them with the header
and the tag gorillas, and

494
00:27:05.880 --> 00:27:09.570
that was, that's like a
really big full pie. And

495
00:27:09.570 --> 00:27:12.030
one of the ways I
kind of came to understand

496
00:27:12.030 --> 00:27:16.020
that problem was it couldn't
understand that even though they

497
00:27:16.020 --> 00:27:20.340
had darker facial features that
it wasn't to go into

498
00:27:20.340 --> 00:27:25.200
this category. And that again
goes back to having much

499
00:27:25.230 --> 00:27:27.900
broader training sets. Even if
you have a couple of

500
00:27:27.900 --> 00:27:30.420
people of color in your
training set, that might not

501
00:27:30.420 --> 00:27:33.630
be enough, you really do
need to take a better

502
00:27:33.630 --> 00:27:38.850
look at expanding your data
sets. And then, so when

503
00:27:38.850 --> 00:27:42.000
I usually say that I
get questions like, well, that

504
00:27:42.010 --> 00:27:45.720
still seems like, you know,
that might be too much.

505
00:27:45.720 --> 00:27:48.780
Like you're having to really
reach for that. And again,

506
00:27:48.780 --> 00:27:51.660
it's about signaling and saying
like, Hey, this is actually

507
00:27:51.660 --> 00:27:53.010
what we want to do.
We want to provide a

508
00:27:53.010 --> 00:27:55.110
great experience for everyone and
we don't want this to

509
00:27:55.110 --> 00:27:59.250
happen. So instead of just
figuring our ways around this.

510
00:27:59.250 --> 00:28:03.480
So for example, the initial
fix for that issue that

511
00:28:03.480 --> 00:28:05.880
I just described was to
remove the guerrilla tag from

512
00:28:05.880 --> 00:28:09.270
all photo. So nothing could
ever be tagged photos, but

513
00:28:09.510 --> 00:28:12.450
instead of doing that, we
can actually be intentional in

514
00:28:12.450 --> 00:28:15.870
how we expand our data
sets and being able to

515
00:28:15.870 --> 00:28:19.190
provide that great experience for
them. Right. That might be

516
00:28:19.190 --> 00:28:23.330
the appropriate PR maneuver to
shut it down immediately. But

517
00:28:23.330 --> 00:28:26.250
you're right. The right answer
is a dataset and, and

518
00:28:26.360 --> 00:28:28.340
w with a sensitive issue
like that. And in fact,

519
00:28:28.940 --> 00:28:31.620
my friend is the gentleman
who had that happened. My,

520
00:28:31.620 --> 00:28:34.790
my buddy, Jackie. So I,
I know Jackie, I, I

521
00:28:34.790 --> 00:28:37.070
know about this situation and
this became a big deal

522
00:28:37.070 --> 00:28:39.950
in Google photos a couple
of years ago. And you

523
00:28:39.950 --> 00:28:42.170
have to be careful when
you're an engineer, don't you?

524
00:28:42.200 --> 00:28:45.380
Because you could say, well,
actually I can see because

525
00:28:45.380 --> 00:28:47.360
of the low contrast and
the color that, you know,

526
00:28:47.360 --> 00:28:49.280
I could see where you
might think that, I mean,

527
00:28:49.280 --> 00:28:52.370
they clearly are, look like
humans to anyone, but you

528
00:28:52.370 --> 00:28:55.460
can, you can see why
the computer thought that. So

529
00:28:55.460 --> 00:28:57.740
it would be nice to
have them that explainable artificial

530
00:28:57.740 --> 00:29:01.160
intelligence to tell you why
it thought that, but there

531
00:29:01.160 --> 00:29:03.890
are certain things that you
just can't do, their social

532
00:29:04.100 --> 00:29:07.100
mores. Like you said, it's
just a line you don't

533
00:29:07.100 --> 00:29:09.770
cross. And that's a good
example, which isn't Chrissy Tiegen

534
00:29:09.770 --> 00:29:11.870
see a weird thing recently
with AI. Did you hear

535
00:29:11.870 --> 00:29:14.690
about that? No. Can you
tell me about it? So

536
00:29:14.750 --> 00:29:18.650
Chrissy Tiegen is John Legend's
wife, right? And she is

537
00:29:18.650 --> 00:29:20.570
a kind of a geek
and she does all kinds

538
00:29:20.570 --> 00:29:23.660
of cool geeky stuff. And
she was goofing around in

539
00:29:23.660 --> 00:29:27.290
her Apple photos and discovered
that she could search for

540
00:29:27.470 --> 00:29:31.370
Brazil ear and found all
kinds of awkward photos of

541
00:29:31.370 --> 00:29:33.680
her, like getting dressed and
trying on outfits and stuff

542
00:29:33.680 --> 00:29:39.080
like that. But the word
bra had zero. So she

543
00:29:39.080 --> 00:29:43.090
started to think about, maybe
they've blocked the word bra

544
00:29:43.150 --> 00:29:46.180
and underwear, and they kind
of the obvious stuff, but

545
00:29:46.180 --> 00:29:49.480
they didn't Mark the full
word Brazil. And then people

546
00:29:49.480 --> 00:29:51.040
confirmed that and went out
and said, Oh my goodness,

547
00:29:51.040 --> 00:29:52.540
I can, if I search
for Brazil, there's all these

548
00:29:52.540 --> 00:29:55.120
pictures of me getting dressed
and trying outfits on. And

549
00:29:55.120 --> 00:29:57.310
like, I don't want those
kinds of photos to be,

550
00:29:59.770 --> 00:30:03.010
to be searchable. Why is
brought not available, but there

551
00:30:03.010 --> 00:30:06.310
is somebody didn't think about
that. Someone consciously removed one

552
00:30:06.310 --> 00:30:08.170
and not the other. And
we're going to find I'm

553
00:30:08.170 --> 00:30:10.540
sure dozens and dozens, if
not hundreds of those kinds

554
00:30:10.540 --> 00:30:14.830
of things, as we train
more and more systems. Yeah.

555
00:30:15.210 --> 00:30:17.850
I think that's totally true.
And that's really why it's

556
00:30:17.850 --> 00:30:21.690
important to get more users
into our data sets. But

557
00:30:21.690 --> 00:30:25.560
then also to start thinking
about the millions and millions

558
00:30:25.560 --> 00:30:27.930
of people that are not
online right now, I mean,

559
00:30:28.620 --> 00:30:32.430
there's a large portion of
our like planet, the world

560
00:30:32.430 --> 00:30:34.080
of human beings on the
world that are not on

561
00:30:34.080 --> 00:30:37.350
the internet right now from
whether or not they're facing

562
00:30:37.350 --> 00:30:41.040
a disaster, you know, like
Puerto Rico or they're just

563
00:30:41.040 --> 00:30:45.570
actually not online, like a
rural area in Africa, they

564
00:30:45.570 --> 00:30:49.740
have smart phones, but maybe
they don't have the full

565
00:30:49.740 --> 00:30:52.080
access to internet like we
do. And there are many

566
00:30:52.080 --> 00:30:54.330
populations in our world. If
you think about like the

567
00:30:54.330 --> 00:30:59.430
rainforest areas or anything like
that, that don't have access

568
00:30:59.430 --> 00:31:02.100
to the internet. And then
that means that our human

569
00:31:02.100 --> 00:31:07.770
experience really hasn't fully been
captured by the world and

570
00:31:07.770 --> 00:31:11.820
by the internet, by social
media. And so we probably

571
00:31:11.820 --> 00:31:16.020
don't truly understand how rich
our lives can be and

572
00:31:16.020 --> 00:31:18.510
how much more information that
we're missing. Things that we

573
00:31:18.510 --> 00:31:22.080
don't know. We don't know
that can be captured by

574
00:31:22.830 --> 00:31:26.070
our technology experience. And so
that's something to keep in

575
00:31:26.070 --> 00:31:28.530
mind as they start to
come online. As we start

576
00:31:28.530 --> 00:31:31.980
to see more of these
communities come online, we might

577
00:31:31.980 --> 00:31:34.800
have holes in our products.
I can't serve them. And

578
00:31:34.800 --> 00:31:38.010
they're the best people to
know, you know, how these

579
00:31:38.010 --> 00:31:41.370
products should be designed to
serve and to include their

580
00:31:41.430 --> 00:31:44.490
communities as well. That's really
great. Yeah. I hope that

581
00:31:44.490 --> 00:31:47.100
we make machine learning algorithms
that work, not just for

582
00:31:47.100 --> 00:31:49.860
the people who have connectivity,
but for the next billion

583
00:31:49.860 --> 00:31:52.080
that come after them, that
may not necessarily look like

584
00:31:52.080 --> 00:31:55.380
you and me. Exactly. Great.
Well, thank you so much

585
00:31:55.380 --> 00:31:58.170
for chatting with me today.
Thank you. It's been great.

586
00:31:58.170 --> 00:32:01.020
It's really fun. We've been
talking with Camille Eddy, and

587
00:32:01.020 --> 00:32:04.230
this has been another episode
of Hansel minutes. We'll see

588
00:32:04.230 --> 00:32:05.220
you again next week.

