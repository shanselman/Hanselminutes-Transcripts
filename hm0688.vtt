WEBVTT FILE

1
00:00:00.480 --> 00:00:05.400
This episode is sponsored by
Datadog with 350 plus integrations,

2
00:00:05.400 --> 00:00:09.810
as well as metrics logs,
traces, and APM Datadog enables

3
00:00:09.810 --> 00:00:12.390
your team to quickly find
and fix issues before they

4
00:00:12.390 --> 00:00:17.340
escalate. Visualize your microservices architecture
with a service map, spot

5
00:00:17.370 --> 00:00:20.640
unusual trends with the log
patterns, view and monitor the

6
00:00:20.640 --> 00:00:25.050
availability of your services with
synthetics. Start a free 14

7
00:00:25.050 --> 00:00:28.620
day trial of Datadog today
and receive a complimentary t-shirt

8
00:00:28.650 --> 00:00:53.570
by visiting bitly.com/data dog shirt.
That's bitly.com/datadog shirt, Hansel minutes.

9
00:00:53.570 --> 00:00:56.420
And today I'm talking with
Lauren McPhail and wants to

10
00:00:56.420 --> 00:00:59.930
get app. Hi, I'm doing
well. Thanks. How are you?

11
00:01:00.530 --> 00:01:02.090
I'm doing pretty well and
I'm happy to be chatting

12
00:01:02.090 --> 00:01:05.300
with you. So get app
is a gardener Company. We

13
00:01:05.300 --> 00:01:07.730
are. So we like to
say that get up is

14
00:01:07.730 --> 00:01:10.910
TripAdvisor for B to B
software. The idea is that

15
00:01:10.910 --> 00:01:13.820
if you are a smaller
midsize business owner and you

16
00:01:13.820 --> 00:01:17.720
need different software to run
your business, whether it's HR

17
00:01:17.870 --> 00:01:21.290
or marketing or business intelligence
software, you would go to

18
00:01:21.290 --> 00:01:26.240
get app.com. You would read
LinkedIn verified reviews of thousands

19
00:01:26.240 --> 00:01:29.720
of different cloud SAS products
you would filter by the

20
00:01:29.720 --> 00:01:32.810
features you need. And eventually
you would choose the tool

21
00:01:32.960 --> 00:01:35.210
that is the best fit
for your business in the

22
00:01:35.210 --> 00:01:40.340
context of your app stack,
Because I speak from ignorance.

23
00:01:40.340 --> 00:01:42.440
So forgive me if these
are dumb questions, but I

24
00:01:42.440 --> 00:01:45.230
read a lot of like
Gartner reports and like analyst

25
00:01:45.230 --> 00:01:48.020
reports. And I wonder, what
is the skill set of

26
00:01:48.020 --> 00:01:50.480
an analyst? I mean, you
have, you have to know

27
00:01:50.480 --> 00:01:53.360
business, you have to know
technology. How do you keep

28
00:01:53.540 --> 00:01:56.510
track of it all? And
what is your background? Yeah,

29
00:01:56.510 --> 00:01:59.630
it's a great question. I
was a media studies major

30
00:01:59.630 --> 00:02:02.420
in college, so I don't
have a computer science degree.

31
00:02:02.420 --> 00:02:05.840
And I actually think that
my media studies education was

32
00:02:05.840 --> 00:02:09.590
better preparation for my career
as a Gartner analyst, because

33
00:02:09.860 --> 00:02:14.570
a lot of it is
doing large scale evaluation of

34
00:02:14.570 --> 00:02:20.720
different technologies and really separating
hype from what's realistic for

35
00:02:20.750 --> 00:02:23.990
business owners to be doing
now versus what they should

36
00:02:23.990 --> 00:02:26.930
be doing even in five
years time, because you need

37
00:02:26.930 --> 00:02:29.330
to advise them on when
to make the right investment

38
00:02:29.330 --> 00:02:34.460
in those technologies. And oftentimes
for example, we as analysts

39
00:02:34.460 --> 00:02:37.340
for small and midsize business
owners will tell them this

40
00:02:37.340 --> 00:02:39.530
isn't the right time for
your small business to be

41
00:02:39.530 --> 00:02:42.920
investing in blockchain. It's not
going to reach maturity for

42
00:02:42.920 --> 00:02:46.040
five to 10 years. And
so you're better off investing

43
00:02:46.040 --> 00:02:50.570
in this until then, or
we'll say to them, AI

44
00:02:50.570 --> 00:02:54.890
is in this all encompassing
technology. It's actually integrated into

45
00:02:54.890 --> 00:02:57.920
a lot of very popular
cloud SAS products that you

46
00:02:57.920 --> 00:03:01.420
probably use every day, whether
it's Google drive or Slack.

47
00:03:01.660 --> 00:03:04.180
And so this is how
you can start thinking about

48
00:03:04.180 --> 00:03:07.720
using AI in a more
productive way. So a lot

49
00:03:07.720 --> 00:03:11.200
of it is educating the
masses and really speaking to

50
00:03:11.200 --> 00:03:14.350
your audience in terms of
their level of technical expertise.

51
00:03:14.590 --> 00:03:19.000
Oftentimes our readers and software
buyers are people who are

52
00:03:19.180 --> 00:03:22.060
pretty technically savvy, but they
just want to know more

53
00:03:22.060 --> 00:03:25.990
about particular products. And that's
a separate issue audience from

54
00:03:25.990 --> 00:03:29.200
someone else who needs the
tools, but really is starting

55
00:03:29.200 --> 00:03:31.990
from scratch in terms of
their knowledge of the SAS

56
00:03:31.990 --> 00:03:35.760
landscape. So you are truly
an analyst and you are

57
00:03:35.760 --> 00:03:40.140
analyzing your ads. It's a
multi I'm trying to think

58
00:03:40.170 --> 00:03:43.710
what the right word for
it. It's multiple disciplines to

59
00:03:43.710 --> 00:03:45.690
be able to do, analyze
these things and to provide

60
00:03:45.690 --> 00:03:48.420
a bridge between, like you
said, the regular person and

61
00:03:48.420 --> 00:03:51.930
the technical person. That's true.
And I think the distinction

62
00:03:51.960 --> 00:03:54.930
as well is that you
have to be up on

63
00:03:54.930 --> 00:03:59.520
your game in terms of
technical expertise and understanding your

64
00:03:59.580 --> 00:04:02.670
audience, but you also have
to be able to distill

65
00:04:02.820 --> 00:04:06.990
technical concepts in a pretty
easy to digest way. I

66
00:04:06.990 --> 00:04:09.570
was at a conference out
in San Jose last year

67
00:04:09.570 --> 00:04:12.600
and overheard an attendee, say,
I know a lot about

68
00:04:12.630 --> 00:04:15.330
blockchain and what it is.
I still really don't know

69
00:04:15.330 --> 00:04:17.730
a lot about what it
actually does. So if I

70
00:04:17.730 --> 00:04:20.880
had to distill my role
as an analyst into one

71
00:04:21.390 --> 00:04:24.120
sentence, it would really be
to explain what these technologies

72
00:04:24.120 --> 00:04:28.530
do on the business side.
And you do have a

73
00:04:28.530 --> 00:04:31.680
master's degree, but you've also
done few followed up with

74
00:04:31.680 --> 00:04:34.830
additional certificates. And anytime you
can take classes, I guess

75
00:04:34.830 --> 00:04:37.350
you recently took a class
at MIT at the Sloan

76
00:04:37.350 --> 00:04:39.740
school to kind of start
getting up to speed on,

77
00:04:39.740 --> 00:04:42.510
on AI. This is an
area of interest. It is

78
00:04:42.510 --> 00:04:45.510
I, I am most interested
in AI and I'm happy

79
00:04:45.510 --> 00:04:48.780
to specialize in it. Last
year. I noticed that Sloan

80
00:04:48.990 --> 00:04:53.400
was running courses on AI
for business strategy. And that's

81
00:04:53.400 --> 00:04:57.600
actually a course that I
haven't really seen replicated in

82
00:04:57.600 --> 00:05:01.380
many other places. Even if
you look at MBA programs

83
00:05:01.380 --> 00:05:06.090
or other similar higher education
programs, you often find I

84
00:05:06.180 --> 00:05:10.140
AI it, you know, lumped
within a larger computer science

85
00:05:10.140 --> 00:05:12.870
education. And it's not that
that's a bad thing, but

86
00:05:12.870 --> 00:05:16.710
in terms of the very
particular application of AI in

87
00:05:16.710 --> 00:05:20.520
business contexts, there isn't a
lot out there besides the

88
00:05:20.520 --> 00:05:23.610
MIT course that Sloan offered.
And I found it so

89
00:05:23.610 --> 00:05:26.040
relevant to my work as
an analyst that I decided

90
00:05:26.040 --> 00:05:28.560
to take it. And it
really has paid itself off

91
00:05:28.590 --> 00:05:31.350
in spades in terms of
what I learned and my

92
00:05:31.350 --> 00:05:34.320
ability to apply it to
my work. As an analyst.

93
00:05:34.320 --> 00:05:37.170
I also went on to
coauthor, a research note for

94
00:05:37.170 --> 00:05:40.260
Gartner enterprise clients on the
use of what we call

95
00:05:40.260 --> 00:05:45.480
everyday AI in social software
applications like Slack and Google

96
00:05:45.480 --> 00:05:49.260
drive, and everyday AI, as
something we define as AI

97
00:05:49.260 --> 00:05:52.170
that is so subtle. You
don't even realize that it's

98
00:05:52.170 --> 00:05:55.080
in the system and the
goal there was really to

99
00:05:55.080 --> 00:05:59.150
emphasize to the readers that
you don't have to be

100
00:05:59.150 --> 00:06:01.760
scared of AI. It's already
a part of tools that

101
00:06:01.760 --> 00:06:04.550
you're very familiar with and
understanding the benefits that it

102
00:06:04.550 --> 00:06:07.730
brings to your work will
help you make more informed

103
00:06:07.730 --> 00:06:12.580
choices about integrating AI further
into your business. And one

104
00:06:12.580 --> 00:06:14.590
of the things that you
were concerned about, and as

105
00:06:14.590 --> 00:06:16.840
you have been exploring the
area of AI is this

106
00:06:16.840 --> 00:06:18.970
idea of bias in AI.
And I've done a couple

107
00:06:18.970 --> 00:06:22.750
of, of shows about that.
How does bias get baked

108
00:06:22.750 --> 00:06:26.590
into AI if people are,
if people mean well, like

109
00:06:26.590 --> 00:06:29.740
I don't want to bake
bias into my software, I

110
00:06:29.740 --> 00:06:32.770
can afford it. That's a
great question, because I think

111
00:06:32.770 --> 00:06:37.030
when we talk about bias
in AI, the end result

112
00:06:37.030 --> 00:06:40.720
whether intentional or not is
that it comes off as

113
00:06:41.380 --> 00:06:46.330
purposeful. And the reality is
that it's more often not

114
00:06:46.330 --> 00:06:49.420
the case. It's more often
the case that bias, creeps

115
00:06:49.420 --> 00:06:53.920
into the data sets used
to train machine learning algorithms

116
00:06:54.040 --> 00:06:58.630
as indirect bias. And that
is when you have byproducts

117
00:06:58.690 --> 00:07:03.370
of sensitive attributes that correlate
with nonsensitive attributes. So when

118
00:07:03.370 --> 00:07:06.160
we're talking about a sensitive
attribute in the context of

119
00:07:06.160 --> 00:07:09.430
bias, it's something that is
direct. So let's say you're

120
00:07:09.430 --> 00:07:13.120
building a machine learning algorithm
that is going to give

121
00:07:13.120 --> 00:07:18.850
home home loans to potential
housing buyers. You an example

122
00:07:18.850 --> 00:07:22.000
of a sensitive attribute would
be race. And so an

123
00:07:22.000 --> 00:07:26.110
example of direct bias would
be that you are specifically

124
00:07:26.110 --> 00:07:30.580
excluding black people from the
dataset or you're exclusive, or

125
00:07:30.580 --> 00:07:34.780
you're intentionally giving them lower
loans. That's an example of

126
00:07:34.780 --> 00:07:39.970
direct bias. Indirect bias again
is an unintended correlation of

127
00:07:39.970 --> 00:07:44.650
sensitive and nonsensitive attributes. And
so one example is that

128
00:07:44.890 --> 00:07:49.150
you might have with, for
example, red lining, which is

129
00:07:49.150 --> 00:07:52.570
a practice that was common
in Portland, Oregon for over

130
00:07:52.570 --> 00:07:57.370
a hundred years, red lining
basically locked black people of

131
00:07:57.370 --> 00:08:01.060
color out from buying homes
in certain areas of Portland.

132
00:08:01.330 --> 00:08:06.040
And even though that was
declared illegal in 1990, we

133
00:08:06.040 --> 00:08:10.090
still have over a century's
worth of data that was

134
00:08:10.720 --> 00:08:14.410
predicated on practice. That's now
illegal. And so if you

135
00:08:14.410 --> 00:08:17.950
were to use that data
in a dataset to train

136
00:08:17.950 --> 00:08:23.110
an ML algorithm, you might
be indirectly reinforcing bias, even

137
00:08:23.110 --> 00:08:26.370
though that's not your intent.
And I think it's, it's

138
00:08:26.370 --> 00:08:29.380
interesting to point that out
that it can happen, even

139
00:08:29.380 --> 00:08:31.270
though it is not your
intent, but it's a very

140
00:08:31.270 --> 00:08:35.620
sensitive topic. So if someone,
then let's just say criticizes

141
00:08:35.620 --> 00:08:37.900
and say, Hey, look, your,
your area. It has bias

142
00:08:37.900 --> 00:08:39.970
and you go, Hey, I'm
not racist. I didn't mean

143
00:08:39.970 --> 00:08:43.000
to do that. It's it's,
it's it creeps in there

144
00:08:43.000 --> 00:08:45.790
whether you are explicitly doing
it or not. And it's

145
00:08:45.850 --> 00:08:49.750
the awareness of that. That's
important. Definitely. So, and that's

146
00:08:49.750 --> 00:08:52.600
the thing that I always
try to emphasize upfront when

147
00:08:52.600 --> 00:08:56.940
I give talks on machine
in ML algorithms. I first

148
00:08:56.940 --> 00:09:00.870
start by saying, we all
have bias as humans, whether

149
00:09:00.960 --> 00:09:03.810
we acknowledge it or not.
I say to them, I

150
00:09:03.810 --> 00:09:06.660
know less biased than you
are just because I'm up

151
00:09:06.660 --> 00:09:11.370
here giving this talk. And
we, and again, it, the

152
00:09:11.370 --> 00:09:13.500
data sets that are used
to train these ML algorithms

153
00:09:13.500 --> 00:09:17.370
are so complex that there
really is no way to

154
00:09:17.370 --> 00:09:20.940
take bias out of the
algorithm, but there are specific

155
00:09:20.940 --> 00:09:22.980
things that you can do
from the outset of the

156
00:09:22.980 --> 00:09:27.180
product's life cycle that will
mitigate it throughout and hopefully

157
00:09:27.360 --> 00:09:30.360
keep you from retraining your
model from scratch. That's something

158
00:09:30.360 --> 00:09:34.650
else I emphasize is the
concept that ethical debt in

159
00:09:34.650 --> 00:09:38.430
this case bias is actually
technical debt. Because if you

160
00:09:38.430 --> 00:09:42.780
find out down the line
that your algorithm is unacceptably

161
00:09:42.780 --> 00:09:45.690
biased, your only recourse is
going to be to scrap

162
00:09:45.690 --> 00:09:48.570
the model and completely retrain
it. And that creates an

163
00:09:48.570 --> 00:09:51.420
enormous amount of work for
your team when you're already

164
00:09:51.420 --> 00:09:54.480
working on a very complicated
problem. So if you work

165
00:09:54.480 --> 00:09:56.820
to mitigate bias from the
start, when you're writing your

166
00:09:56.820 --> 00:09:59.340
texts spec, you can eliminate
a lot of those issues

167
00:09:59.340 --> 00:10:03.440
moving. How do we catch
all the things though? There's

168
00:10:03.440 --> 00:10:06.050
always going to be an
exception, right? Like you and

169
00:10:06.050 --> 00:10:09.410
your, you wrote an article
for opensource.com where you talked

170
00:10:09.410 --> 00:10:11.720
about how, you know, not
everyone thinks about how their

171
00:10:11.720 --> 00:10:14.300
models are going to be
used. And if you don't

172
00:10:14.480 --> 00:10:18.710
consider all aspects of how
your, your model will be

173
00:10:18.710 --> 00:10:20.600
used, you're going to find
yourself in a situation where

174
00:10:20.600 --> 00:10:22.760
someone could potentially be in
life or death. And one

175
00:10:22.760 --> 00:10:25.850
of the examples that you
gave was like a self

176
00:10:25.850 --> 00:10:30.710
driving car or something that
takes voice commands. And like,

177
00:10:30.710 --> 00:10:34.820
for example, my wife has
constantly frustrated with Alexa because

178
00:10:35.510 --> 00:10:38.000
she has an accent and
it doesn't seem to understand

179
00:10:38.030 --> 00:10:40.820
her like it does me.
So she'll actually ask me

180
00:10:40.820 --> 00:10:43.250
to tell Alexa stuff. If
we were trying to tell

181
00:10:43.250 --> 00:10:46.370
Alexa to stop doing something
dangerous or turn something off

182
00:10:46.370 --> 00:10:48.440
that needed to be turned
off immediately, we could have

183
00:10:48.440 --> 00:10:51.290
a real, a real problem.
I love that example that

184
00:10:51.290 --> 00:10:54.170
you gave because it's something
that I use in my

185
00:10:54.170 --> 00:10:57.890
talk on machine bias as
well. I specifically talk about

186
00:10:57.890 --> 00:11:01.670
a woman named dr. Carol Riley,
who is a very renowned

187
00:11:01.790 --> 00:11:06.410
AI expert. She's an entrepreneur.
She has a PhD in

188
00:11:06.410 --> 00:11:10.790
the subject. And when she
was building a voice command

189
00:11:10.790 --> 00:11:15.740
interface using Microsoft's speech recognition
API, she found that her

190
00:11:15.740 --> 00:11:20.840
own creation couldn't recognize her
voice, presumably because the API

191
00:11:20.840 --> 00:11:24.200
was not exposed to enough
voice variations in training. And

192
00:11:24.200 --> 00:11:29.090
so then it couldn't recognize
accents, inflections, or even people

193
00:11:29.090 --> 00:11:32.180
of different genders. And so
Carol wrote an op ed

194
00:11:32.180 --> 00:11:34.520
for tech crunch, where she
talked about how she tried

195
00:11:34.520 --> 00:11:39.140
lowering her voice and her
own creation. Couldn't still recognize

196
00:11:39.140 --> 00:11:41.210
her voice. And the end
result was she actually had

197
00:11:41.210 --> 00:11:44.930
to have a male graduate
student present her her own

198
00:11:44.930 --> 00:11:48.770
work because it couldn't recognize
her voice. And the example

199
00:11:48.770 --> 00:11:52.220
that she uses in her
op ed is to think

200
00:11:52.220 --> 00:11:56.050
about self driving and to
say, this is a mild,

201
00:11:56.170 --> 00:12:01.030
maybe somewhat humorous problem, but
think about the implications. If

202
00:12:01.030 --> 00:12:03.400
you are in a life
or death situation with an

203
00:12:03.400 --> 00:12:06.550
autonomous product, and you need
to communicate with it, but

204
00:12:06.550 --> 00:12:10.810
it doesn't recognize your voice
as human. The consequences could

205
00:12:10.810 --> 00:12:13.480
be very serious. And I
think because we are still

206
00:12:13.480 --> 00:12:17.290
in the early days of
AI is fourth wave. This

207
00:12:17.290 --> 00:12:20.650
is the time now to
be acknowledging the bias and

208
00:12:20.680 --> 00:12:24.370
designing products that are more
thoughtful. So it really isn't

209
00:12:24.370 --> 00:12:27.490
just about doing what's quote
unquote fair. We need to

210
00:12:27.490 --> 00:12:31.560
design with consumer safety in
mind. What would you say

211
00:12:31.590 --> 00:12:33.720
to someone who might be
listening to this podcast and

212
00:12:33.720 --> 00:12:37.860
listening to this conversation and
might be thinking to themselves,

213
00:12:38.100 --> 00:12:41.580
you know, this is a,
an overly progressive viewpoint. I

214
00:12:41.580 --> 00:12:44.580
don't really liberal viewpoint, or
you're making computers, political and

215
00:12:44.580 --> 00:12:47.250
computers. Aren't political. Is this
a political thing, or is

216
00:12:47.250 --> 00:12:50.190
this about software doing what
software is supposed to do?

217
00:12:51.510 --> 00:12:53.910
I, it, my opinion is
that it shouldn't be a

218
00:12:53.910 --> 00:12:57.840
political thing, but it is
theirs. I think I would

219
00:12:57.840 --> 00:13:00.510
say I would argue the
whole concept of diversity and

220
00:13:00.510 --> 00:13:04.530
inclusion is one that has
been politicized. It is often

221
00:13:04.530 --> 00:13:09.720
seen as being politically correct.
And I view it as

222
00:13:09.780 --> 00:13:14.520
a business strategy because the
management research is quite clear

223
00:13:14.520 --> 00:13:18.510
on the consensus that more
diverse teams not only produce

224
00:13:18.510 --> 00:13:23.160
more fi stronger financial returns,
but they also build products

225
00:13:23.160 --> 00:13:26.670
that serve a wider range
of users. So I would

226
00:13:26.670 --> 00:13:30.420
approach it from that perspective.
And, and similarly again, when

227
00:13:30.420 --> 00:13:34.080
you're talking about building data
sets that are less biased

228
00:13:34.080 --> 00:13:37.260
or built with bias in
mind, you're talking about not

229
00:13:37.260 --> 00:13:40.140
only serving a wider range
of users, but you're talking

230
00:13:40.140 --> 00:13:43.770
about mitigating the risk, the
risk of work down the

231
00:13:43.770 --> 00:13:46.860
road. So my argument is
that it's a, it's a

232
00:13:46.860 --> 00:13:51.240
technical strategy and a business
strategy in tandem, but I'm

233
00:13:51.600 --> 00:13:56.280
afraid that in this, especially
politically polarized climate, it has

234
00:13:56.280 --> 00:14:00.180
become an issue. I am
however, heartened by the fact

235
00:14:00.180 --> 00:14:03.300
that I give talks on
machine bias at pretty mainstream

236
00:14:03.300 --> 00:14:06.570
tech events, like Drupal con
North America and the opensource

237
00:14:06.570 --> 00:14:09.390
summit. And I've met a
wide range of people in

238
00:14:09.390 --> 00:14:12.540
the sector who are not
only aware of this problem,

239
00:14:12.540 --> 00:14:15.600
but do want to take
it to the next step

240
00:14:15.630 --> 00:14:19.080
and solve it. So that
gives me confidence that we

241
00:14:19.080 --> 00:14:23.310
will hopefully make some progress.
I appreciate that you're pointing

242
00:14:23.310 --> 00:14:25.740
out that it, it doesn't
have to be politicized in

243
00:14:25.740 --> 00:14:28.350
the sense of if you
want your data to be

244
00:14:28.350 --> 00:14:30.870
fair, and you want your
product to work everywhere. And

245
00:14:30.870 --> 00:14:34.020
you want to, presumably if
you're a capitalist make money,

246
00:14:34.020 --> 00:14:36.030
you'd like to sell it
to everybody. And if you're

247
00:14:36.040 --> 00:14:39.720
a product indirectly excluded someone,
then like we've, we've, we've

248
00:14:39.720 --> 00:14:43.500
all seen examples of automatic
soap dispensers that can't see

249
00:14:43.500 --> 00:14:47.250
different colors of skin that
would certainly block you out

250
00:14:47.250 --> 00:14:50.070
of a market. So you
should probably, you say, train

251
00:14:50.070 --> 00:14:53.810
your data under fairness constraints.
Maybe you could talk about

252
00:14:53.810 --> 00:14:58.090
fairness constraints. Sure. So that
really starts with going back

253
00:14:58.090 --> 00:15:02.260
to the tech spec for
any ML algorithm that you

254
00:15:02.260 --> 00:15:06.130
are going to train. And
so when you are trying

255
00:15:06.130 --> 00:15:09.100
to reduce ethical debt in
your product, you are going

256
00:15:09.100 --> 00:15:12.610
to have to answer two
questions in that product specification

257
00:15:12.610 --> 00:15:15.880
phase. You're going to have
to talk about which methods

258
00:15:15.880 --> 00:15:18.790
of fairness you will use
and how you're going to

259
00:15:18.790 --> 00:15:21.970
prioritize them. And the reason
this is important is because

260
00:15:21.970 --> 00:15:25.510
building a product based on
ML isn't, if you're doing

261
00:15:25.510 --> 00:15:28.750
this, it's not enough to
reactively fix bugs or pull

262
00:15:28.750 --> 00:15:32.200
products from the shelves. You'll
have to answer these questions

263
00:15:32.200 --> 00:15:34.450
in your tech specs, so
that you're thinking about them

264
00:15:34.450 --> 00:15:37.060
from the start of the
life cycle. And this is

265
00:15:37.060 --> 00:15:42.340
also the time to exclude
sensitive attributes, like race, religion

266
00:15:42.340 --> 00:15:44.830
from the data set, you're
going to use to train

267
00:15:44.830 --> 00:15:47.710
the algorithm. If you're going
to include them, you need

268
00:15:47.710 --> 00:15:51.910
to make a specific documented
case for why that's appropriate,

269
00:15:52.090 --> 00:15:54.970
not just for your users,
but also because we are

270
00:15:55.120 --> 00:15:59.740
slowly but surely starting to
see more regulation on the

271
00:15:59.740 --> 00:16:02.860
tech world, especially in Europe,
but we're seeing the early

272
00:16:02.860 --> 00:16:06.580
stages of it in the
States as well. And so

273
00:16:06.580 --> 00:16:09.700
you, I think we're going
to see more tech companies

274
00:16:09.700 --> 00:16:12.640
be more accountable for their
creations in ways that they

275
00:16:12.640 --> 00:16:17.350
haven't been previously. So documentation,
I know tech teams don't

276
00:16:17.350 --> 00:16:20.740
always love it, but it's
really essential before you get

277
00:16:20.740 --> 00:16:25.060
moving on the next steps
here, Are you looking for

278
00:16:25.060 --> 00:16:28.630
an alternative maps API provider
at Tom Tom? We have

279
00:16:28.630 --> 00:16:32.200
over 28 years experience in
perfecting our maps and technology.

280
00:16:32.800 --> 00:16:36.130
We believe in freedom of
choice, meaning no advertising, no

281
00:16:36.130 --> 00:16:38.890
subscription and no need for
a credit card on file.

282
00:16:39.490 --> 00:16:45.460
Visit developer dot Tom, tom.com
to get access to 2,500

283
00:16:45.460 --> 00:16:49.510
daily free API transactions and
unlimited access to our maps

284
00:16:49.510 --> 00:16:52.900
and traffic flow tiles within
our maps, SDKs for Android

285
00:16:52.900 --> 00:16:57.580
and iOS until September 30th, you
can get an extra 10,000

286
00:16:57.580 --> 00:17:04.390
credits by going to developer
dot Tom, tom.com/promo using the

287
00:17:04.390 --> 00:17:08.650
promo code Hanselman. It's exclusively
for Hanselman its listeners. That's

288
00:17:08.650 --> 00:17:14.800
developer dot Tom, tom.com/promo using
promo code Hansel minutes. Can

289
00:17:14.800 --> 00:17:17.860
we make this better by
allowing people to have custom

290
00:17:17.860 --> 00:17:19.990
models? So you can have
a baseline model and they

291
00:17:20.020 --> 00:17:24.520
custom the ability to basically
adjust something on a user

292
00:17:24.520 --> 00:17:27.610
by user basis, tagging your
own data, perhaps, I guess

293
00:17:28.330 --> 00:17:31.570
That's an interesting question. So
tag data is definitely a

294
00:17:31.570 --> 00:17:34.870
key step. I would say,
prior to that, or in

295
00:17:34.870 --> 00:17:37.180
tandem with it, you should
be training your data under

296
00:17:37.180 --> 00:17:40.600
fairness constraints, like you mentioned
earlier, and this stuff is

297
00:17:40.600 --> 00:17:43.810
important, but also difficult because
when you try to control

298
00:17:43.810 --> 00:17:47.380
or eliminate direct or indirect
bias, you're going to find

299
00:17:47.380 --> 00:17:51.540
yourself in a catch 22
related the algorithms accuracy. So

300
00:17:51.540 --> 00:17:55.530
if you've trained exclusively on
Nancy nonsensitive attributes, you will

301
00:17:55.530 --> 00:17:59.850
eliminate direct discrimination, but you'll
introduce or reinforced direct bias,

302
00:18:00.240 --> 00:18:04.590
but you can't train separate
classifiers for each sensitive feature

303
00:18:04.590 --> 00:18:08.940
because that will re-introduce direct
discrimination. So to reduce these

304
00:18:08.940 --> 00:18:11.910
risks, you don't want to
just measure the average strengths

305
00:18:11.910 --> 00:18:15.480
of acceptance and rejection across
sensitive groups. Instead, what you

306
00:18:15.480 --> 00:18:17.910
can do is use limits
to determine what is or

307
00:18:17.910 --> 00:18:20.850
isn't included in the model
you're training. And when you

308
00:18:20.850 --> 00:18:25.110
do this discrimination tests are
expressed as restrictions and limitations

309
00:18:25.110 --> 00:18:28.230
on the learning process, which
can ease the risk of

310
00:18:28.230 --> 00:18:33.440
bias creeping in It's this
concept of ethical debt. Is

311
00:18:33.440 --> 00:18:34.880
that something that you came
up with? Where did that

312
00:18:34.880 --> 00:18:37.220
name come from? Because it
really is an excellent way

313
00:18:37.220 --> 00:18:40.840
to express technical debt as
it relates to machine learning.

314
00:18:41.180 --> 00:18:43.730
I can't take full credit
for it. I first read

315
00:18:43.730 --> 00:18:47.600
that term on the new
stack, which is a great

316
00:18:47.600 --> 00:18:53.120
publication by Alex Williams, that talks
about different open source topics.

317
00:18:53.420 --> 00:18:56.870
And it's particularly in the
it and business intelligence space.

318
00:18:56.870 --> 00:18:59.810
And one of their, the
new stacks writers wrote a

319
00:18:59.810 --> 00:19:02.120
piece on ethical debt at
the start of the year.

320
00:19:02.330 --> 00:19:05.600
And I thought that concept
was a perfect way to

321
00:19:05.600 --> 00:19:09.440
explain what we're discussing. I
will give myself a little

322
00:19:09.440 --> 00:19:12.080
bit of credit for coming
up with the correlation between

323
00:19:12.290 --> 00:19:15.200
ethical debt and technical debt,
because at the end of

324
00:19:15.200 --> 00:19:18.500
the day, that's really what
it is. If the algorithm

325
00:19:18.500 --> 00:19:23.300
is found to be unacceptably
biased, you have to retrain

326
00:19:23.300 --> 00:19:26.210
it and start all over.
And that's a misnomer that

327
00:19:26.210 --> 00:19:28.130
I think a lot of
people have in the business

328
00:19:28.130 --> 00:19:31.520
world. They don't often think
about the technical requirements of

329
00:19:31.520 --> 00:19:35.570
technologies that are complex before
they adopt them. I spoke

330
00:19:35.630 --> 00:19:39.260
at Barnard college last year
and a consultant for IBM

331
00:19:39.470 --> 00:19:42.590
came up to me after
the talk, because she was

332
00:19:42.590 --> 00:19:45.680
in charge of selling Watson
to people. And she said,

333
00:19:45.680 --> 00:19:48.470
the number one questions she
got was, what do you

334
00:19:48.470 --> 00:19:52.340
mean I have to train
it. They just thought that

335
00:19:52.340 --> 00:19:56.840
they were getting this full
fledged product that was going

336
00:19:56.840 --> 00:20:01.400
to solve all of their
business challenges without understanding all

337
00:20:01.400 --> 00:20:03.500
of the enormous work that
they would have to do

338
00:20:03.500 --> 00:20:07.130
with Watson to customize it
for their business. And so

339
00:20:07.130 --> 00:20:10.190
if you don't have the
technical talent in house adopting

340
00:20:10.190 --> 00:20:13.010
a product like Watson, really,
isn't going to get you

341
00:20:13.010 --> 00:20:15.890
that far. And there's a
real, I think, lack of

342
00:20:15.890 --> 00:20:21.500
education around AI and products
in general. And to your

343
00:20:21.500 --> 00:20:23.540
point about what the role
of an analyst is, I

344
00:20:23.540 --> 00:20:26.270
think that's a big aspect
of our job as well.

345
00:20:27.410 --> 00:20:29.360
The thing that scares me
the most about just machine

346
00:20:29.360 --> 00:20:31.370
learning and AI in generally
is that it is a

347
00:20:31.370 --> 00:20:34.100
black box. It's almost, it's
a, it is a function

348
00:20:34.490 --> 00:20:37.610
that someone wrote that you
can't see inside. Like you

349
00:20:37.610 --> 00:20:40.910
can't see the source code
of the model. You can

350
00:20:40.910 --> 00:20:43.790
only see the training data
that created the model, which

351
00:20:43.790 --> 00:20:47.470
is itself, a magical black
box where something goes and

352
00:20:47.470 --> 00:20:50.110
something comes out, you know,
some text goes in and

353
00:20:50.110 --> 00:20:53.410
some sentiment analysis goes out
and it works in 2019,

354
00:20:53.770 --> 00:20:57.130
but then maybe some new
slang comes out in 2022.

355
00:20:57.490 --> 00:21:00.820
And now your sentiment analysis
thing kicks off the wrong

356
00:21:00.820 --> 00:21:03.850
results because the kids are
using a new, a new

357
00:21:03.850 --> 00:21:06.130
term and then you have
to go and retrain it.

358
00:21:06.220 --> 00:21:08.980
And if people can't see
inside it, they're just going

359
00:21:08.980 --> 00:21:12.040
to change the code around
it and not redo the

360
00:21:12.040 --> 00:21:14.830
training data. People are not
going to revise their models.

361
00:21:15.510 --> 00:21:19.200
Exactly. I, that's also a
great example because that really

362
00:21:19.200 --> 00:21:21.390
gets to the heart of
why machine bias is such

363
00:21:21.390 --> 00:21:24.840
a problem. It's it really
relates to black box algorithms.

364
00:21:24.840 --> 00:21:28.950
We often don't know how
these algorithms make decisions. Even

365
00:21:28.950 --> 00:21:31.920
their creators don't know how
they make decisions. And this

366
00:21:31.920 --> 00:21:35.580
is really most pronounced in
a product called compass, which

367
00:21:35.580 --> 00:21:38.340
is a machine learning algorithm
that has been used to

368
00:21:38.340 --> 00:21:43.710
predict. Defendant's really likely hoods
of recidivism and research from

369
00:21:43.710 --> 00:21:48.270
ProPublica and nonprofit journalism outlet
found that compass was making

370
00:21:48.810 --> 00:21:53.610
incorrect decisions correlated with race.
So they were incorrectly predicting

371
00:21:53.610 --> 00:21:57.030
that black defendants were more
likely to recommit crimes. And

372
00:21:57.030 --> 00:22:00.570
they were incorrectly predicting that
white defendants were more likely

373
00:22:00.570 --> 00:22:05.070
to recommit crimes, but both
ex both predictions were wrong.

374
00:22:05.310 --> 00:22:08.730
And this isn't a hypothetical
product judges in over a

375
00:22:08.730 --> 00:22:13.950
dozen us States have used
campuses outputs to impact the

376
00:22:13.980 --> 00:22:17.880
sentences of defendants, whether they
were released on parole or

377
00:22:17.880 --> 00:22:21.930
whether they got longer sentences.
And when one person impacted

378
00:22:21.930 --> 00:22:24.150
by campus's results tries to
take his case to the

379
00:22:24.210 --> 00:22:27.840
Supreme court. The judge has
actually refused to hear it.

380
00:22:27.840 --> 00:22:30.870
And that to me, sets
a really dangerous legal precedent

381
00:22:30.870 --> 00:22:34.110
because it signals that more
than half of Supreme court

382
00:22:34.110 --> 00:22:37.770
justices were willing to trust
how the compass algorithm was

383
00:22:37.770 --> 00:22:41.910
making decisions, even when those
decisions were wrong. And so

384
00:22:41.910 --> 00:22:48.180
that really sets again a
problematic legal precedent for how

385
00:22:48.180 --> 00:22:51.360
AI will be regulated or
not. The good news is

386
00:22:51.360 --> 00:22:54.240
that if you have someone
to monitor your data sets

387
00:22:54.250 --> 00:22:58.560
throughout the product life cycle,
this can be mitigated. So

388
00:22:58.590 --> 00:23:01.560
the developers often build training
sets based on data. They

389
00:23:01.560 --> 00:23:04.830
hope their models will encounter
in deployment, but many of

390
00:23:04.830 --> 00:23:07.620
them don't monitor the data
that they will receive in

391
00:23:07.620 --> 00:23:10.830
deployment and ML products are
unique in the sense that

392
00:23:10.830 --> 00:23:14.370
they're constantly taking in new
data and refining the results

393
00:23:14.370 --> 00:23:17.700
based on that data. But
the problem as mentioned is

394
00:23:17.700 --> 00:23:21.420
that when they encounter different
data, that could have a

395
00:23:21.420 --> 00:23:25.290
negative impact. And so it's
also not uncommon for algorithms

396
00:23:25.290 --> 00:23:30.000
to update without the model
itself being revalidated. And so

397
00:23:30.000 --> 00:23:32.430
if you have someone on
your team, who's monitoring the

398
00:23:32.430 --> 00:23:36.180
source history and context of
the data in your algorithm,

399
00:23:36.240 --> 00:23:39.870
not just in production, but
also in deployment, they can

400
00:23:39.900 --> 00:23:44.940
conduct continuous audits and find
unacceptable behavior. Yeah. It also

401
00:23:44.960 --> 00:23:49.130
seems like if, if people
are, some people are using

402
00:23:49.130 --> 00:23:51.620
or they're refining their algorithms
and they're refining their model,

403
00:23:51.650 --> 00:23:54.470
other people, other people aren't,
but the idea that the

404
00:23:54.470 --> 00:23:59.390
average Joe or Jane doesn't
understand it much less understand

405
00:23:59.390 --> 00:24:02.390
code. I think we've all
read and heard when machine,

406
00:24:02.600 --> 00:24:06.050
when lawyers and judges try
to understand like even basic

407
00:24:06.050 --> 00:24:08.990
code, like I've, I've been
involved in lawsuits where there's

408
00:24:08.990 --> 00:24:11.990
an analysis of an algorithm
and the quote unquote algorithm

409
00:24:11.990 --> 00:24:14.480
is like a couple of
four loops, right? It's not

410
00:24:14.930 --> 00:24:19.190
rocket surgery. If you then
introduce machine learning and artificial

411
00:24:19.190 --> 00:24:22.580
intelligence to it, they have
no idea what's going on.

412
00:24:22.880 --> 00:24:27.080
How can they possibly litigate
or understand that and, and

413
00:24:27.080 --> 00:24:29.090
do the right thing. It's
really just rolling the dice.

414
00:24:29.890 --> 00:24:32.920
Right? And I think that
the key there is not

415
00:24:32.920 --> 00:24:37.240
so much that everyone should
be so skilled on machine

416
00:24:37.240 --> 00:24:41.170
learning that they can read
and interpret code. It's more

417
00:24:41.170 --> 00:24:46.270
about having the general awareness
that algorithms are not unbiased,

418
00:24:46.270 --> 00:24:49.960
and that is important for
the devs and product teams

419
00:24:49.960 --> 00:24:52.480
to know who are building
these creations, but it's important

420
00:24:52.480 --> 00:24:54.940
for everyone to know it.
I would argue it's very

421
00:24:54.940 --> 00:25:00.010
important for Supreme court justices
to not blindly trust algorithms

422
00:25:00.010 --> 00:25:04.150
that are being used to
make incorrect projections about whether

423
00:25:04.150 --> 00:25:06.760
a person is more likely
or less likely to recommit

424
00:25:06.970 --> 00:25:10.240
a crime, they should instead
be asking, how did you

425
00:25:10.300 --> 00:25:14.200
make that decision and holding
the creators accountable for those

426
00:25:14.200 --> 00:25:18.700
decisions? And so it's not
about, again, understanding the code

427
00:25:18.700 --> 00:25:23.230
at a really deep level.
It's about understanding that algorithms

428
00:25:23.230 --> 00:25:27.100
have their own biases, just
like people, especially as we're

429
00:25:27.100 --> 00:25:29.260
in this stage where we
are trying to get closer

430
00:25:29.260 --> 00:25:32.500
to general AI, where it
does more closely mimic the

431
00:25:32.500 --> 00:25:36.760
thought processes and decisions of
humans. That means if you're

432
00:25:36.760 --> 00:25:40.840
taking that to its logical
conclusion that the AI could

433
00:25:40.840 --> 00:25:43.720
potentially get even more biased.
And so then it becomes

434
00:25:43.720 --> 00:25:48.850
even more important to introduce
measures from the start to

435
00:25:49.390 --> 00:25:53.620
mitigate it. Okay. So if
we explain that to nontechnical

436
00:25:53.650 --> 00:25:56.890
parent or partner, and they
start to understand that algorithms

437
00:25:56.890 --> 00:26:01.120
exist and that the algorithms
sometimes are generated, sometimes are

438
00:26:01.120 --> 00:26:05.650
written, sometimes have bias. How
do we, and should we

439
00:26:05.650 --> 00:26:09.280
separate the humans that wrote
it from the algorithm? Like,

440
00:26:09.280 --> 00:26:11.830
for example, if we use
Facebook, like, is there an

441
00:26:11.860 --> 00:26:16.060
evil mustache, twirling programmer who
made a biased algorithm and

442
00:26:16.060 --> 00:26:20.800
should nontechnical parent think that,
or is it that humans

443
00:26:20.800 --> 00:26:24.520
are complicated and messy meat
bags? And we sometimes screw

444
00:26:24.520 --> 00:26:27.640
up like, how much should
we consider malice in the

445
00:26:27.640 --> 00:26:32.530
terms of bias and AI
versus incompetence? Definitely the latter.

446
00:26:32.560 --> 00:26:36.490
It's definitely not a wizard
of Oz scenario where there's

447
00:26:36.490 --> 00:26:40.300
one person behind the curtain
pulling puppet strings. It really

448
00:26:40.300 --> 00:26:44.760
is very easy for bias
to, into an algorithm for

449
00:26:44.760 --> 00:26:49.290
many of the reasons mentioned.
And oftentimes again, the creators

450
00:26:49.290 --> 00:26:53.280
of these algorithms can't see
how they made decisions. The

451
00:26:53.280 --> 00:26:55.680
other thing that you have
to consider is that the

452
00:26:55.680 --> 00:27:00.180
more fair an algorithm becomes,
the more of an increase

453
00:27:00.180 --> 00:27:03.750
in risk. You have that
an algorithm won't be accurate.

454
00:27:03.750 --> 00:27:06.690
And so it's a delicate
trade off in fairness versus

455
00:27:06.690 --> 00:27:10.530
accuracy. That again, you have
to define at the outset.

456
00:27:10.680 --> 00:27:13.320
So it's not an either,
I don't want to frame

457
00:27:13.320 --> 00:27:16.200
this as an either or
scenario because you can have

458
00:27:16.230 --> 00:27:20.550
an algorithm that is both
unbiased and accurate, but you

459
00:27:20.550 --> 00:27:25.140
do have to account for
that as well, because those

460
00:27:25.140 --> 00:27:28.080
are, again, there are so
many factors that going into

461
00:27:28.080 --> 00:27:32.100
the algorithm because they're built
on endless neurons. And so

462
00:27:32.130 --> 00:27:34.380
you have to just be
thinking about these things from

463
00:27:34.380 --> 00:27:37.680
the start once before you
go in and start training

464
00:27:37.680 --> 00:27:41.450
the models. So if the,
if the listener is now

465
00:27:41.450 --> 00:27:45.380
sufficiently overwhelmed and probably thinking,
well, I should just not

466
00:27:45.380 --> 00:27:48.740
touch this at all. I
shouldn't even introduce AI and

467
00:27:48.740 --> 00:27:51.920
ML into my product. What
would you tell them? I

468
00:27:51.920 --> 00:27:54.560
would tell them that in
many ways it has never

469
00:27:54.560 --> 00:27:59.960
been easier to be working
on algorithms that are unbiased,

470
00:27:59.960 --> 00:28:03.680
because we are fortunate in
that 2018 in particular saw

471
00:28:03.710 --> 00:28:06.830
the release of many different
open source data sets that

472
00:28:06.830 --> 00:28:10.340
you can use to train
your algorithms. And that has

473
00:28:10.400 --> 00:28:15.230
historically been a big barrier
is not only the volume

474
00:28:15.230 --> 00:28:19.220
of data, but how specific
it is. You mentioned tagging

475
00:28:19.220 --> 00:28:22.790
earlier. And that's a very
important aspect of building algorithms

476
00:28:22.790 --> 00:28:26.030
that are fair tagging, refers
to classes that are present

477
00:28:26.030 --> 00:28:29.570
in an image and their
locations. And this sounds simple

478
00:28:29.570 --> 00:28:31.550
until you realize how much
work it would take to

479
00:28:31.550 --> 00:28:34.970
dress shapes around every single
person in a photo of

480
00:28:34.970 --> 00:28:37.640
a crowd, or you would
draw a box around every

481
00:28:37.640 --> 00:28:40.790
single person on a highway.
Even if you succeeded, you

482
00:28:40.790 --> 00:28:43.430
might rush the tagging and
you might draw your shape

483
00:28:43.440 --> 00:28:46.460
slop, really. And that would
lead to a neural network

484
00:28:46.460 --> 00:28:49.940
that's poorly trained, but there
are to that point where

485
00:28:49.940 --> 00:28:53.360
a product's coming to market.
One example is a data

486
00:28:53.360 --> 00:28:57.200
annotation project called brain builder.
And they use open source

487
00:28:57.200 --> 00:29:00.860
frameworks like TensorFlow to help
users manage and annotate the

488
00:29:00.860 --> 00:29:04.820
training data. It also aims
to bring diverse class examples

489
00:29:04.820 --> 00:29:08.570
to data sets so that
you can teach algorithms positive

490
00:29:08.570 --> 00:29:12.560
examples of something and negative
examples of something. So I

491
00:29:12.560 --> 00:29:17.270
would encourage people who are
interested in building algorithms that

492
00:29:17.270 --> 00:29:21.620
are less biased to seek
out date training sets that

493
00:29:21.620 --> 00:29:24.770
they can use on places
like GitHub brain builder is

494
00:29:24.770 --> 00:29:28.370
a product that is worth
looking into. There are more

495
00:29:28.370 --> 00:29:32.390
options than ever to build
unbiased algorithms. And so I

496
00:29:32.390 --> 00:29:35.180
would encourage them to go
the open source route. And

497
00:29:35.180 --> 00:29:38.270
that really goes back to
the crucial role that transparency

498
00:29:38.270 --> 00:29:42.250
plays in building these algorithms.
Fantastic. Well, I appreciate You

499
00:29:42.250 --> 00:29:45.160
chatting with me today, Laura
McPhail all about bias in

500
00:29:45.160 --> 00:29:48.790
AI, and I'll be sure
to include links to Lauren's

501
00:29:48.790 --> 00:29:53.260
articles on the next web
and on opensource.com. And thanks

502
00:29:53.260 --> 00:29:55.450
for the work that you're
doing, helping to put all

503
00:29:55.450 --> 00:29:58.020
of this into content. Thanks
so much for having me.

504
00:29:58.020 --> 00:30:00.510
I had a great time
talking to you. This has

505
00:30:00.510 --> 00:30:03.120
been another episode of Hansel
minutes, and we'll see you

506
00:30:03.120 --> 00:30:03.990
again next week.

