WEBVTT FILE

1
00:00:00.270 --> 00:00:03.420
Hi, this is Scott. I
really appreciate our sponsors because

2
00:00:03.420 --> 00:00:06.390
they make the show possible.
Today's show is sponsored by

3
00:00:06.390 --> 00:00:10.590
developer express, become a UI
superhero with dev express controls

4
00:00:10.590 --> 00:00:15.120
and libraries. Deliver elegant.net solutions
that address customer needs today

5
00:00:15.480 --> 00:00:19.590
by leveraging your existing knowledge,
you can build next generation

6
00:00:19.590 --> 00:00:23.700
touch enabled solutions for tomorrow,
you can download your free

7
00:00:23.790 --> 00:00:45.600
30 day trial@dxdothanselminutes.com. That's dx.hanselminutes.com.
Hi, this is Scott Hanselman.

8
00:00:45.600 --> 00:00:48.270
This is another episode of
Hanselminutes today. I'm talking with

9
00:00:48.300 --> 00:00:52.500
Leslie Caseta. She's a transportation
technologist at the Atlanta regional

10
00:00:52.500 --> 00:00:55.410
commission, a full stack web
developer, and she's got a

11
00:00:55.410 --> 00:00:58.140
master's degree in city and
regional planning. How are you

12
00:00:58.910 --> 00:01:01.610
Scott? Doing well? Thank you
for having me on Very

13
00:01:01.610 --> 00:01:03.920
excited to talk to you.
And we met at the

14
00:01:03.920 --> 00:01:07.820
we rise conference down in
Atlanta and within five minutes

15
00:01:07.820 --> 00:01:11.420
we were geeking out about
self driving cars. It sounds

16
00:01:11.420 --> 00:01:13.730
like one of the things
that you do in your

17
00:01:13.730 --> 00:01:16.160
work as a planner is
kind of preparing for the

18
00:01:16.160 --> 00:01:19.370
changes that are happening in
transportation. And it seems like

19
00:01:19.400 --> 00:01:22.250
there's no better time to
be interested in the topic

20
00:01:22.250 --> 00:01:25.820
than 2017. Yeah, you're right
on the money. I could

21
00:01:25.820 --> 00:01:29.150
not have imagined being in
a more exciting role back

22
00:01:29.150 --> 00:01:31.040
when I was in my
master's program, I thought I

23
00:01:31.040 --> 00:01:34.070
was going to be in
environmental planning doing water work,

24
00:01:34.100 --> 00:01:37.820
but which was also very
exciting, but the concepts behind

25
00:01:38.240 --> 00:01:41.970
emerging technologies in transportation are
just blowing my mind and

26
00:01:41.970 --> 00:01:44.090
I, it's hard to keep
up, which is very exciting.

27
00:01:44.810 --> 00:01:47.090
And I get to do
work that looks at how

28
00:01:47.450 --> 00:01:50.450
we make the best use
of all these technologies into

29
00:01:50.450 --> 00:01:54.050
the future and how we
fund them in a way

30
00:01:54.050 --> 00:01:56.840
that we can get our
hands around. And it's, it's

31
00:01:56.840 --> 00:01:59.960
more than just like transportation
technology in the sense of

32
00:01:59.960 --> 00:02:03.170
there's newer and fancier cars
coming out. Isn't it. There's

33
00:02:03.170 --> 00:02:06.290
also big data behind it.
And there's the internet of

34
00:02:06.290 --> 00:02:08.690
things like these cars need
to be tracked and thought

35
00:02:08.690 --> 00:02:11.720
about. So it's more than
just the advanced vehicle technologies.

36
00:02:12.170 --> 00:02:16.130
Yeah, Absolutely. The infrastructure that
needs to support the connected

37
00:02:16.130 --> 00:02:19.670
and autonomous vehicles are streetlights
and all the data sharing

38
00:02:19.670 --> 00:02:22.880
that will come out of
just smarter roads. It's pretty

39
00:02:22.880 --> 00:02:26.300
exciting. And exhaustive. One thing
we're definitely pushing on is

40
00:02:26.300 --> 00:02:29.450
how do we share that
data amongst jurisdictions in real

41
00:02:29.450 --> 00:02:33.020
time to make better decisions
or get into predictive analytics

42
00:02:33.020 --> 00:02:36.080
as well? What does that
mean when you said smarter

43
00:02:36.080 --> 00:02:39.140
roads? How is the road
smart? Sure. So there's some

44
00:02:39.620 --> 00:02:42.380
applications that we've seen case
studies of already of just

45
00:02:42.770 --> 00:02:46.880
sensors and parking spaces to
better know when a parking

46
00:02:46.880 --> 00:02:49.670
deck or on street parking
is available or not. And

47
00:02:49.670 --> 00:02:54.290
getting that to drivers through
connected vehicles quicker so that

48
00:02:54.470 --> 00:02:57.680
we have less roads on
the car just circling. So

49
00:02:57.680 --> 00:02:59.890
a lot of time is
wasted. Sure. You've had the

50
00:02:59.890 --> 00:03:04.030
experience of just circling a
parking lot. I'm looking for

51
00:03:04.030 --> 00:03:07.690
that one space. So that's
one application that's out there.

52
00:03:07.690 --> 00:03:10.240
There's also neural networks that
we can put on streetlights

53
00:03:10.600 --> 00:03:15.970
to pick up on weather
patterns, air quality and crashes

54
00:03:15.970 --> 00:03:18.970
as well, perhaps, and, and
do some really cool, big

55
00:03:18.970 --> 00:03:23.850
data, predictive analytics with that.
Sometimes it's fancy stuff like

56
00:03:23.850 --> 00:03:26.160
neural networks, but sometimes it
can be as simple as

57
00:03:26.190 --> 00:03:28.740
a green light on an
led that tells me that

58
00:03:28.740 --> 00:03:31.500
a parking spot is available
here in Portland, where I

59
00:03:31.500 --> 00:03:35.730
live at the Portland international
airport. They put lights above

60
00:03:35.730 --> 00:03:38.100
every parking spot and finding
a parking spot is just

61
00:03:38.100 --> 00:03:40.890
simply driving to wherever the
first green light is. And

62
00:03:40.890 --> 00:03:43.740
it's not super smart. It's
like if it sees a

63
00:03:43.740 --> 00:03:48.750
car, the light turns red.
Yeah. Yeah. We had a

64
00:03:48.750 --> 00:03:51.540
fun experience here in Atlanta
a couple of months ago

65
00:03:51.540 --> 00:03:56.730
where a major part of
our network <inaudible> collapsed. It's

66
00:03:56.740 --> 00:03:59.760
a couple of hundred feet
of road that, that fell

67
00:03:59.970 --> 00:04:03.510
due to a fire. And
anyway, put a lot of

68
00:04:03.510 --> 00:04:05.760
pressure on our transit system,
as you could imagine, cause

69
00:04:05.760 --> 00:04:08.340
people were, couldn't get to
work through the regular means

70
00:04:08.340 --> 00:04:11.970
of driving in some areas.
And we had a good

71
00:04:11.970 --> 00:04:16.260
chance to put in some
quick, not super sophisticated, very

72
00:04:16.260 --> 00:04:21.390
crowdsourced application of figuring out
communicating when our transit parking

73
00:04:21.390 --> 00:04:25.080
lot, right. And lot right
in parking lots had openings

74
00:04:25.080 --> 00:04:26.760
and one where they were
full. So people could go

75
00:04:26.760 --> 00:04:30.900
to the next stop if
possible. And, and that was

76
00:04:30.900 --> 00:04:35.760
just an online, you know,
red, green system with every

77
00:04:35.760 --> 00:04:38.130
parking lot name next to
it that people could check

78
00:04:38.130 --> 00:04:42.540
in on. It's interesting how,
how simple ideas like that

79
00:04:42.540 --> 00:04:45.540
can add up to a
smart city, but you had

80
00:04:45.540 --> 00:04:47.700
a presentation that you gave
where you talk about like,

81
00:04:47.730 --> 00:04:50.190
what does it, what does
a city have to have

82
00:04:50.220 --> 00:04:52.140
for it to be a
smart city? And there's quite

83
00:04:52.140 --> 00:04:55.530
a few components there. Yeah,
you're right. So last year

84
00:04:55.530 --> 00:04:59.490
we, my team, I got
to lead a policy document

85
00:04:59.490 --> 00:05:02.820
on transportation technology, looking at
what a smart, smart city

86
00:05:02.820 --> 00:05:06.000
means from that perspective and
getting to some policy implications

87
00:05:06.000 --> 00:05:10.020
out of that, it really
all circled back to better

88
00:05:10.020 --> 00:05:13.260
management of data, whether it's
existing data or future data

89
00:05:13.260 --> 00:05:16.470
that we could get our
hands on. But then I

90
00:05:16.470 --> 00:05:23.070
guess you could look further
into some sustainability applications of,

91
00:05:23.130 --> 00:05:27.300
of kind of green infrastructure,
solar roads, getting more into

92
00:05:27.360 --> 00:05:31.830
electric vehicles and bioswells to
handle water runoff during big

93
00:05:31.830 --> 00:05:36.480
storms. We, so we looked
at a lot of components

94
00:05:36.480 --> 00:05:39.660
and, and try to apply
what it all means to

95
00:05:39.660 --> 00:05:44.700
our policy goals. So we
want equity for more people.

96
00:05:44.700 --> 00:05:48.210
We want better access to
jobs, better access to healthcare.

97
00:05:48.600 --> 00:05:50.640
So we're keeping an eye
on what we can do

98
00:05:50.640 --> 00:05:56.430
to, to embrace a new
fancy or existing technologies to,

99
00:05:56.430 --> 00:06:00.640
to help our grow. One
of the things that I

100
00:06:00.670 --> 00:06:03.040
can tell that you're passionate
about from reading your papers

101
00:06:03.040 --> 00:06:08.140
and your policy documents is
about technology enabled mobility options,

102
00:06:08.650 --> 00:06:11.140
the idea that people, everyone
deserves to be able to

103
00:06:11.140 --> 00:06:14.440
get up and go where
they want to go. Yes.

104
00:06:14.440 --> 00:06:17.020
So something really exciting that
came out of the document

105
00:06:17.020 --> 00:06:20.380
that blew my mind was
this concept of mobility as

106
00:06:20.380 --> 00:06:24.130
a service, which I understood
before, but the finances behind

107
00:06:24.130 --> 00:06:28.300
it, which some companies already
thinking about going from right

108
00:06:28.300 --> 00:06:30.250
now, most of us are
in a car and it

109
00:06:30.250 --> 00:06:33.400
sits vacant 94% of the
time. Maybe some people have

110
00:06:33.400 --> 00:06:36.580
it on Turo or some
car sharing application, but most

111
00:06:36.580 --> 00:06:38.200
of the time it's not
making you income. It's just

112
00:06:38.200 --> 00:06:40.300
set it sitting there. And
it's a huge expense. It's

113
00:06:40.300 --> 00:06:42.760
like the second biggest expense
after buying a home in

114
00:06:42.760 --> 00:06:46.810
most households and mobility is
a service really makes it

115
00:06:47.140 --> 00:06:49.960
pushes the envelope to give
you more mobility options without

116
00:06:49.960 --> 00:06:53.500
having to own a car.
And I think many OEMs

117
00:06:53.500 --> 00:06:55.630
like GM and Ford are
seeing that trend and getting

118
00:06:55.630 --> 00:06:58.810
into this car sharing space.
But the math really works

119
00:06:58.810 --> 00:07:05.080
out to two more people
per car. So you have

120
00:07:05.080 --> 00:07:10.540
less pollution per car, better
dollars for maintenance per car.

121
00:07:10.840 --> 00:07:12.250
And at the end of
the day, you have a

122
00:07:12.250 --> 00:07:15.670
car maybe sitting 4% of
the time instead of 96%

123
00:07:15.670 --> 00:07:17.770
of the time. So you
get you better utility out

124
00:07:17.770 --> 00:07:20.110
of that. And if you
throw in car sharing like

125
00:07:20.140 --> 00:07:25.300
Uber pool or Lyft line,
you get greener trips. And

126
00:07:25.300 --> 00:07:29.230
that, that is likely to
come about as a, kind

127
00:07:29.230 --> 00:07:31.270
of like a cell phone
service. So you buy minutes

128
00:07:31.270 --> 00:07:34.840
or miles in a car
like we do with cell

129
00:07:34.840 --> 00:07:37.300
phone companies now. And, and
that's how you'd get around.

130
00:07:37.300 --> 00:07:39.610
And you could see that
really taking off in the,

131
00:07:39.700 --> 00:07:42.730
in the next five, 10
years in urban settings and

132
00:07:42.730 --> 00:07:47.830
may be expanding to suburban
communities shortly after That seems

133
00:07:47.830 --> 00:07:51.760
like something that would be
like extremely disruptive. I mean,

134
00:07:51.790 --> 00:07:54.100
there are like just to
speak about it from an

135
00:07:54.100 --> 00:07:57.940
American's perspective, we really like
our cars and Americans tend

136
00:07:57.940 --> 00:08:01.360
to buy bigger cars than
they need. And one's wealth

137
00:08:01.990 --> 00:08:05.020
can, sometimes one can sometimes
express their wealth with their,

138
00:08:05.500 --> 00:08:07.390
with their, I have a
lot of cars or I

139
00:08:07.390 --> 00:08:09.700
have a big car. We
would need to turn that

140
00:08:09.700 --> 00:08:14.050
entire idea on its head.
Sure. Yeah. It's, it's exciting

141
00:08:14.050 --> 00:08:16.900
and scary depending on who
you ask, right. Someone you

142
00:08:16.900 --> 00:08:19.270
can pencil, present that idea
to, and they'll think that's

143
00:08:19.270 --> 00:08:22.450
fantastic. That's a kind of
a heaven scenario. And then

144
00:08:22.720 --> 00:08:25.360
someone else who really enjoys
driving and has an awesome

145
00:08:25.360 --> 00:08:28.840
maybe Porsche Carerra or something
thinks it's a health scenario.

146
00:08:28.840 --> 00:08:32.500
Like I don't want to
drive around with autonomous vehicles

147
00:08:32.500 --> 00:08:34.660
everywhere. I want to, you
know, get the feel of

148
00:08:34.660 --> 00:08:37.930
the car. So it's, it's
a balance. We'll see how

149
00:08:37.930 --> 00:08:41.230
the market does. I'd say
in Atlanta, we are a

150
00:08:41.230 --> 00:08:44.050
very car dependent city. We've
been built that way and

151
00:08:44.110 --> 00:08:47.590
people love their cars. And
some of the big companies

152
00:08:47.590 --> 00:08:53.500
that are relocating here, state
farm Mercedes NCR are all

153
00:08:53.740 --> 00:08:59.490
building headquarters next to transit,
getting, knowing that they want

154
00:08:59.520 --> 00:09:02.670
good talent. And that talent
tends to be younger, I

155
00:09:02.670 --> 00:09:05.820
guess, software developers or the
like, and they want to

156
00:09:05.820 --> 00:09:08.520
be car-free. So they're following
the market and building that

157
00:09:08.520 --> 00:09:11.430
market in a very car
centric city. So that, that

158
00:09:11.430 --> 00:09:13.440
makes me excited for the
possibility of it to take

159
00:09:13.440 --> 00:09:17.760
off. And in cities like
New York or Portland, where, where

160
00:09:17.760 --> 00:09:21.390
the, the shift is already
happening towards more of a

161
00:09:21.390 --> 00:09:25.790
sharing economy. Yeah. The idea
of like zip cars or

162
00:09:25.790 --> 00:09:28.370
cars that I can just
walk up to and, you

163
00:09:28.370 --> 00:09:30.530
know, put a smart card
against and drive away has

164
00:09:30.530 --> 00:09:33.650
really taken off here in
Portland. Ultimately it comes down

165
00:09:33.650 --> 00:09:36.350
to, like you said, mobility
on demand. I want to

166
00:09:36.350 --> 00:09:38.930
go somewhere. Is it a
big deal? Do I have

167
00:09:38.930 --> 00:09:42.140
to wait 45 minutes for
an Uber? Do I have

168
00:09:42.140 --> 00:09:44.840
to go and hunt around
for a zip car? If

169
00:09:44.840 --> 00:09:47.810
the car can come to
me in either either I

170
00:09:47.810 --> 00:09:51.020
drive it or it drives
itself, then that's a good

171
00:09:51.020 --> 00:09:56.090
system. Yeah. There's some with
any company that's a startup,

172
00:09:56.090 --> 00:09:58.100
you get some really cool
ideas and then you have

173
00:09:58.100 --> 00:10:01.550
to ground-truth them. Right. And
one of the coolest one

174
00:10:01.550 --> 00:10:06.710
I heard was from an
AB company about focusing a

175
00:10:06.710 --> 00:10:09.080
lot on the human design
interface so that you can

176
00:10:09.110 --> 00:10:12.380
request a car, it comes
to you and sometimes you

177
00:10:12.380 --> 00:10:14.570
don't even have there. They're
getting to not needing to

178
00:10:14.570 --> 00:10:17.480
request it, but it knowing
your calendar syncing with your

179
00:10:17.480 --> 00:10:20.810
Google calendar or outlook calendar
icon, and knowing that you'll

180
00:10:20.810 --> 00:10:22.880
need a car in a
few minutes, so it'll send

181
00:10:22.880 --> 00:10:27.290
it and be ready at
the corner intersection near you.

182
00:10:27.890 --> 00:10:31.880
And then once you're in
the car, the ads or

183
00:10:31.880 --> 00:10:35.270
videos or magazines will be
catered to you as well,

184
00:10:35.690 --> 00:10:37.850
which is really creepy and
fun. I think that's, yeah,

185
00:10:37.880 --> 00:10:39.950
I'm not sure. I like
the idea, like I'm the

186
00:10:39.950 --> 00:10:41.570
F when I get in
a cab, like I have

187
00:10:41.570 --> 00:10:43.220
to go to Vegas tomorrow.
And the first thing I'm

188
00:10:43.220 --> 00:10:44.420
going to do is get
in the cab and then

189
00:10:44.450 --> 00:10:47.120
turn off the automatic video
that decides to play. And

190
00:10:47.120 --> 00:10:50.960
tell me all about Vegas.
Yeah. It's, it's interesting. I

191
00:10:50.960 --> 00:10:52.670
think we talked about this
when we met at re

192
00:10:52.670 --> 00:10:57.170
where we rise a little
bit about convenience and ease

193
00:10:57.200 --> 00:10:59.720
and privacy, and you can
almost see it as a

194
00:10:59.720 --> 00:11:05.030
graph. We all have concerns
about our data getting out

195
00:11:05.030 --> 00:11:08.240
there and being used for
someone else's gain and not

196
00:11:08.240 --> 00:11:11.360
our own. And I think
an example with Google is

197
00:11:11.360 --> 00:11:14.660
that they've made things so
easy that you're willing to

198
00:11:14.660 --> 00:11:17.480
give up some of that,
those privacy concerns for the

199
00:11:17.480 --> 00:11:20.150
convenience that you get of
their service. And I think

200
00:11:20.150 --> 00:11:22.520
the same thing could be
applied in mobility in the

201
00:11:22.520 --> 00:11:25.520
future. One of the things
that you pointed out in

202
00:11:25.520 --> 00:11:29.090
your presentation was that in
the past the, a number

203
00:11:29.090 --> 00:11:31.640
of years until the technology
was used by a quarter

204
00:11:31.640 --> 00:11:35.960
of Americans was quite long,
almost a generation. And now

205
00:11:35.960 --> 00:11:41.660
it is absolutely measurable. Yes.
The pace at which technology

206
00:11:42.290 --> 00:11:46.070
gets developed and tested and
meat gets to the market

207
00:11:46.070 --> 00:11:49.520
and gets consumed by. I
think that example had a

208
00:11:49.520 --> 00:11:54.610
quarter of a million of
Americans. It's just a crazy,

209
00:11:54.610 --> 00:11:58.630
it's absolutely crazy. There's a
lot of flaws, you know,

210
00:11:58.630 --> 00:12:02.320
in that, in that chart
that I showed. But nevertheless,

211
00:12:02.320 --> 00:12:04.090
there are a lot of
opportunities. I think that showed

212
00:12:04.090 --> 00:12:08.470
like the salt, the home
phone is being introduced in

213
00:12:08.470 --> 00:12:12.010
about 1936 and it taking
like 35 years or something

214
00:12:12.370 --> 00:12:15.490
to, to really make it
to market and be consumed

215
00:12:15.490 --> 00:12:18.160
by a quarter million or
a quarter of the population

216
00:12:18.160 --> 00:12:20.650
of Americans. And then the
cell phone coming out in

217
00:12:20.650 --> 00:12:24.160
1986 or so, and it
only taking like 13 years

218
00:12:24.160 --> 00:12:27.040
to be consumed. So that's
a third of reduction in

219
00:12:27.040 --> 00:12:29.530
time. And, and I think
we see it in the

220
00:12:30.190 --> 00:12:33.700
technologies, in our cars right
now that it's really come

221
00:12:33.700 --> 00:12:37.020
a long way since the
model T Ford. Oh yeah.

222
00:12:37.050 --> 00:12:38.730
And you can look at
it, it happens over and

223
00:12:38.730 --> 00:12:40.410
over and over again, you
look at Bluetooth in the

224
00:12:40.410 --> 00:12:44.310
car, but my, my nontechnical
parents use Bluetooth in the

225
00:12:44.310 --> 00:12:49.350
car. The, and actually one
of the things I was

226
00:12:49.350 --> 00:12:51.420
talking to my dad about
is that even stupidly named

227
00:12:51.420 --> 00:12:54.210
technologies get used like blue
Ray is a dumb name

228
00:12:54.210 --> 00:12:57.180
for, for a thing. And
Bluetooth, like, they don't know

229
00:12:57.180 --> 00:12:58.620
why it's called that. I
don't know why it's called

230
00:12:58.620 --> 00:13:00.360
that, but it only took
a couple of years. And

231
00:13:00.360 --> 00:13:05.970
now we've got these, you
know, Amazon Alexa, voice devices

232
00:13:05.970 --> 00:13:08.820
in our houses. And there's
concern of course around, is

233
00:13:08.820 --> 00:13:10.950
it listening all the time,
but that didn't take more

234
00:13:10.950 --> 00:13:14.550
than maybe five years from
the time it was conceived

235
00:13:14.550 --> 00:13:18.240
of. Right. Yeah. And that's
another thing where convenience kind

236
00:13:18.240 --> 00:13:20.820
of lets you buy one.
You're like, Oh cool. I'll

237
00:13:20.820 --> 00:13:23.340
play music just by the
sound of my voice or

238
00:13:23.880 --> 00:13:30.120
order more. I don't know
something like dryer sheets or

239
00:13:31.590 --> 00:13:35.730
Amazon. So one of the
things that I'm interested in

240
00:13:35.730 --> 00:13:38.190
that you've worked on is
this idea of the self

241
00:13:38.190 --> 00:13:42.000
driving car. And it seems
to me like, at least

242
00:13:42.000 --> 00:13:44.730
in the news, like the
Tesla came out and people

243
00:13:44.820 --> 00:13:48.060
had, it had a feature
called autopilot, which seems to

244
00:13:48.060 --> 00:13:51.750
me like it's, it's kind
of disingenuous. It's not autopilot.

245
00:13:51.780 --> 00:13:56.010
It's, it's fancy stay in
the lines, cruise control. But

246
00:13:56.010 --> 00:13:58.440
by calling it autopilot that
may have set up some

247
00:13:58.440 --> 00:14:02.160
expectations a little earlier than
appropriate for people. And I

248
00:14:02.160 --> 00:14:05.550
wonder if that is disrupted
or cause trouble in the

249
00:14:05.580 --> 00:14:09.210
thinking around self driving cars.
Yeah. I think it's, it

250
00:14:09.210 --> 00:14:11.550
may have set it back
a little bit, but Elan

251
00:14:11.550 --> 00:14:14.760
is such a, a snazzy
guy. Then I think it

252
00:14:14.760 --> 00:14:17.250
added enough of a sex
appeal. If I can say

253
00:14:17.250 --> 00:14:21.870
that, that it's still something
people are investing in. I

254
00:14:21.870 --> 00:14:24.450
mean their stock is wavering,
but it's still pretty high.

255
00:14:26.250 --> 00:14:29.160
And I think that's something
that's definitely something we've discussed

256
00:14:29.700 --> 00:14:34.140
with local government partners in
the Atlanta region about we're

257
00:14:34.350 --> 00:14:37.050
September 14th, we're rolling out a
big project. The city of

258
00:14:37.050 --> 00:14:41.880
Atlanta is with some autonomous
shuttles and autonomous vehicles for

259
00:14:41.880 --> 00:14:44.370
the public to use along
one of our major corridors.

260
00:14:44.760 --> 00:14:46.860
And a lot of the
discussion behind that has been

261
00:14:46.860 --> 00:14:50.040
well, do we do an
education campaign kind of to

262
00:14:50.040 --> 00:14:55.340
get over that fear or
do we just subtly release

263
00:14:55.340 --> 00:14:59.570
it and let people have
their fears overcome by experience?

264
00:15:00.380 --> 00:15:02.030
And I think, I think
the city, I don't want

265
00:15:02.030 --> 00:15:03.740
to speak for them, but
I think they're going for

266
00:15:03.740 --> 00:15:07.720
the latter. Cause I've seen
zero education campaigns. So that

267
00:15:07.720 --> 00:15:10.070
implies that people will just
figure it out and we're

268
00:15:10.070 --> 00:15:13.720
doing nothing is a perfectly
reasonable strategy. Okay. Yeah. Setting

269
00:15:13.720 --> 00:15:15.820
up, making sure. I think
on the city's on a

270
00:15:15.820 --> 00:15:18.350
lot of the liability questions
are out of other ways.

271
00:15:18.350 --> 00:15:20.800
So ensuring that the technology
is where we want it

272
00:15:20.800 --> 00:15:22.900
to be and people will
be able to use it

273
00:15:22.900 --> 00:15:26.980
safely and get to, and
fro easily it'll be a

274
00:15:26.980 --> 00:15:30.430
free service. So that helps.
But yeah, once people try

275
00:15:30.430 --> 00:15:33.430
something that, that is convenient,
I think they, they overlook

276
00:15:33.430 --> 00:15:38.890
certain things. We, we, we
talk about driver liability, but

277
00:15:38.890 --> 00:15:40.930
it seems to me that
it's really an issue of

278
00:15:40.930 --> 00:15:46.600
responsibility. Like ultimately when is
when, when is it my

279
00:15:46.600 --> 00:15:49.360
job to be aware of
this giant 4,000 pound vehicle

280
00:15:49.360 --> 00:15:53.050
that I'm piloting and when
is it not, when do

281
00:15:53.050 --> 00:15:55.330
I give up full responsibility?
And I was just on

282
00:15:55.330 --> 00:15:57.460
the train, like I know
that if I take the

283
00:15:57.460 --> 00:16:00.580
train, like I said, I'm
going to Vegas. I'm going

284
00:16:00.580 --> 00:16:02.830
to get on the monorail.
If the monorail crashes in

285
00:16:02.830 --> 00:16:05.620
no way, am I in
trouble? You know, I have

286
00:16:05.620 --> 00:16:09.490
no responsibility at all. Yeah.
Every company that's coming out

287
00:16:09.490 --> 00:16:12.460
with an autonomous shuttle or
autonomous car has a slightly

288
00:16:12.460 --> 00:16:15.430
different model. The one I
liked the most is the

289
00:16:15.430 --> 00:16:17.980
one from a, from a
local government perspective is the

290
00:16:17.980 --> 00:16:20.740
one that allows you to
kind of lease the vehicles

291
00:16:20.740 --> 00:16:24.400
and, and the company, the
provider has all the responsibility.

292
00:16:24.400 --> 00:16:26.020
And I think that's kind
of the wave of the

293
00:16:26.020 --> 00:16:29.560
future. So it's on them
to make sure that the

294
00:16:29.560 --> 00:16:33.760
software has the best recognition
or redundancy built in and

295
00:16:33.760 --> 00:16:38.560
that the, yeah, the, the
vehicle is safe for, for

296
00:16:38.560 --> 00:16:42.640
whatever speeds it'll go on,
whatever turns it'll make. And

297
00:16:42.670 --> 00:16:45.400
all of that is kind
of exciting in, in the

298
00:16:45.400 --> 00:16:50.140
transportation world, in that for
regulating cars, there's so many

299
00:16:50.140 --> 00:16:53.590
specs, so many very detailed
specs about a car has

300
00:16:53.590 --> 00:16:56.980
to have a steering wheel
and brake pads and things

301
00:16:56.980 --> 00:16:58.990
that look like this. And
at that angle and very

302
00:16:58.990 --> 00:17:03.280
detailed and with an autonomous
vehicle, that's, you know, kind

303
00:17:03.280 --> 00:17:05.380
of level four or five
where you don't need a

304
00:17:05.380 --> 00:17:08.980
driver at all the steering
wheel and the brakes go

305
00:17:08.980 --> 00:17:12.790
away. So the regulators at
the federal level are definitely

306
00:17:12.790 --> 00:17:14.590
coming to terms with that
and trying to figure that

307
00:17:14.590 --> 00:17:18.430
out. So at this point
I liked the model of

308
00:17:18.430 --> 00:17:22.420
leasing the vehicle and the
company's figuring it out because

309
00:17:22.420 --> 00:17:25.030
it makes more sense than,
than the local government's figuring

310
00:17:25.030 --> 00:17:27.490
it out for them. And
also we're still in this

311
00:17:27.520 --> 00:17:30.970
holding pattern of what the
federal regulations will look like

312
00:17:31.000 --> 00:17:34.480
for all these Let's dig
a little bit into that.

313
00:17:34.480 --> 00:17:36.500
You, you, you threw out
a couple of ideas. I

314
00:17:36.500 --> 00:17:38.710
don't think people are familiar
with, you talked about levels

315
00:17:38.710 --> 00:17:41.140
and you said level one
through level four, but there's

316
00:17:41.140 --> 00:17:43.690
also a level zero. What
are these levels of autonomy?

317
00:17:44.140 --> 00:17:48.190
Sure. Yeah. Levels one through
two. I might, I don't

318
00:17:48.190 --> 00:17:49.950
have the figures right in
front me, but from what

319
00:17:49.950 --> 00:17:53.550
I remember, it's kind of
most cars that are out

320
00:17:53.550 --> 00:17:57.780
there. Are there levels one
or two with maybe some

321
00:17:58.350 --> 00:18:00.570
detection that the cars there's
a car in the lane

322
00:18:00.570 --> 00:18:04.080
next to you and, and
cruise control level three, I

323
00:18:04.080 --> 00:18:08.670
believe gets into more of
the Tesla autopilot model. And

324
00:18:08.670 --> 00:18:12.630
then level four is where
you really get into barely

325
00:18:12.630 --> 00:18:15.900
needing a driver. And, and
then everyone kind of breaks

326
00:18:15.900 --> 00:18:18.030
it down a little bit
differently. There's a level four,

327
00:18:18.030 --> 00:18:21.300
a four B or straight
to level five where you

328
00:18:21.660 --> 00:18:26.090
it's the Waymo car kind
of system. Okay. So at

329
00:18:26.090 --> 00:18:29.330
level zero, the car is
not smart. The driver has

330
00:18:29.330 --> 00:18:32.360
control it's, you know, the
cars that, you know, are

331
00:18:32.450 --> 00:18:36.260
our parents drove cars in
the sixties and then level

332
00:18:36.260 --> 00:18:38.990
one or two would be
things like detecting that you're

333
00:18:38.990 --> 00:18:41.210
about to crash and maybe
hitting the brakes for you

334
00:18:41.210 --> 00:18:44.060
or cruise control. But the
car is telling you all

335
00:18:44.060 --> 00:18:49.880
about that. Right. It's informing
you. Yeah. And then yeah,

336
00:18:49.910 --> 00:18:52.490
level three, you might have
to take control back over

337
00:18:52.490 --> 00:18:54.380
and level four. You should
not have to touch the

338
00:18:54.380 --> 00:18:56.930
steering wheel if there is
one. Okay. So it seems

339
00:18:56.930 --> 00:18:59.030
like the big leap is
between three and four. So

340
00:18:59.030 --> 00:19:01.370
in level three, the driver
can grab the, grab the

341
00:19:01.370 --> 00:19:03.770
wheel and an emergency. And
like, no, not that, you

342
00:19:03.770 --> 00:19:06.710
know, correct. But level four
is when there's no steering

343
00:19:06.710 --> 00:19:10.430
wheel and then we all
get very uncomfortable. Hopefully not,

344
00:19:10.430 --> 00:19:14.480
but yes, likely, especially You're
competent as though like you,

345
00:19:14.540 --> 00:19:15.710
this is going to be
a thing, like, I know

346
00:19:15.710 --> 00:19:17.180
I'm teasing you. I say
that we're going to be

347
00:19:17.180 --> 00:19:19.280
uncomfortable without the steering wheel,
but you're like, no, we

348
00:19:19.280 --> 00:19:21.410
really aren't. It's going to
be okay. Yeah. I really

349
00:19:21.410 --> 00:19:25.670
don't think so. Hopefully the
autonomous shuttles will make it

350
00:19:25.670 --> 00:19:29.210
out first, make it to
market first. And, and those

351
00:19:29.210 --> 00:19:31.220
are like, those are going
to be at a magical

352
00:19:31.220 --> 00:19:36.410
place in Florida. You deduce
from there where I'm talking

353
00:19:36.410 --> 00:19:39.740
about. And, and you can
just hop in like a

354
00:19:39.740 --> 00:19:43.310
shuttle and take it from
parking lot Z to the

355
00:19:43.310 --> 00:19:46.940
front door of the park.
They'll also be in Tokyo

356
00:19:46.940 --> 00:19:50.750
for the Olympics. I think
the government or a subsidiary

357
00:19:50.780 --> 00:19:53.360
there bought a ton of,
I don't know, like 2000

358
00:19:53.360 --> 00:19:58.040
shuttles that are fully autonomous.
And once it starts making

359
00:19:58.040 --> 00:20:02.240
it into cars that are
produced by GM and Ford

360
00:20:02.240 --> 00:20:04.790
and the others, then, then
you might have a little

361
00:20:04.790 --> 00:20:08.270
bit more skepticism. But I
think by that point, you'll

362
00:20:08.270 --> 00:20:11.210
have a bigger database for
all the cars to pull

363
00:20:11.210 --> 00:20:15.020
from to detect things that
are odd, like deer or

364
00:20:15.020 --> 00:20:18.410
skunks or something that we
can't really model for very

365
00:20:18.410 --> 00:20:22.220
easily. Interesting. So you talk
about that, that modeling idea,

366
00:20:22.220 --> 00:20:24.560
but then at the very
beginning of our chat, you

367
00:20:24.560 --> 00:20:28.280
mentioned the idea of smart
cities, smart parking spots, smart

368
00:20:28.280 --> 00:20:32.360
roads, I guess I imagine
a self driving car to

369
00:20:32.360 --> 00:20:36.020
be like an AI in
a vehicle with wheels that

370
00:20:36.020 --> 00:20:39.590
was just super smart. But
now you mentioned skunks or

371
00:20:39.590 --> 00:20:42.590
raccoons and it starts, it
makes me think about there's

372
00:20:42.590 --> 00:20:46.280
so much stuff that could
potentially happen that I don't

373
00:20:46.280 --> 00:20:48.790
know how anyone even a
human could be prepared to

374
00:20:48.790 --> 00:20:52.030
get into this vehicle much
less than AI are these

375
00:20:52.030 --> 00:20:55.840
initial shuttles going to know
more about the roads because

376
00:20:55.840 --> 00:21:01.540
they are kind of constrained
to stay within a certain,

377
00:21:01.630 --> 00:21:04.480
you know, several city blocks
or constrained to stay within

378
00:21:04.480 --> 00:21:06.460
an area. Do they have
additional sensors that we can't

379
00:21:06.460 --> 00:21:09.030
see? Yeah. I think, I
think you're right on the,

380
00:21:09.090 --> 00:21:12.210
on the right path. Yeah.
The Thomas shuttles that I

381
00:21:12.210 --> 00:21:14.340
speak of that I get
really excited about will be

382
00:21:14.340 --> 00:21:16.170
on a kind of a
fixed route. It'll be a

383
00:21:16.170 --> 00:21:18.630
little bit harder to divert
cause you have to plan

384
00:21:18.630 --> 00:21:20.820
the route, set it up
in about two hours, which

385
00:21:20.820 --> 00:21:23.040
is still pretty quick, but
they'll kind of know what

386
00:21:23.040 --> 00:21:26.880
to expect, cause they'll be
on a college campus or

387
00:21:26.910 --> 00:21:29.910
one road or, or something
like that. One when loop,

388
00:21:30.180 --> 00:21:33.630
once you get it into
cars that are more on

389
00:21:33.630 --> 00:21:38.850
demand, like division of, of
mobility on demand, then you

390
00:21:38.850 --> 00:21:42.900
do need to tap into
some larger database for the

391
00:21:42.900 --> 00:21:47.220
unexpected. Right. And what's really
exciting. There is augmented reality

392
00:21:47.220 --> 00:21:52.290
is, is helping get to
that, get cars or the

393
00:21:52.290 --> 00:21:56.280
technology to figure out stuff
faster than we could by

394
00:21:56.520 --> 00:22:00.000
deploying and piloting cars in,
in the real world. So

395
00:22:00.000 --> 00:22:04.260
in a virtual world, they're
actually testing, what does a

396
00:22:04.260 --> 00:22:07.260
car do when it sees
this or that at a

397
00:22:07.260 --> 00:22:11.250
much greater speed for, for
lower costs and then getting

398
00:22:11.250 --> 00:22:14.100
cars out in Nevada or
Arizona where they're testing a

399
00:22:14.100 --> 00:22:18.960
lot now. Okay. So the,
the more you understand, the

400
00:22:18.960 --> 00:22:21.030
more you control the area,
the more you control the

401
00:22:21.030 --> 00:22:23.610
streets and the environment, the
more the car is going

402
00:22:23.610 --> 00:22:26.100
to be more effective. And
if I theoretically said, now

403
00:22:26.130 --> 00:22:28.980
drive me out into the
boonies. The car might be

404
00:22:28.980 --> 00:22:31.380
able to go only so
far, but then in the

405
00:22:31.380 --> 00:22:34.080
future of the cars might
no, no enough to go

406
00:22:34.080 --> 00:22:38.280
anywhere in the country. Yeah.
That, that's what I'm hearing

407
00:22:38.280 --> 00:22:42.240
now. And the ability to
learn on the fly will,

408
00:22:42.330 --> 00:22:46.650
will increase in share that
learned information with other connected

409
00:22:46.650 --> 00:22:50.610
vehicles or autonomous vehicles will
be greater, which is pretty

410
00:22:50.610 --> 00:22:55.050
awesome from a database building
perspective. When, when I learned

411
00:22:55.050 --> 00:22:57.570
to drive a car and
I I'm curious when, when

412
00:22:57.570 --> 00:23:00.930
you learned to drive a
car, my, my dad insisted

413
00:23:00.930 --> 00:23:03.570
that I learned to drive
a stick shift, a manual

414
00:23:04.080 --> 00:23:07.650
shift car because he believed
that understanding how the car

415
00:23:07.650 --> 00:23:10.710
worked, provided some value. And
he wouldn't let me drive

416
00:23:10.890 --> 00:23:14.100
an automatic until I was
competent on a, on a

417
00:23:14.100 --> 00:23:17.730
manual. But there are people
saying that people who are

418
00:23:17.730 --> 00:23:21.750
born today, won't ever drive
a car. Yeah. That's one

419
00:23:21.750 --> 00:23:23.640
of my favorite quotes. You
can argue with it all

420
00:23:23.640 --> 00:23:26.550
you want. But I love
that quote from Henry Christenson,

421
00:23:26.550 --> 00:23:29.340
who used to be at
Georgia tech and we lost

422
00:23:29.340 --> 00:23:31.980
him to UC San Diego,
but he's still, he's still

423
00:23:31.980 --> 00:23:36.450
in our network. Yeah. They,
they might not ever drive

424
00:23:36.450 --> 00:23:39.960
a car. They might have
mobility as a service everywhere.

425
00:23:39.960 --> 00:23:42.870
They go mobility on demand,
which could be a really

426
00:23:42.870 --> 00:23:48.200
exciting opportunity. I think that
opens up a great opportunity

427
00:23:48.200 --> 00:23:51.740
for folks who might be
mobility impaired. There's a lot

428
00:23:51.740 --> 00:23:56.960
of thought in that field
going on about every autonomous

429
00:23:56.960 --> 00:24:01.640
vehicle shuttle that that operates
under a city would have

430
00:24:02.000 --> 00:24:04.400
the ability to deploy a
ramp or be a voice

431
00:24:04.400 --> 00:24:09.410
command operated. The Ali by
local motors is partnering with

432
00:24:09.440 --> 00:24:12.500
IBM to bring Watson into
the car, into the vehicles

433
00:24:12.830 --> 00:24:16.410
so that they can, you
know, people can troubleshoot or

434
00:24:16.940 --> 00:24:19.760
set up the directions without
having to be able to

435
00:24:19.760 --> 00:24:24.370
see your control system. That's
such an important thing to

436
00:24:24.370 --> 00:24:27.130
point out because when we
think about like old people

437
00:24:27.130 --> 00:24:30.820
who aren't mobile, the simplistic
way of thinking about it

438
00:24:30.820 --> 00:24:33.370
is, Oh, well there's us.
And then there's people in

439
00:24:33.370 --> 00:24:37.450
a wheelchair, but I, my
mother-in-law lived with us for,

440
00:24:37.900 --> 00:24:42.160
for almost four years. She
came from Zimbabwe. She was

441
00:24:42.160 --> 00:24:44.500
an older lady. She didn't
have, didn't know how to

442
00:24:44.500 --> 00:24:48.220
drive mobility suddenly, which we
didn't ever thought about became

443
00:24:48.220 --> 00:24:51.010
a big issue for her.
She's perfectly capable of walking,

444
00:24:51.010 --> 00:24:55.150
but the nearest bus stop
is three miles away. So

445
00:24:55.150 --> 00:24:58.690
she had to take, there
was rideshares for seniors. There

446
00:24:58.690 --> 00:25:00.730
were small buses that were
taking her around. We learned

447
00:25:00.730 --> 00:25:03.460
all about the mobility options
around us. And it gave

448
00:25:03.460 --> 00:25:06.280
us a little bit more
empathy. We're realizing that it's

449
00:25:06.280 --> 00:25:10.030
not about ability or disability.
It's, it's about choice. It

450
00:25:10.030 --> 00:25:12.610
can be about age. It
can be about familiarity. There's

451
00:25:12.610 --> 00:25:15.610
so many things that could
change how someone moves around.

452
00:25:15.610 --> 00:25:18.970
And if we have self
driving cars, at some point,

453
00:25:18.970 --> 00:25:21.040
she would just have a
shuttle come to the door.

454
00:25:21.730 --> 00:25:26.380
Yeah. It's really exciting from,
from access prep perspective, I'll

455
00:25:26.380 --> 00:25:29.560
tell you, we work a
lot in human services, transportation,

456
00:25:29.560 --> 00:25:33.400
which covers that world that
you got intimately acquainted with.

457
00:25:34.840 --> 00:25:38.440
And there's so much room
for disruption in that field,

458
00:25:39.160 --> 00:25:42.070
which I think autonomous vehicles
will make, make it possible

459
00:25:42.070 --> 00:25:45.670
for in software developers, anyone
listening, who needs a startup

460
00:25:45.700 --> 00:25:49.660
idea, look into HST. There's
such a low bar to

461
00:25:49.660 --> 00:25:54.370
improve upon. Yeah, it's, it's
really exciting from the access

462
00:25:54.370 --> 00:25:57.730
and equity perspective. I'll tell
you candidly, that it is

463
00:25:57.850 --> 00:26:01.360
a nightmare. From a modeling
perspective, we do a lot

464
00:26:01.360 --> 00:26:04.630
of transportation modeling at my
office to see what our

465
00:26:04.660 --> 00:26:08.620
congestion will look like. I'm
into 2040 right now, and

466
00:26:08.620 --> 00:26:11.980
we'll go further next year.
But that adds so many

467
00:26:11.980 --> 00:26:15.040
more trips. And how do
you plan for better signal

468
00:26:15.040 --> 00:26:19.390
phasing or, or whatever can
improve congestion issues when you're

469
00:26:19.390 --> 00:26:23.380
adding doubt hundreds of thousands,
more trips, hopefully. Cause it

470
00:26:23.380 --> 00:26:26.890
would be great for more
people to get around independently

471
00:26:28.030 --> 00:26:30.610
that can't right now. But
with every good, there is

472
00:26:30.610 --> 00:26:32.800
a, there's something to work
through. So it's a good

473
00:26:32.800 --> 00:26:35.170
challenge to have. Yeah. And
it can be made to

474
00:26:35.170 --> 00:26:37.630
benefit everyone. Like if I
had a car that was

475
00:26:37.630 --> 00:26:40.600
super smart, I could put
it into the pool and

476
00:26:40.600 --> 00:26:42.760
then it could be sent
off. Like if I don't

477
00:26:42.760 --> 00:26:44.910
need my car until lunchtime,
I could have my car

478
00:26:44.910 --> 00:26:47.570
driving people around town on
its own. And then maybe

479
00:26:47.580 --> 00:26:50.400
it could come back just
like solar can push power

480
00:26:50.400 --> 00:26:52.560
back onto the grid. There
could be a grid of

481
00:26:52.590 --> 00:26:55.940
vehicle. Yeah, exactly. It's pretty
exciting from a cost sharing

482
00:26:55.940 --> 00:26:59.750
perspective, bringing your price of
ownership down and making the

483
00:26:59.750 --> 00:27:02.810
price of getting around cheaper
for someone else who might

484
00:27:02.810 --> 00:27:07.220
be, you know, on the
lower income spectrum, you kind

485
00:27:07.220 --> 00:27:11.390
of touched on it there
too about electricity or solar.

486
00:27:11.840 --> 00:27:14.810
I'd like to see more
in this field of solar

487
00:27:14.810 --> 00:27:17.660
roads. We're, we're testing some
solar roads here in Georgia

488
00:27:17.660 --> 00:27:20.890
and I'm pretty excited about
the outset of that. And,

489
00:27:20.890 --> 00:27:25.970
and some inductive charging charging
while driving would be awesome

490
00:27:26.000 --> 00:27:28.220
to have deployed and sit
in us cities so that

491
00:27:28.220 --> 00:27:31.040
we can get a better
handle on our air quality.

492
00:27:32.060 --> 00:27:34.520
It's so interesting. It's not
the future we, we thought

493
00:27:34.520 --> 00:27:36.620
we would get when we
watch movies like I robot,

494
00:27:36.650 --> 00:27:38.810
but it actually sounds like
it turned could turn out

495
00:27:38.810 --> 00:27:42.770
to be pretty amazing. I
hope so. One of the

496
00:27:42.770 --> 00:27:45.830
last questions I wanted to
ask was about the trolley

497
00:27:45.830 --> 00:27:49.640
problem and the ideas of
how does an AI make

498
00:27:49.640 --> 00:27:53.420
a decision from an ethical
perspective? The ethics of this

499
00:27:53.420 --> 00:27:54.890
is really interesting and I
know we don't have a

500
00:27:54.890 --> 00:27:57.260
lot of time left, but
can you explain the trolley

501
00:27:57.260 --> 00:27:58.940
problem and why that can
be an issue for self

502
00:27:58.940 --> 00:28:02.540
driving cars? Yeah. The trolley
problem. Yeah. I think you,

503
00:28:02.540 --> 00:28:06.590
you got to it pretty
quickly is that if an

504
00:28:06.590 --> 00:28:09.800
autonomous vehicle is about to
crash, which decision does it

505
00:28:09.800 --> 00:28:11.810
make? It could take out
a kid, it could take

506
00:28:11.810 --> 00:28:13.610
that in adults. It could
take out the people in

507
00:28:13.610 --> 00:28:16.700
the vehicle. What's the lesser
of all evils, I guess.

508
00:28:16.700 --> 00:28:19.970
And how quickly does it
make that decision? That is

509
00:28:19.970 --> 00:28:23.420
a very valid question. That
many people are, are working

510
00:28:23.420 --> 00:28:26.720
harder than I could imagine
on that's something that the

511
00:28:26.720 --> 00:28:29.480
insurance companies definitely want an
answer to sooner rather than

512
00:28:29.480 --> 00:28:32.210
later. And I'm always impressed
when I check in with,

513
00:28:32.250 --> 00:28:35.420
with insurance companies to see
where they're at, but I

514
00:28:35.420 --> 00:28:38.060
like to take a step
back when that question is

515
00:28:38.060 --> 00:28:41.300
asked, asked a little bit
about the number of accidents

516
00:28:41.300 --> 00:28:43.970
on the road or crashes
or on the road every

517
00:28:43.970 --> 00:28:47.690
day that are fetal there's
I forget the exact figure,

518
00:28:47.690 --> 00:28:51.200
but like 30,000 crashes on
the road everyday that are

519
00:28:51.200 --> 00:28:57.080
close to fatal and they're
the propensity of, or actually

520
00:28:57.080 --> 00:29:02.870
the, the possibility of autonomous
vehicles, they could reduce crashes

521
00:29:02.870 --> 00:29:07.610
by up to 90%. I
think the feds released 86%

522
00:29:07.610 --> 00:29:14.750
after some, some testing reducing
30,000 daily crashes by 86%

523
00:29:14.750 --> 00:29:18.350
or 90% is just unfathomable.
So having those situations come

524
00:29:18.350 --> 00:29:22.400
up 86% less of the
time is, is something that

525
00:29:23.180 --> 00:29:25.730
I think everyone should applaud
a hundred percent of the

526
00:29:25.730 --> 00:29:28.970
time. It would be better,
but yeah, reduction in potential

527
00:29:28.970 --> 00:29:31.910
crashes by 86% of the
time is something we, we

528
00:29:31.910 --> 00:29:35.720
can't do as humans. I
think that's really, really important

529
00:29:35.720 --> 00:29:38.930
to remember that we might
freak out. Let's say that

530
00:29:38.930 --> 00:29:41.090
a self driving car makes
a mistake and someone, one

531
00:29:41.110 --> 00:29:46.270
person dies 40,000 people died,
you know, in 2016 on

532
00:29:46.270 --> 00:29:49.630
the road, that's insane. And
like you said, millions and

533
00:29:49.630 --> 00:29:54.340
millions are injured and we
only have 300 million. Like we

534
00:29:54.340 --> 00:29:56.800
all know someone who's been
in an accident, right. There's

535
00:29:56.800 --> 00:29:59.410
only 350 million people in the
country. Every one of us

536
00:29:59.410 --> 00:30:01.120
know someone who's been in
a car accident and been

537
00:30:01.120 --> 00:30:03.190
somewhat injured, if we can
reduce that, that would be

538
00:30:03.190 --> 00:30:07.910
amazing. Yeah. Yeah. It's, it's
almost unfathomable the potential for,

539
00:30:07.910 --> 00:30:12.390
for safety that could be
improved there, but also when

540
00:30:12.390 --> 00:30:15.360
a crash does happen, the
information about that crash can

541
00:30:15.360 --> 00:30:18.990
re report it a lot
quicker to the local jurisdictions

542
00:30:18.990 --> 00:30:23.610
and then whatever caused that
crash can get mitigated quicker

543
00:30:23.610 --> 00:30:27.090
so that it won't happen
again. Good point. I do.

544
00:30:27.240 --> 00:30:28.710
The one thing I am
worried about is I don't

545
00:30:28.710 --> 00:30:31.740
want people on their self
driving cars texting. I still

546
00:30:31.740 --> 00:30:33.930
would rather that this is
just me being old. I'd

547
00:30:33.930 --> 00:30:36.390
rather that they just pay
attention, even if they can't

548
00:30:36.390 --> 00:30:39.570
do anything. I'm so frustrated
right now when I see

549
00:30:39.570 --> 00:30:43.500
people on their cell phones
at stoplights, I don't know

550
00:30:43.500 --> 00:30:46.650
when that became normalized behavior.
Oh yeah. I'm a hundred

551
00:30:46.650 --> 00:30:49.230
percent with you. And I
don't drive that often. And

552
00:30:49.290 --> 00:30:51.030
I find it hard to
put the phone down when

553
00:30:51.030 --> 00:30:53.040
I hear dang. So I
just turned my phone off

554
00:30:53.040 --> 00:30:57.180
sometimes. Cause it's, it's kinda
like, it's not even fear

555
00:30:57.180 --> 00:31:00.750
of missing out. It's just
a little addiction. Yeah. I'm

556
00:31:00.750 --> 00:31:03.870
looking forward to iOS 11
on the iPhone is going

557
00:31:03.870 --> 00:31:06.450
to be able to detect
when you are driving and

558
00:31:06.450 --> 00:31:08.730
it'll put the phone in
totally quiet mode while you're

559
00:31:08.730 --> 00:31:12.360
driving, which is not self
driving cars, but it'll definitely

560
00:31:12.360 --> 00:31:15.030
make my life better. Yeah.
Our car right now, won't

561
00:31:15.030 --> 00:31:17.640
allow us to text back
when we're driving or when

562
00:31:17.640 --> 00:31:20.610
it detects that the car's
moving, but nice. There's more

563
00:31:20.610 --> 00:31:23.760
improvements to come there. Fantastic.
Well, thank you so much,

564
00:31:23.790 --> 00:31:27.000
Leslie Caseta for talking to
me today about technology and

565
00:31:27.000 --> 00:31:29.610
transportation and what's coming in
the future. Yeah. Thank you,

566
00:31:29.610 --> 00:31:33.360
Scott. This has been another
episode of Hansel minutes and

567
00:31:33.360 --> 00:31:34.710
we'll see you again next
week.

